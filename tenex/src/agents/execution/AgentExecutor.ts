import type { CompletionResponse, LLMService, Message } from "@/core/llm/types";
import type { ConversationPublisher } from "@/nostr";
import { PromptBuilder } from "@/prompts";
import { getProjectContext } from "@/runtime";
import { ToolExecutionManager } from "@/tools/execution";
import {
  type TracingContext,
  type TracingLogger,
  createAgentExecutionContext,
  createChildContext,
  createToolExecutionContext,
  createTracingLogger,
} from "@/tracing";
import type { Phase } from "@/types/conversation";
import type { LLMMetadata } from "@/types/nostr";
import { inventoryExists } from "@/utils/inventory";
import type { NDKEvent } from "@nostr-dev-kit/ndk";
import { logger } from "@/utils/logger";
import type {
  AgentExecutionContext,
  AgentExecutionResult,
  AgentPromptContext,
} from "./types";
import type { ToolExecutionResult } from "@/types/tool";

export class AgentExecutor {
  private toolManager = new ToolExecutionManager();

  constructor(
    private llmService: LLMService,
    private conversationPublisher: ConversationPublisher
  ) {}

  /**
   * Execute an agent's assignment for a conversation
   */
  async execute(
    context: AgentExecutionContext,
    triggeringEvent: NDKEvent,
    parentTracingContext?: TracingContext
  ): Promise<AgentExecutionResult> {
    // Create agent execution tracing context
    const tracingContext = parentTracingContext
      ? createAgentExecutionContext(parentTracingContext, context.agent.name)
      : createAgentExecutionContext(
          { conversationId: context.conversation.id, executionId: "root", startTime: Date.now() },
          context.agent.name
        );

    const tracingLogger = createTracingLogger(tracingContext, "agent");

    tracingLogger.startOperation("agent_execution", {
      agentName: context.agent.name,
      agentPubkey: context.agent.pubkey,
      phase: context.phase,
    });

    try {
      // 1. Build the agent's prompt
      const promptContext = await this.buildPromptContext(context);

      // 2. Generate initial response via LLM
      const llmStartTime = Date.now();
      tracingLogger.logLLMRequest(context.agent.llmConfig || "default");

      const { response: initialResponse, userPrompt } = await this.generateResponse(
        promptContext,
        context.agent.llmConfig || "default"
      );

      const llmDuration = Date.now() - llmStartTime;
      tracingLogger.logLLMResponse(
        context.agent.llmConfig || "default",
        llmDuration,
        initialResponse.usage?.promptTokens,
        initialResponse.usage?.completionTokens
      );

      // 3. Execute the Reason-Act loop
      const reasonActResult = await this.executeReasonActLoop(
        initialResponse,
        context,
        promptContext.systemPrompt,
        userPrompt,
        tracingContext
      );

      // 4. Build metadata with final response
      const llmMetadata = this.buildLLMMetadata(
        reasonActResult.finalResponse,
        promptContext.systemPrompt,
        userPrompt
      );

      // 5. Determine next responder
      const nextResponder = this.determineNextResponder(context, reasonActResult.finalContent);

      // 6. Publish response to Nostr
      const publishedEvent = await this.publishResponse(
        context,
        triggeringEvent,
        reasonActResult.finalContent,
        nextResponder,
        llmMetadata,
        tracingContext
      );

      tracingLogger.completeOperation("agent_execution", {
        agentName: context.agent.name,
        responseLength: reasonActResult.finalContent.length,
        toolExecutions: reasonActResult.allToolResults.length,
        nextAgent: nextResponder,
      });

      return {
        success: true,
        response: reasonActResult.finalContent,
        llmMetadata,
        toolExecutions: reasonActResult.allToolResults,
        nextAgent: nextResponder,
        publishedEvent,
      };
    } catch (error) {
      tracingLogger.failOperation("agent_execution", error, {
        agentName: context.agent.name,
      });

      return {
        success: false,
        error: error instanceof Error ? error.message : String(error),
      };
    }
  }

  /**
   * Build the complete prompt context for the agent
   */
  private async buildPromptContext(context: AgentExecutionContext): Promise<AgentPromptContext> {
    const projectContext = getProjectContext();
    const promptBuilder = new PromptBuilder();

    // Build inventory prompt if needed
    let inventoryPrompt: string | undefined;
    if (context.phase === "chat") {
      const hasInventory = await inventoryExists(projectContext.projectPath);
      if (hasInventory) {
        inventoryPrompt = `## Project Inventory

An inventory file exists for this project. To get detailed project structure and file information, please refer to the inventory file generated by Claude Code.`;
      }
    }

    // Build system prompt
    const systemPrompt = promptBuilder
      .add("agent-system-prompt", {
        agent: context.agent,
        phase: context.phase,
        projectTitle: projectContext.title,
        projectRepository: projectContext.repository,
        inventoryPrompt,
      })
      .build();

    // Build conversation history
    const conversationHistory = new PromptBuilder()
      .add("conversation-history", {
        history: context.conversation.history,
      })
      .build();

    // Build phase context
    const phaseContext = new PromptBuilder()
      .add("phase-context", {
        phase: context.phase,
        phaseMetadata: context.conversation.metadata,
      })
      .build();

    const constraints = this.getPhaseConstraints(context.phase);

    return {
      systemPrompt,
      conversationHistory,
      phaseContext,
      availableTools: context.agent.tools,
      constraints: constraints,
    };
  }

  /**
   * Generate initial response from LLM
   */
  private async generateResponse(
    promptContext: AgentPromptContext,
    llmConfig: string
  ): Promise<{ response: CompletionResponse; userPrompt: string }> {
    // Build full user prompt
    const userPrompt = new PromptBuilder()
      .add("full-prompt", {
        conversationContent: promptContext.conversationHistory || "",
        phaseContext: promptContext.phaseContext,
        constraints: promptContext.constraints,
        agentType: promptContext.phaseContext.includes("Phase:") ? "assigned expert" : "project assistant",
      })
      .build();

    const messages: Message[] = [
      { role: "system", content: promptContext.systemPrompt },
      { role: "user", content: userPrompt },
    ];

    const response = await this.llmService.complete({ messages });
    return { response, userPrompt };
  }

  /**
   * Determine who should respond next
   */
  private determineNextResponder(
    context: AgentExecutionContext,
    response: string
  ): string | undefined {
    // Check if response indicates phase transition
    if (response.includes("PHASE_TRANSITION:")) {
      return undefined; // Router will handle
    }

    // Check if response indicates specific agent handoff
    const handoffMatch = response.match(/HANDOFF_TO:\s*(\S+)/);
    if (handoffMatch) {
      return handoffMatch[1];
    }

    // Default: continue with user
    return undefined;
  }

  /**
   * Publish the agent's response to Nostr
   */
  private async publishResponse(
    context: AgentExecutionContext,
    triggeringEvent: NDKEvent,
    content: string,
    nextResponder: string | undefined,
    llmMetadata?: LLMMetadata,
    tracingContext?: TracingContext
  ): Promise<NDKEvent> {
    const tracingLogger = tracingContext
      ? createTracingLogger(tracingContext, "nostr")
      : logger.forModule("nostr");
    const event = await this.conversationPublisher.publishAgentResponse(
      triggeringEvent,
      content,
      nextResponder || "",
      context.agent.signer,
      llmMetadata
    );

    if (tracingContext && "logEventPublished" in tracingLogger) {
      (tracingLogger as TracingLogger).logEventPublished(event.id || "unknown", "agent_response", {
        agentName: context.agent.name,
        nextResponder,
        hasLLMMetadata: !!llmMetadata,
      });
    }

    return event;
  }

  /**
   * Execute the Reason-Act loop for tool usage
   */
  private async executeReasonActLoop(
    initialResponse: CompletionResponse,
    context: AgentExecutionContext,
    systemPrompt: string,
    userPrompt: string,
    tracingContext: TracingContext
  ): Promise<{
    finalResponse: CompletionResponse;
    finalContent: string;
    allToolResults: ToolExecutionResult[];
  }> {
    const tracingLogger = createTracingLogger(tracingContext, "agent");
    const maxIterations = 3; // Prevent infinite loops
    let currentResponse = initialResponse;
    let finalContent = currentResponse.content;
    const allToolResults: ToolExecutionResult[] = [];
    let iteration = 0;

    const toolContext = {
      projectPath: getProjectContext().projectPath,
      conversationId: context.conversation.id,
      agentName: context.agent.name,
      phase: context.phase,
    };

    // Process tool invocations iteratively
    while (iteration < maxIterations) {
      // Process response for tool invocations
      const { enhancedResponse, toolResults, invocations } = await this.toolManager.processResponse(
        currentResponse.content,
        toolContext
      );

      // If no tools were invoked, we're done
      if (invocations.length === 0) {
        tracingLogger.debug("No tool invocations found, completing Reason-Act loop", {
          agent: context.agent.name,
          iteration,
        });
        break;
      }

      // Map tool results with proper tool names
      const mappedResults: ToolExecutionResult[] = toolResults.map((result, index) => {
        const invocation = invocations[index];
        return {
          ...result,
          toolName: invocation?.toolName || "unknown",
        };
      });

      // Log tool execution with tracing
      for (const invocation of invocations) {
        const toolTracingContext = createToolExecutionContext(tracingContext, invocation.toolName);
        const toolLogger = createTracingLogger(toolTracingContext, "tools");
        
        toolLogger.startOperation("tool_execution", {
          tool: invocation.toolName,
          action: invocation.action,
        });
        
        const toolResult = mappedResults.find(r => r.toolName === invocation.toolName);
        
        toolLogger.completeOperation("tool_execution", {
          tool: invocation.toolName,
          success: toolResult?.success || false,
        });
      }

      allToolResults.push(...mappedResults);

      // If tools were invoked, send results back to LLM for reasoning
      tracingLogger.info("Tool invocations completed, continuing Reason-Act loop", {
        agent: context.agent.name,
        iteration,
        toolCount: invocations.length,
        tools: invocations.map(i => i.toolName),
      });

      // Build continuation prompt with tool results
      const continuationPrompt = this.buildContinuationPrompt(
        userPrompt,
        enhancedResponse,
        mappedResults
      );

      // Generate next response
      const messages: Message[] = [
        { role: "system", content: systemPrompt },
        { role: "user", content: userPrompt },
        { role: "assistant", content: currentResponse.content },
        { role: "user", content: continuationPrompt },
      ];

      tracingLogger.logLLMRequest(context.agent.llmConfig || "default");
      const llmStartTime = Date.now();

      currentResponse = await this.llmService.complete({ messages });

      const llmDuration = Date.now() - llmStartTime;
      tracingLogger.logLLMResponse(
        context.agent.llmConfig || "default",
        llmDuration,
        currentResponse.usage?.promptTokens,
        currentResponse.usage?.completionTokens
      );

      finalContent = currentResponse.content;
      iteration++;
    }

    if (iteration >= maxIterations) {
      tracingLogger.warning("Reason-Act loop reached maximum iterations", {
        agent: context.agent.name,
        iterations: maxIterations,
      });
    }

    return {
      finalResponse: currentResponse,
      finalContent,
      allToolResults,
    };
  }

  /**
   * Build continuation prompt with tool results
   */
  private buildContinuationPrompt(
    originalPrompt: string,
    enhancedResponse: string,
    toolResults: ToolExecutionResult[]
  ): string {
    let prompt = "Based on the tool execution results:\n\n";

    for (const result of toolResults) {
      prompt += `**${result.toolName}**: `;
      if (result.success) {
        prompt +=
          typeof result.output === "string"
            ? result.output
            : JSON.stringify(result.output, null, 2);
      } else {
        prompt += `Error: ${result.error}`;
      }
      prompt += "\n\n";
    }

    prompt += "\nPlease continue with your analysis or provide a final response. ";
    prompt += "If you need to use more tools, you can do so. ";
    prompt += "If you have all the information needed, provide your complete response.";

    return prompt;
  }

  /**
   * Build LLM metadata for response tracking
   */
  private buildLLMMetadata(
    response: CompletionResponse,
    systemPrompt: string,
    userPrompt: string
  ): LLMMetadata | undefined {
    if (!response.usage) {
      return undefined;
    }

    return {
      model: response.model || "unknown",
      cost: 0, // TODO: Calculate cost based on model and token usage
      promptTokens: response.usage.promptTokens,
      completionTokens: response.usage.completionTokens,
      totalTokens: response.usage.totalTokens,
    };
  }

  /**
   * Get phase-specific constraints
   */
  private getPhaseConstraints(phase: Phase): string[] {
    switch (phase) {
      case "chat":
        return [
          "Focus on understanding requirements",
          "Ask one or two clarifying questions at most",
          "Keep responses concise and friendly",
        ];

      case "plan":
        return [
          "Create a structured plan with clear milestones",
          "Include time estimates when possible",
          "Identify potential risks or challenges",
        ];

      case "execute":
        return [
          "Focus on implementation details",
          "Provide code examples when relevant",
          "Explain technical decisions",
        ];

      case "review":
        return [
          "Provide constructive feedback",
          "Highlight both strengths and areas for improvement",
          "Suggest specific improvements",
        ];

      default:
        return [];
    }
  }
}
