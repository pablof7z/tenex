This file is a merged representation of the entire codebase, combined into a single document by Repomix.
The content has been processed where content has been compressed (code blocks are separated by ⋮---- delimiter).

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Content has been compressed - code blocks are separated by ⋮---- delimiter
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
critical-issues/
  2025-01-02-e2e-test-failures.md
  2025-08-01-testing-infrastructure-improvements.md
  2025-08-02-mock-llm-regexp-support.md
  2025-08-02-testing-gaps-analysis.md
  20250102-bun-module-mocking-limitations.md
docs/
  AGENT-EXECUTION-ARCHITECTURE.md
  routing-system-redesign.md
  testing-improvements-2025-08-01.md
  testing-improvements-performance-2025-08-02.md
scripts/
  build-bundled.js
src/
  agents/
    __tests__/
      AgentPublisher.test.ts
      AgentRegistry.republish.test.ts
      AgentRegistry.test.ts
      mcp-integration.test.ts
      tool-assignment.test.ts
    built-in/
      executor.ts
      orchestrator.ts
      planner.ts
      project-manager.ts
    execution/
      __tests__/
        AgentExecutor-simple.test.ts
        AgentExecutor.test.ts
        ClaudeBackend.test.ts
        complete-reminder.test.ts
        completionHandler.test.ts
        ReasonActLoop.errorRecovery.test.ts
        ReasonActLoop.orchestrator-reminder.test.ts
        ReasonActLoop.toolError.test.ts
        RoutingBackend.test.ts
      AgentExecutor.ts
      ClaudeBackend.ts
      completionHandler.ts
      constants.ts
      control-flow-types.ts
      ExecutionBackend.ts
      index.ts
      ReasonActLoop-old.ts
      ReasonActLoop.ts
      RoutingBackend.ts
      StreamStateManager.ts
      TerminationHandler.ts
      ToolStreamHandler.ts
      types.ts
    AgentPublisher.ts
    AgentRegistry.ts
    builtInAgents.ts
    constants.ts
    index.ts
    types.ts
    utils.ts
  claude/
    executor.ts
    index.ts
    orchestrator.ts
  commands/
    agent/
      add.ts
      index.ts
      list.ts
      remove.ts
    debug/
      chat.ts
      claudeCode.ts
      conversation.ts
      conversationSelector.ts
      index.ts
      timeline.ts
    inventory/
      index.ts
    mcp/
      __tests__/
        add.test.ts
        list.test.ts
      add.ts
      index.ts
      list.ts
      remove.ts
    project/
      index.ts
      init.ts
      run.ts
    run/
      constants.ts
      processedEventTracking.ts
      ProjectDisplay.ts
      StatusPublisher.ts
      SubscriptionManager.ts
    setup/
      index.ts
      llm.ts
    daemon.ts
  conversations/
    __tests__/
      ConversationManager.integration.test.ts
      ConversationManager.synchronizeAgentContext.test.ts
      ConversationManager.test.ts
      executionTime.test.ts
      orchestrator-marker-simple.test.ts
    persistence/
      __tests__/
        FileSystemAdapter.integration.test.ts
        FileSystemAdapter.state-persistence.test.ts
        FileSystemAdapter.test.ts
      FileSystemAdapter.ts
      index.ts
      schemas.ts
      types.ts
    ConversationManager.ts
    executionTime.ts
    index.ts
    phases.ts
    types.ts
  daemon/
    __tests__/
      EventMonitor.test.ts
      ProcessManager.test.ts
      ProjectManager.test.ts
    EventMonitor.ts
    ProcessManager.ts
    ProjectManager.ts
  event-handler/
    __tests__/
      newConversation.test.ts
      reply.test.ts
      task.test.ts
    index.ts
    newConversation.ts
    project.ts
    reply.ts
    task.ts
  events/
    index.ts
    NDKAgent.ts
    NDKAgentLesson.ts
    NDKMCPTool.ts
  lib/
    fs/
      filesystem.ts
      index.ts
      tenex.ts
    shell.ts
  llm/
    __tests__/
      pricing.test.ts
    callLogger.ts
    constants.ts
    index.ts
    LLMConfigEditor.ts
    models.ts
    pricing.ts
    router.ts
    ToolPlugin.ts
    ToolResult.ts
    types.ts
  logging/
    __tests__/
      ExecutionLogger.test.ts
    ExecutionLogger.ts
  nostr/
    __tests__/
      NostrPublisher.test.ts
      TaskPublisher.test.ts
      TypingIndicatorManager.integration.test.ts
      TypingIndicatorManager.manual.test.ts
      TypingIndicatorManager.test.ts
    index.ts
    ndkClient.ts
    NostrPublisher.ts
    tags.ts
    TaskPublisher.ts
    types.ts
    TypingIndicatorManager.ts
    utils.ts
  prompts/
    __tests__/
      FragmentRegistry.test.ts
      PromptBuilder.test.ts
    core/
      FragmentRegistry.ts
      index.ts
      PromptBuilder.ts
      types.ts
    fragments/
      __tests__/
        agent-execution.test.ts
        agent-tools.test.ts
        agentFragments.test.ts
        available-agents.test.ts
        expertise-boundaries.test.ts
        integration.test.ts
        mcp-tools.test.ts
        orchestrator-routing-clarity.test.ts
        orchestrator-routing.test.ts
        phase-definitions.test.ts
        tool-registry.test.ts
        voice-mode.test.ts
      agent-common.ts
      agent-completion-guidance.ts
      agent-reasoning.ts
      agent-tools.ts
      agentFragments.ts
      available-agents.ts
      domain-expert-guidelines.ts
      execute-task-prompt.ts
      expertise-boundaries.ts
      inventory.ts
      mcp-tools.ts
      orchestrator-routing.ts
      phase-definitions.ts
      phase.ts
      project-md.ts
      project.ts
      referenced-article.ts
      retrieved-lessons.ts
      tool-use.ts
      voice-mode.ts
    utils/
      __tests__/
        systemPromptBuilder.test.ts
      llmMetadata.ts
      messageBuilder.ts
      systemPromptBuilder.ts
    index.ts
  services/
    __tests__/
      ConfigService.test.ts
    config/
      __tests__/
        mcp-config.test.ts
      types.ts
    mcp/
      __tests__/
        MCPService.integration.test.ts
        MCPService.test.ts
      mcpInstaller.ts
      MCPService.ts
      MCPToolAdapter.ts
    ConfigService.ts
    index.ts
    ProjectContext.ts
  test-utils/
    mock-llm/
      scenarios/
        concurrency-workflow.ts
        error-handling.ts
        index.ts
        inventory-generation.ts
        network-resilience.ts
        orchestrator-workflow.ts
        performance-testing.ts
        routing-decisions.ts
        state-persistence.ts
      example-e2e.test.ts
      index.ts
      MockLLMService.ts
      performance.test.ts
      types.ts
    index.ts
    mock-factories.ts
    test-persistence-adapter.ts
  tools/
    __tests__/
      zod-schema.test.ts
    implementations/
      __tests__/
        analyze.test.ts
        complete.test.ts
        continue.test.ts
        createMilestoneTask.test.ts
        endConversation.test.ts
        generateInventory.test.ts
        learn.test.ts
        readPath.test.ts
        shell-simple.test.ts
        shell.test.ts
        writeContextFile.test.ts
      analyze.ts
      complete.ts
      continue.ts
      createMilestoneTask.ts
      endConversation.ts
      generateInventory.ts
      learn.ts
      readPath.ts
      shell.ts
      writeContextFile.ts
    core.ts
    executor.ts
    registry.ts
    toolLogger.ts
    types.ts
    utils.ts
    zod-schema.ts
  tracing/
    __tests__/
      TracingContext.test.ts
      TracingLogger.test.ts
    index.ts
    TracingContext.ts
    TracingLogger.ts
  utils/
    __tests__/
      lessonMetrics.test.ts
    git/
      createExecutionBranch.ts
      gitignore.ts
      index.ts
      initializeGitRepo.ts
    agentFetcher.ts
    conversationFetcher.ts
    error-formatter.ts
    errors.ts
    formatting.ts
    inventory.ts
    lessonMetrics.ts
    logger.ts
    process.ts
    projectInitialization.ts
    relays.ts
    repomix.ts
    setup.ts
    string.ts
  browser.ts
  index.ts
  tenex.ts
tests/
  e2e/
    agent-error-recovery.test.ts
    complete-tool-integration.test.ts
    concurrency-multiple-conversations.test.ts
    inventory-generation-simple.test.ts
    mcp-service-error-handling.test.ts
    nostr-network-resilience.test.ts
    orchestrator-simple.test.ts
    orchestrator-workflow.test.ts
    performance-execution-timeout.test.ts
    performance-simple.test.ts
    performance-timeout.test.ts
    simple-flow.test.ts
    state-persistence.test.ts
    state-recovery.test.ts
    test-harness.ts
.gitignore
.npmignore
biome.json
bunfig.toml
E2E_FRAMEWORK.md
eslint.config.js
find_orphaned_files.sh
package.json
test-stream-publisher.js
tsconfig.build.json
tsconfig.eslint.json
tsconfig.json
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="critical-issues/2025-01-02-e2e-test-failures.md">
# Critical Issue: E2E Test Failures

**Date**: 2025-01-02
**Severity**: High
**Component**: E2E Testing Infrastructure

## Summary

Multiple E2E tests are failing due to recent changes in the codebase. The test harness and several tests need to be updated to match the current implementation.

## Impact

- 66 out of 70 E2E tests are failing
- New performance timeout tests cannot run properly due to infrastructure issues
- Testing infrastructure is not properly validating system behavior

## Root Causes

1. **RoutingBackend expects AgentExecutor in context**
   - Error: "AgentExecutor not available in context"
   - The RoutingBackend now expects `context.agentExecutor` to be present

2. **ConfigService API changes**
   - Error: "configService.setProjectConfig is not a function"
   - The ConfigService interface has changed and tests are using outdated methods

3. **ExecutionContext missing conversationId**
   - Error: "undefined is not an object (evaluating 'context.conversationId')"
   - ExecutionContext now requires conversationId field

4. **JSON parsing errors in routing decisions**
   - Error: "Failed to parse routing decision: No JSON found in response"
   - Mock responses need to return proper JSON format for orchestrator routing

## Proposed Solution

### 1. Update Test Harness
- Add agentExecutor to execution context
- Update ConfigService usage to match current API
- Ensure ExecutionContext includes all required fields

### 2. Fix Mock Responses
- Ensure orchestrator responses return valid JSON
- Update response format to match expected routing decision structure

### 3. Update Individual Tests
- Update all tests to use new API signatures
- Fix context creation to include all required fields
- Update mock expectations to match current behavior

## Priority Actions

1. **Immediate**: Update test harness to fix context issues
2. **Short-term**: Fix all E2E tests to pass with current codebase
3. **Medium-term**: Add validation to prevent API changes from breaking tests

## Technical Details

### Failed Test Categories:
- Agent Error Recovery: 5 tests
- Complete Tool Integration: 2 tests  
- MCP Service Error Handling: 8 tests
- Performance Execution Timeout: 5 tests (newly added)
- Various other E2E tests: ~46 tests

### Example Fix for ExecutionContext:
```typescript
const executionContext: ExecutionContext = {
    conversationId: conversationId, // Add this field
    conversation: conversation!,
    conversationManager: context.conversationManager,
    agent: context.agentRegistry.getAgent("Orchestrator")!,
    agentExecutor: executor, // Add this field
    // ... rest of context
};
```

## Next Steps

1. Create a comprehensive fix for the test harness
2. Update all mock scenarios to use correct response formats
3. Run full E2E test suite to ensure all tests pass
4. Add CI checks to prevent test infrastructure breakage
</file>

<file path="critical-issues/2025-08-01-testing-infrastructure-improvements.md">
# Critical Issue: Testing Infrastructure Improvements Needed

**Date**: 2025-08-01  
**Severity**: Medium  
**Impact**: Development velocity, system reliability

## Summary

While implementing E2E tests for critical system components, several testing infrastructure issues were discovered that prevent comprehensive test coverage.

## Issues Found

### 1. NostrEvent Serialization in Tests
- **Problem**: Mock NostrEvents don't have the `serialize()` method required by FileSystemAdapter
- **Impact**: E2E tests fail when ConversationManager tries to persist conversations
- **Location**: `src/conversations/persistence/FileSystemAdapter.ts:73`
- **Workaround Needed**: Mock the serialize method or use a test-specific persistence adapter

### 2. Module Mocking Limitations
- **Problem**: Bun's module mocking has limitations with complex dependency chains
- **Impact**: Difficult to mock external services like MCP servers and child processes
- **Examples**:
  - Cannot mock global objects like `Bun.spawn`
  - Child process mocking requires complex workarounds
  - Deep module dependencies make isolation difficult

### 3. Missing Test Utilities
- **Problem**: Lack of comprehensive test utilities for common scenarios
- **Needed**:
  - Proper NostrEvent factory with all required methods
  - Mock persistence adapter for testing
  - Better agent execution mocking
  - Simplified conversation creation for tests

## Recommendations

### Immediate Actions
1. Create a `MockNostrEvent` class that properly implements all required methods
2. Add a `TestPersistenceAdapter` that stores in memory instead of filesystem
3. Document module mocking patterns that work with Bun

### Future Improvements
1. **Test Data Builders**: Create builder pattern utilities for complex objects
2. **Integration Test Mode**: Add environment flag to disable certain features in tests
3. **Mock Service Registry**: Centralized place to register and manage mock services
4. **Test Scenarios Library**: Expand the mock LLM scenarios for common workflows

## Code Examples

### MockNostrEvent Implementation
```typescript
export class MockNostrEvent implements NostrEvent {
    // ... standard NostrEvent properties ...
    
    serialize(includeSignature?: boolean, includeId?: boolean): any {
        return {
            id: includeId ? this.id : undefined,
            pubkey: this.pubkey,
            created_at: this.created_at,
            kind: this.kind,
            tags: this.tags,
            content: this.content,
            sig: includeSignature ? this.sig : undefined
        };
    }
}
```

### Test Persistence Adapter
```typescript
export class TestPersistenceAdapter implements IPersistenceAdapter {
    private storage = new Map<string, any>();
    
    async save(conversation: Conversation): Promise<void> {
        this.storage.set(conversation.id, conversation);
    }
    
    async load(id: string): Promise<Conversation | null> {
        return this.storage.get(id) || null;
    }
}
```

## Impact on Current Tests

The following test files are affected and would benefit from these improvements:
- `tests/e2e/agent-error-recovery.test.ts` - Currently fails due to serialization
- `tests/e2e/mcp-integration.test.ts` - Complex mocking required
- Future conversation persistence tests
- Future multi-agent collaboration tests

## Next Steps

1. Implement MockNostrEvent utility
2. Create TestPersistenceAdapter
3. Update existing E2E tests to use new utilities
4. Add integration test configuration to project
5. Document testing best practices in E2E_FRAMEWORK.md
</file>

<file path="critical-issues/2025-08-02-mock-llm-regexp-support.md">
# Critical Issue: MockLLMService RegExp Support

## Issue Summary
The MockLLMService in the E2E testing framework has a type mismatch issue where it expects `agentName` to be a string but the trigger interface allows it to be a RegExp. This causes a runtime error when trying to call `toLowerCase()` on a RegExp object.

## Impact
- E2E tests using RegExp patterns for agent name matching fail
- Limits flexibility in writing mock scenarios
- Affects the new network resilience test and potentially other tests

## Error Details
```
TypeError: trigger.agentName.toLowerCase is not a function
at findMatchingResponse (/src/test-utils/mock-llm/MockLLMService.ts:163:56)
```

## Root Cause
In MockLLMService.ts line 163:
```typescript
if (trigger.agentName && trigger.agentName.toLowerCase() !== agentName.toLowerCase()) {
    continue;
}
```

The code assumes `trigger.agentName` is always a string, but the type definition allows:
```typescript
agentName?: string | RegExp;
```

## Proposed Solution
Update MockLLMService to properly handle both string and RegExp agent names:

```typescript
if (trigger.agentName) {
    if (typeof trigger.agentName === 'string') {
        if (trigger.agentName.toLowerCase() !== agentName.toLowerCase()) {
            continue;
        }
    } else if (trigger.agentName instanceof RegExp) {
        if (!trigger.agentName.test(agentName)) {
            continue;
        }
    }
}
```

## Workaround
For now, avoid using RegExp patterns in mock scenario triggers. Use exact string matches instead.

## Files Affected
- `/src/test-utils/mock-llm/MockLLMService.ts` - Needs fix for RegExp support
- `/tests/e2e/nostr-network-resilience.test.ts` - New test affected by this issue
- `/src/test-utils/mock-llm/scenarios/network-resilience.ts` - Scenario needs string agent names

## Priority
Medium - This blocks certain E2E test patterns but has a workaround
</file>

<file path="critical-issues/2025-08-02-testing-gaps-analysis.md">
# Critical Testing Gaps Analysis - TENEX Backend
Date: 2025-08-02

## Executive Summary
Analysis of the TENEX testing infrastructure reveals several critical components lacking unit tests that could lead to system failures and regressions. These components are central to the event-driven architecture and agent execution flow.

## Critical Components Without Tests

### 1. EventMonitor (src/daemon/EventMonitor.ts)
**Impact**: CRITICAL
- Responsible for monitoring incoming Nostr events and triggering project processes
- Handles project startup logic and event filtering
- A failure here would prevent the entire system from responding to events
- Risk: Silent failures in event processing, projects not starting when they should

### 2. TaskPublisher (src/nostr/TaskPublisher.ts)
**Impact**: HIGH
- Manages the lifecycle of NDKTask events (creation, completion, progress)
- Handles task-related Nostr event publishing
- A failure here would break task tracking and reporting
- Risk: Tasks not being published, progress not tracked, completion status lost

### 3. NostrPublisher (src/nostr/NostrPublisher.ts)
**Impact**: HIGH
- Core component for publishing all Nostr events
- Used by agents to communicate back to users
- A failure here would prevent all agent responses from reaching users
- Risk: Silent communication failures, lost messages

### 4. ExecutionLogger (src/logging/ExecutionLogger.ts)
**Impact**: MEDIUM-HIGH
- Provides structured logging for agent decisions and phase transitions
- Critical for debugging and monitoring system behavior
- A failure here would make troubleshooting production issues extremely difficult
- Risk: Loss of audit trail, inability to debug complex workflows

### 5. TracingLogger (src/tracing/TracingLogger.ts)
**Impact**: MEDIUM
- Provides OpenTelemetry tracing capabilities
- Important for performance monitoring and distributed tracing
- A failure here would impact observability
- Risk: Loss of performance metrics and trace data

## Priority Recommendation

Based on impact analysis, the **EventMonitor** should be the first priority for test implementation because:

1. **Single Point of Failure**: It's the entry point for all system activity
2. **Complex Logic**: Involves event filtering, project management, and process spawning
3. **Critical Path**: Every user interaction depends on this component working correctly
4. **Error Prone**: Involves multiple async operations and external dependencies
5. **Silent Failures**: Errors here might not be immediately visible to users

## Implementation Approach

For EventMonitor testing:
1. Create comprehensive unit tests mocking IProjectManager and IProcessManager
2. Test event filtering logic with various event types
3. Test error handling for project startup failures
4. Test subscription lifecycle (start/stop)
5. Add integration tests with real NDK subscription behavior

## Future Testing Priorities

After EventMonitor:
1. TaskPublisher - Task lifecycle management
2. NostrPublisher - Core communication layer
3. ExecutionLogger - Debugging and monitoring
4. TracingLogger - Observability

## Recommended Actions

1. Implement EventMonitor unit tests immediately
2. Add error boundary tests for each component
3. Create integration tests for the event processing pipeline
4. Add performance tests for high-volume event scenarios
5. Implement proper mocking boundaries for better test isolation
</file>

<file path="critical-issues/20250102-bun-module-mocking-limitations.md">
# Critical Issue: Bun Module Mocking Limitations

**Date**: January 2, 2025  
**Severity**: High  
**Impact**: Testing Infrastructure

## Problem

The current testing infrastructure faces significant challenges with Bun's module mocking capabilities, particularly when testing components with complex dependency chains like `ProjectManager` and `ProcessManager`.

### Specific Issues

1. **Module Mock Order Dependency**
   - Mocks must be defined before any imports that use them
   - This creates issues with complex dependency chains
   - Example: `ProjectManager` imports `child_process` indirectly through multiple layers

2. **Promisify Pattern Incompatibility**
   - Node's `util.promisify` doesn't work well with Bun's mock system
   - Async functions created via promisify bypass mock implementations
   - This affects testing of git operations, file system operations, etc.

3. **Deep Dependency Mocking**
   - Components like `AgentRegistry`, `ConfigService` have deep import chains
   - Mocking these requires mocking entire dependency trees
   - This makes tests brittle and hard to maintain

### Failed Attempts

1. Tried mocking `node:child_process` module - bypassed by promisify
2. Tried mocking `node:util` promisify - didn't intercept actual calls
3. Tried spying on global exec - not accessible in module scope

### Impact on Testing

- Cannot properly test git clone operations in ProjectManager
- Cannot test concurrent access scenarios reliably
- Cannot isolate external command execution
- Reduces confidence in error handling paths

## Proposed Solutions

### Short Term (Recommended)

1. **Create Test Doubles**
   ```typescript
   // Create injectable dependencies
   interface IGitOperations {
     clone(url: string, path: string): Promise<void>;
     init(path: string): Promise<void>;
   }
   
   // Inject into ProjectManager constructor
   constructor(
     projectsPath?: string,
     private gitOps: IGitOperations = new GitOperations()
   ) {}
   ```

2. **Use Environment-Based Testing**
   - Create actual test repositories for integration tests
   - Use temp directories with real git operations
   - Skip unit tests that require complex mocking

3. **Separate Integration Tests**
   - Move tests requiring real external operations to integration suite
   - Use Docker containers for isolated testing environments
   - Accept longer test execution times for thorough coverage

### Long Term

1. **Migrate to Vitest**
   - Better module mocking support
   - Compatible with most Bun features
   - More mature mocking ecosystem

2. **Refactor for Testability**
   - Extract all external operations into injectable services
   - Use dependency injection consistently
   - Avoid direct module imports for external operations

3. **Abstract External Operations**
   - Create abstraction layer for all file system operations
   - Create abstraction layer for all process operations
   - Make these swappable at runtime

## Recommendation

For immediate progress, implement injectable dependencies for critical components:

1. Extract git operations into `GitService`
2. Extract file operations into enhanced `FileSystemService`
3. Make these services injectable into components
4. Create simple in-memory implementations for testing

This approach will:
- Improve testability immediately
- Not require major refactoring
- Allow gradual migration
- Maintain backward compatibility

## Example Implementation

```typescript
// services/git/GitService.ts
export interface IGitService {
  clone(url: string, dest: string): Promise<void>;
  init(path: string): Promise<void>;
  // ... other git operations
}

export class GitService implements IGitService {
  async clone(url: string, dest: string): Promise<void> {
    const { stdout, stderr } = await execAsync(`git clone "${url}" "${dest}"`);
    // ... implementation
  }
}

// In tests
class MockGitService implements IGitService {
  async clone(url: string, dest: string): Promise<void> {
    // Mock implementation
  }
}

const projectManager = new ProjectManager(
  tempDir,
  new MockGitService()
);
```

## Action Items

1. Refactor ProjectManager to use injectable GitService
2. Refactor ProcessManager to use injectable ProcessService
3. Update tests to use mock implementations
4. Document testing patterns for other developers
5. Consider long-term migration strategy
</file>

<file path="docs/AGENT-EXECUTION-ARCHITECTURE.md">
# Agent Execution Architecture

## Overview

The agent execution system in TENEX follows a **strategy pattern** where different execution backends implement the `ExecutionBackend` interface. Each backend provides a different execution strategy suited to specific agent types.

## Execution Backends

### 1. **RoutingBackend** (Orchestrator Only)
- Used exclusively by the orchestrator agent
- Handles message routing between agents
- No tool execution - pure routing decisions
- Returns structured JSON routing decisions

### 2. **ReasonActLoop** (Default)
- Standard execution backend for most agents
- Implements a reason-act loop with tool calling
- Handles streaming responses from LLMs
- Enforces proper termination with `complete()` tool

### 3. **ClaudeBackend** (Special Purpose)
- Direct integration with Claude Desktop
- Bypasses standard tool system
- Used for specialized Claude-native operations

## ReasonActLoop Architecture

The ReasonActLoop backend has been refactored into a clean, modular architecture following SRP (Single Responsibility Principle):

```
┌─────────────────────────────────────────────────────────┐
│                     ReasonActLoop                        │
│                   (Orchestrator - 426 lines)             │
│  - Manages execution flow                                │
│  - Coordinates handlers                                  │
│  - Implements retry logic                                │
└────────────────────┬────────────────────────────────────┘
                     │
        ┌────────────┼────────────┬──────────────┐
        ▼            ▼            ▼              ▼
┌──────────────┐ ┌──────────────┐ ┌──────────────┐ ┌──────────────┐
│StreamState   │ │ToolStream    │ │Termination   │ │Control Flow  │
│Manager       │ │Handler       │ │Handler       │ │Types         │
│(178 lines)   │ │(368 lines)   │ │(183 lines)   │ │(70 lines)    │
├──────────────┤ ├──────────────┤ ├──────────────┤ ├──────────────┤
│Manages       │ │Handles tool  │ │Enforces      │ │Type guards   │
│mutable state │ │start/complete│ │termination   │ │for routing   │
│during stream │ │events        │ │requirements  │ │decisions     │
│processing    │ │              │ │              │ │              │
└──────────────┘ └──────────────┘ └──────────────┘ └──────────────┘
```

## Component Responsibilities

### **ReasonActLoop** (Main Orchestrator)
Primary responsibilities:
- Initialize and coordinate handlers
- Manage the main execution loop
- Handle retry attempts for termination
- Create and process LLM streams
- Delegate to specialized handlers

Key methods:
- `execute()` - Main entry point from ExecutionBackend interface
- `executeStreamingInternal()` - Core async generator loop
- `processStream()` - Routes stream events to appropriate handlers

### **StreamStateManager** 
Encapsulates all mutable state during stream processing:
- Tool execution results
- Accumulated content
- Continue flow decisions
- Termination state
- Stream publisher reference

Key methods:
- `appendContent()` - Add content to accumulated text
- `addToolResult()` - Store tool execution results
- `setContinueFlow()` - Record routing decisions
- `setTermination()` - Mark completion/end states
- `resetForRetry()` - Prepare state for retry attempts

### **ToolStreamHandler**
Manages all tool-related stream events:
- Process `tool_start` events
- Process `tool_complete` events
- Publish typing indicators
- Handle tool errors
- Generate human-readable tool descriptions

Key responsibilities:
- Tool lifecycle management
- Error handling and reporting
- Logging tool execution metrics
- Terminal tool detection

### **TerminationHandler**
Enforces proper agent termination:
- Check if termination is required
- Generate reminder messages
- Auto-complete when agents fail to terminate
- Manage retry logic

Key methods:
- `shouldRetryForTermination()` - Determine if retry needed
- `getReminderMessage()` - Generate context-appropriate reminders
- `prepareRetryMessages()` - Build message array for retry

### **Supporting Modules**

**control-flow-types.ts**
- Type guards for `ContinueFlow`, `Complete`, `EndConversation`
- Validates routing decisions and completion states

**constants.ts**
- `MAX_TERMINATION_ATTEMPTS` - Maximum retry attempts (2)
- `TOOL_INDICATOR_DELAY_MS` - Typing indicator delay (100ms)
- `DEFAULT_TOOL_DURATION_MS` - Default tool execution time (1000ms)

**error-formatter.ts**
- Formats various error types into readable strings
- Extracts meaningful error properties

## Execution Flow

### 1. **Initialization Phase**
```typescript
// Create handlers
const stateManager = new StreamStateManager();
const toolHandler = new ToolStreamHandler(stateManager, executionLogger);
const terminationHandler = new TerminationHandler(stateManager);
```

### 2. **Main Execution Loop**
```typescript
while (attempt < MAX_TERMINATION_ATTEMPTS) {
    // Create LLM stream
    const stream = createLLMStream(context, messages, tools);
    
    // Process events
    yield* processStream(stream, handlers...);
    
    // Check termination
    if (!terminationHandler.shouldRetryForTermination()) {
        break;
    }
    
    // Prepare retry with reminder
    messages = terminationHandler.prepareRetryMessages(messages);
}
```

### 3. **Stream Event Processing**
Each event type is routed to appropriate handler:
- `content` → Accumulate in StateManager
- `tool_start` → ToolStreamHandler
- `tool_complete` → ToolStreamHandler → Terminal check
- `done` → Store final response
- `error` → Error handling

### 4. **Termination Enforcement**
Non-chat/brainstorm agents MUST call terminal tools:
- **Orchestrator**: Must call `continue()` to route
- **Other agents**: Must call `complete()` to return control
- **Auto-completion**: After 2 failed attempts

## Key Design Decisions

### 1. **Orchestrator Routing Removed**
The original ReasonActLoop contained orchestrator-specific routing logic (executing target agents). This has been completely removed as it belongs in RoutingBackend.

### 2. **State Encapsulation**
All mutable state is encapsulated in StreamStateManager with controlled access through methods, preventing direct manipulation.

### 3. **Clear Separation of Concerns**
Each class has a single, well-defined responsibility:
- State management is isolated
- Tool handling is centralized
- Termination logic is extracted

### 4. **Testability**
Each component can be tested in isolation:
- StreamStateManager: Pure state transitions
- ToolStreamHandler: Tool event processing
- TerminationHandler: Termination logic

### 5. **No Over-Engineering**
- No unnecessary abstractions
- Direct method calls, no complex patterns
- Clear, linear flow

## Migration from Old Architecture

### Before (934 lines, single class):
- Mixed responsibilities
- Deep nesting
- Duplicated logic
- Orchestrator routing mixed in

### After (426 lines main + helpers):
- Single responsibility per class
- Flat structure
- Reusable components
- Clean separation of backends

## Testing Strategy

### Unit Tests
Each component should have isolated unit tests:
```typescript
// StreamStateManager tests
- State initialization
- Content accumulation
- Tool result storage
- Termination state transitions

// ToolStreamHandler tests
- Tool start event handling
- Tool complete event handling
- Error publishing
- Terminal tool detection

// TerminationHandler tests
- Retry logic
- Reminder message generation
- Auto-completion
```

### Integration Tests
Test the full ReasonActLoop with mock LLM service:
- Complete execution flow
- Retry behavior
- Terminal tool handling
- Error recovery

## Future Improvements

1. **Event-Driven Architecture**
   - Consider event emitter pattern for stream events
   - Would further decouple components

2. **Strategy Pattern for Tool Descriptions**
   - Extract tool description logic to separate registry
   - Make extensible for new tools

3. **Metrics Collection**
   - Add structured metrics for execution performance
   - Track retry rates and termination failures

4. **Configuration**
   - Make retry attempts configurable per agent
   - Allow custom termination enforcement rules
</file>

<file path="docs/routing-system-redesign.md">
# Complete Routing System Design & Implementation Plan

## Core Principles

1. **Orchestrator as Silent Router**: Never speaks to users, only routes
2. **Phase Outputs via complete()**: Each phase has defined input/output
3. **Automatic Quality Control**: PLAN and EXECUTE phases include review cycles
4. **Direct Message Passing**: No rephrasing or interpretation by orchestrator
5. **Organic User Communication**: Users see all agent complete() messages naturally

## Phase Flow Architecture

```
User Message → Orchestrator (analyzes, routes)
     ↓
Phase Agent (works, completes)
     ↓
Orchestrator (decides next action)
     ↓
[Continue until all phases complete]
```

## Detailed Phase Behaviors

### CHAT Phase
**Input**: User's raw message  
**Agent**: project-manager  
**Output**: Clear requirements/understanding  

```
Example 1 - Clear Request:
User: "Fix the typo in the login page title"
Orchestrator → project-manager: "Fix the typo in the login page title"
project-manager complete(): "Clear task: Fix typo in login page title"
Orchestrator: Recognizes trivial task → Skip to EXECUTE
```

```
Example 2 - Ambiguous Request:
User: "Make the app faster"
Orchestrator → project-manager: "Make the app faster"
project-manager complete(): "Requirements gathered: User wants performance improvements. Need to identify specific bottlenecks and optimization targets."
Orchestrator: Recognizes need for planning → PLAN phase
```

### PLAN Phase
**Input**: Requirements from CHAT  
**Process**: Plan → Review → Refine (if needed)  
**Output**: Actionable implementation plan  

```
Example Flow:
1. Orchestrator → planner: [requirements from CHAT]
   planner complete(): "Plan: 1) Profile current performance 2) Optimize database queries 3) Add caching layer"

2. Orchestrator analyzes plan, identifies relevant experts
   → @database-expert, @performance-expert: "Review this plan"
   
3. Experts respond:
   database-expert complete(): "Consider connection pooling for step 2"
   performance-expert complete(): "LGTM"
   
4. Orchestrator → planner: "Feedback: Consider connection pooling for step 2"
   planner complete(): "Updated plan: 1) Profile 2) Optimize queries with connection pooling 3) Add caching"
   
5. All satisfied → EXECUTE phase
```

### EXECUTE Phase
**Input**: Plan from PLAN phase (or requirements if skipped PLAN)  
**Process**: Recommendations → Implementation → Review → Fix (if needed)  
**Output**: Completed implementation  

```
Example - With Domain Experts:
1. Pre-work consultation:
   Orchestrator identifies relevant experts from plan
   → @ndkswift, @nostr: "Provide recommendations"
   
   ndkswift complete(): "Use maxAge: 0 for real-time streaming"
   nostr complete(): "Limit relay connections to necessary ones only"

2. Implementation:
   Orchestrator → executor: 
   "<work_on>[plan details]</work_on>
    <recommendations>
    - Use maxAge: 0 for real-time streaming
    - Limit relay connections to necessary ones only
    </recommendations>"
   
   executor complete(): "Implemented: Added streaming with maxAge:0, optimized relay connections"

3. Review:
   Orchestrator → @ndkswift, @project-manager: "Review implementation"
   
   ndkswift complete(): "Issue: Missing error handling for connection failures"
   project-manager complete(): "LGTM"
   
4. Fix cycle:
   Orchestrator → executor: "Feedback: Missing error handling for connection failures"
   executor complete(): "Fixed: Added connection failure handling with retry logic"
   
5. Final review → All satisfied → VERIFICATION
```

### VERIFICATION Phase
**Input**: Implementation details  
**Agent**: project-manager (or dedicated tester)  
**Output**: Functional test results  

```
Example:
Orchestrator → project-manager: [implementation details]
project-manager complete(): "Tested: All features working correctly. Performance improved by 40%."
→ CHORES
```

### CHORES Phase
**Input**: Work summary  
**Agent**: documentation-agent (or similar)  
**Output**: Cleanup/documentation summary  

```
Orchestrator → documentation-agent: [work summary]
documentation-agent complete(): "Updated: API docs, README, and changelog"
→ REFLECTION
```

### REFLECTION Phase
**Input**: Complete conversation history  
**Agent**: lessons-learned-agent  
**Output**: Lessons published as nostr events  

```
Orchestrator → lessons-learned-agent: [full context]
lessons-learned-agent complete(): "Published 3 lessons about performance optimization patterns"
→ Conversation naturally ends (no explicit end_conversation needed)
```

## Orchestrator Decision Logic

```typescript
// Pseudo-code for orchestrator's decision making
function onAgentComplete(agent, completion, phase) {
  if (waitingForOthers()) return;
  
  switch(phase) {
    case "chat":
      if (isTrivialTask(completion)) {
        continue({ phase: "execute", agents: ["executor"] });
      } else if (needsPlanning(completion)) {
        continue({ phase: "plan", agents: ["planner"] });
      } else {
        gatherExpertRecommendations();
      }
      break;
      
    case "plan":
      if (!hasBeenReviewed()) {
        const reviewers = findRelevantExperts(completion) || ["project-manager"];
        continue({ agents: reviewers, message: "Review this plan" });
      } else if (hasOnlyApprovals()) {
        gatherExpertRecommendations();
      } else {
        continue({ agents: ["planner"], message: formatFeedback() });
      }
      break;
      
    case "execute":
      if (!hasRecommendations() && shouldGetRecommendations()) {
        const experts = findRelevantExperts();
        continue({ agents: experts, message: "Provide recommendations" });
      } else if (!hasBeenReviewed()) {
        const reviewers = findRelevantExperts() || ["project-manager"];
        continue({ agents: reviewers, message: "Review implementation" });
      } else if (hasOnlyApprovals()) {
        continue({ phase: "verification", agents: ["project-manager"] });
      } else {
        continue({ agents: ["executor"], message: formatFeedback() });
      }
      break;
      
    // Continue for other phases...
  }
}
```

## Implementation Plan

### Phase 1: Core Orchestrator Changes

#### 1.1 Remove end_conversation from orchestrator
**Files to modify:**
- `/src/agents/constants.ts` - Remove end_conversation from orchestrator tools
- `/src/prompts/fragments/orchestrator-routing.ts` - Remove end_conversation references

#### 1.2 Update orchestrator instructions
**File:** `/src/agents/built-in/orchestrator.ts`
```typescript
instructions: `You are a silent router that NEVER communicates with users directly.

CRITICAL RULES:
- You ONLY use the continue() tool - NEVER speak to users
- Pass messages EXACTLY as received - no interpretation or modification
- After receiving complete() from agents, immediately decide next routing
- You are invisible to users - they only see agent outputs

Your ONLY job:
1. Receive user messages when no agent is p-tagged
2. Route to appropriate phase/agent based on content
3. Coordinate review cycles in PLAN and EXECUTE phases
4. Manage phase transitions

Phase routing logic:
- CHAT: Route all new conversations to project-manager first
- Trivial tasks (typos, simple fixes): Can skip from CHAT → EXECUTE
- Complex tasks: CHAT → PLAN → EXECUTE → VERIFICATION → CHORES → REFLECTION
- Each phase must complete before the next begins`
```

#### 1.3 Enhance continue() tool usage
**File:** `/src/prompts/fragments/orchestrator-routing.ts`
```typescript
`When routing to multiple agents:
- For recommendations: "Provide recommendations for: [task description]"
- For reviews: "Review this [plan/implementation]"
- For phase work: Simply forward the previous phase output

Quality control:
- For complex tasks, ensure thorough review through multiple agent interactions
- For simple tasks, use judgment to avoid unnecessary overhead
- If review cycles exceed 3 iterations without progress, consider the task stalled

Expert selection:
- Analyze agent descriptions to identify domain experts
- When in doubt, include more experts rather than fewer
- Always include project-manager as fallback reviewer

Review interpretation:
- "LGTM", "looks good", "no issues" = approval
- Any specific feedback = needs addressing
- Mixed feedback = route back to primary agent with ALL feedback
- If 3+ review cycles occur without progress, auto-complete with unresolved issues summary`
```

### Phase 2: Domain Expert Prompts

#### 2.1 Update expert agent instructions
**Pattern for all domain expert agents:**
```typescript
`When asked for recommendations:
- Provide HIGH-LEVEL guidance within your expertise ONLY
- Focus on best practices and common pitfalls
- Do NOT hallucinate system details or implementation specifics
- Keep recommendations concise and actionable
- Format: "Consider [recommendation] because [reason]"

When asked to review:
- Be SPECIFIC about issues found
- Provide ACTIONABLE feedback with clear reasoning
- Include examples when possible
- If everything looks good, respond with just "LGTM" or "No issues"
- Focus ONLY on your domain of expertise
- Don't review aspects outside your expertise
- If feedback is unclear or needs more context, explicitly state what information is needed

ALWAYS use complete() to return control to orchestrator.`
```

### Phase 3: Complete Tool Enhancement

#### 3.1 No structural changes needed
The current complete() tool already works well for this architecture.

### Phase 4: Review Cycle Implementation

#### 4.1 Add review state tracking
**File:** `/src/agents/execution/ReasonActLoop.ts`
- Track which agents have been consulted
- Detect review cycles vs initial work

#### 4.2 Update conversation context
**File:** `/src/conversations/ConversationManager.ts`
- Ensure phase context includes review state
- Pass recommendations to executor properly

### Phase 5: Phase Skip Logic

#### 5.1 Add to orchestrator routing fragment
```typescript
`Phase skipping guidelines:
- Trivial fixes (typos, formatting): CHAT → EXECUTE
- Clear, specific implementation tasks: Can skip PLAN if unambiguous
- User explicitly says "just do X": Respect their directness
- Emergency fixes: Can skip VERIFICATION/CHORES/REFLECTION if critical

Always use judgment - when in doubt, follow full phase sequence.`
```

### Phase 6: Testing & Validation

#### 6.1 Test scenarios to implement:
1. **Simple typo fix** - Should skip most phases
2. **Complex feature** - Full phase sequence with reviews
3. **Failed review** - Multiple review cycles
4. **No experts available** - Fallback to project-manager
5. **Multiple domain overlap** - Multiple experts consulted

#### 6.2 Key test files to update:
- `/src/agents/execution/__tests__/complete-reminder.test.ts`
- `/src/prompts/fragments/__tests__/orchestrator-routing-clarity.test.ts`
- Add new test: `orchestrator-silent-routing.test.ts`

### Rollout Strategy

1. **Phase 1**: Implement orchestrator changes (1 day)
2. **Phase 2**: Update all domain expert prompts (1 day)
3. **Phase 3**: Test with simple scenarios (1 day)
4. **Phase 4**: Test complex review cycles (1 day)
5. **Phase 5**: Fine-tune prompts based on testing (1 day)

### Key Metrics to Track

- Review cycle efficiency (avoid infinite loops)
- Expert selection accuracy
- Phase skip appropriateness
- User satisfaction (implicit through conversation flow)
- Escalation triggers (review cycles exceeding 3 iterations)

### Edge Case Handling

#### Review Stalemates (3+ iterations)
- Auto-complete with a summary of unresolved issues
- Include all feedback received in the summary
- Future enhancement: consider human escalation mechanism

#### Ambiguous Feedback
- Experts should explicitly request clarification if needed
- Use phrases like "Need more context about X to provide useful feedback"
- Avoid vague statements that don't lead to actionable improvements

#### Execution Context Simplification
- Consider consolidating execution context types to reduce complexity
- Ensure consistent context passing throughout the pipeline

This architecture creates a clean, predictable system where the orchestrator truly becomes an invisible router, and all user communication happens organically through agent complete() messages.
</file>

<file path="docs/testing-improvements-2025-08-01.md">
# TENEX Testing Infrastructure Improvements - August 1, 2025

## Overview

This document summarizes the incremental improvements made to the TENEX testing infrastructure to enhance system reliability and testability.

## Improvements Implemented

### 1. MockLLMService Updates (Critical Fix)

**Problem**: The MockLLMService was using an outdated interface that didn't match the multi-llm-ts v4 API, causing all E2E tests to fail.

**Solution**:
- Updated MockLLMService to implement the new `complete()` and `stream()` methods
- Fixed response format to match expected `{ type, content, toolCalls }` structure
- Updated type imports from `LLMMessage` to `Message`
- Added routing-decisions scenario for orchestrator tests

**Impact**: Restored E2E test functionality and enabled deterministic testing of LLM interactions.

### 2. AgentExecutor Unit Tests

**Problem**: The core AgentExecutor class had 0% test coverage despite being critical for agent execution.

**Solution**:
- Created comprehensive unit tests for AgentExecutor
- Tested backend selection logic (claude, reason-act, routing)
- Added tests for execution flow with mocked backends
- Implemented error handling test scenarios

**Impact**: Increased confidence in the agent execution pipeline and backend selection logic.

### 3. Event Handler Unit Tests

**Problem**: All event handlers (newConversation, reply, task, project) had 0% test coverage.

**Solution**:
- Created tests for newConversation handler
  - Conversation creation flow
  - Agent selection logic
  - Error handling scenarios
- Created tests for reply handler
  - Message addition flow
  - Agent continuation logic
  - Context preservation

**Impact**: Improved reliability of core user interaction flows and error recovery.

## Testing Gaps Still Remaining

### High Priority
1. **ReasonActLoop** - Core execution logic with 0% coverage
2. **ClaudeBackend** - LLM integration with minimal coverage
3. **Nostr Integration** - Publisher and client components untested
4. **Daemon Components** - ProjectManager and EventMonitor lack tests

### Medium Priority
1. **Tool Implementations** - Several tools lack comprehensive tests
2. **MCP Service** - Integration and adapter components need coverage
3. **State Persistence** - Known issues need resolution and tests
4. **Tracing/Logging** - Infrastructure components need coverage

### Low Priority
1. **CLI Commands** - Command implementations lack tests
2. **Utility Functions** - Various helpers need coverage
3. **Performance Tests** - No load or performance testing framework

## Next Steps

1. **Fix State Persistence Issues**: Address the known serialization problems
2. **Test Critical Execution Paths**: Add tests for ReasonActLoop and ClaudeBackend
3. **Integration Test Suite**: Create comprehensive integration tests
4. **Error Recovery Scenarios**: Expand E2E tests for failure modes

## Summary

The testing infrastructure has been improved with targeted, incremental changes that address critical gaps. The MockLLMService fix was essential for enabling deterministic testing, while the new unit tests for AgentExecutor and event handlers provide coverage for core system components.

Future improvements should focus on the execution backends and integration points to ensure system reliability under various failure scenarios.
</file>

<file path="docs/testing-improvements-performance-2025-08-02.md">
# TENEX Testing Infrastructure Enhancement: Performance Testing

Date: 2025-08-02

## Summary

Implemented performance and timeout testing capabilities for the TENEX E2E testing framework, addressing one of the key future enhancements identified in E2E_FRAMEWORK.md.

## Problem Identified

The E2E testing framework lacked the ability to test system behavior under performance stress conditions:
- No way to simulate slow LLM responses
- No timeout handling verification
- No stress testing capabilities
- No memory usage testing

## Solution Implemented

### 1. Performance Testing Scenarios

Created `src/test-utils/mock-llm/scenarios/performance-testing.ts` with scenarios for:

- **Slow LLM responses** (5-second delays)
- **Very slow planning phase** (8-second delays) 
- **Timeout simulation** (35-second delays)
- **Memory-intensive responses** (50KB payloads)
- **Recovery after timeout**
- **Rapid sequential requests**

### 2. MockLLMService Enhancements

The MockLLMService already supported `streamDelay` functionality:
- Delays are applied in both `complete()` and `stream()` methods
- Configurable per-response delays
- Works with concurrent requests

### 3. Test Coverage

Created comprehensive E2E tests in `tests/e2e/performance-timeout.test.ts` covering:
- Slow response handling
- Timeout scenarios
- Memory usage tracking
- Concurrent request handling
- Recovery mechanisms

### 4. Integration Challenges

Discovered that the orchestrator agent uses a specialized routing backend that expects pure JSON responses. Updated scenarios to match the expected format.

## Key Files Added/Modified

1. **New Files:**
   - `src/test-utils/mock-llm/scenarios/performance-testing.ts` - Performance test scenarios
   - `tests/e2e/performance-timeout.test.ts` - E2E performance tests
   - `src/test-utils/mock-llm/performance.test.ts` - Unit tests for delay functionality
   - `tests/e2e/performance-simple.test.ts` - Simplified performance test examples

2. **Modified Files:**
   - `src/test-utils/mock-llm/scenarios/index.ts` - Added performance scenario exports

## Usage Examples

### Using Performance Scenarios in Tests

```typescript
// Setup test with performance scenarios
const context = await setupE2ETest(['performance-testing']);

// Execute agent with slow response
await executeAgent(context, "Orchestrator", conversationId, "performance test");
```

### Adding Custom Delays

```typescript
mockLLM.addResponse({
    trigger: { userMessage: /slow operation/ },
    response: {
        streamDelay: 5000, // 5 second delay
        content: "Delayed response"
    }
});
```

## Benefits

1. **Early Detection** - Identify timeout issues before production
2. **Performance Baseline** - Establish expected response times
3. **Stress Testing** - Verify system behavior under load
4. **Memory Safety** - Ensure large responses don't cause memory issues

## Future Improvements

1. **Actual Timeout Enforcement** - Currently delays work but actual timeout enforcement needs implementation at the execution layer
2. **Network Latency Simulation** - Add variable latency patterns
3. **Progressive Degradation** - Test system behavior as performance degrades
4. **Metrics Collection** - Automated performance metrics gathering

## Testing the Implementation

Run performance tests:
```bash
bun test ./tests/e2e/performance-timeout.test.ts --timeout 60000
```

Run unit tests for delay functionality:
```bash
bun test src/test-utils/mock-llm/performance.test.ts
```

## Conclusion

This enhancement addresses the "Performance Testing" future enhancement from E2E_FRAMEWORK.md, providing a foundation for testing system behavior under stress conditions. The implementation is minimal and focused, following the principle of incremental improvements.
</file>

<file path="scripts/build-bundled.js">
const packageJson = JSON.parse(
readFileSync(join(process.cwd(), 'package.json'), 'utf8')
⋮----
// Get all dependencies to mark as external
⋮----
...Object.keys(packageJson.dependencies || {}),
...Object.keys(packageJson.peerDependencies || {}),
⋮----
async function buildAll() {
⋮----
console.log('🏗️  Building TENEX CLI...');
⋮----
// Build main entry point
await build({
⋮----
// Build browser entry point (for events export)
⋮----
// Build CLI executable
⋮----
console.log('✅ Build completed successfully!');
⋮----
console.error('❌ Build failed:', error);
process.exit(1);
⋮----
buildAll();
</file>

<file path="src/agents/__tests__/AgentPublisher.test.ts">
import { describe, it, expect, beforeEach, mock, spyOn } from "bun:test";
import { AgentPublisher } from "../AgentPublisher";
import type { AgentConfig } from "../types";
import { EVENT_KINDS } from "@/llm";
import type NDK from "@nostr-dev-kit/ndk";
import { type NDKPrivateKeySigner } from "@nostr-dev-kit/ndk";
⋮----
// Mock implementations
⋮----
// Mock the NDKEvent constructor
⋮----
// Module mock
⋮----
// Mock logger
⋮----
// Import logger after mocking
import { logger } from "@/utils/logger";
⋮----
// Clear all mocks
⋮----
// Reset mock event
⋮----
// Verify NDKEvent was created with correct data
⋮----
// Verify profile content includes all required fields
⋮----
// Verify event was signed and published
⋮----
// Verify NDKEvent was created with correct data
⋮----
// Verify event was signed and published
⋮----
// Verify returned event
⋮----
// Verify e-tag was added for NDKAgent event
⋮----
// Verify both events were created
⋮----
// First call should be profile event (kind 0)
⋮----
// Second call should be request event
⋮----
// Verify both were signed and published
⋮----
// Verify second event (request) has e-tag
⋮----
// Verify request was not attempted after profile failure
⋮----
// First publish succeeds, second fails
⋮----
// Verify both events were attempted
</file>

<file path="src/agents/__tests__/AgentRegistry.republish.test.ts">
import { AgentRegistry } from "@/agents/AgentRegistry";
import { AgentPublisher } from "@/agents/AgentPublisher";
import { getNDK } from "@/nostr";
import { getProjectContext, isProjectContextInitialized, setProjectContext } from "@/services";
import type { NDKPrivateKeySigner } from "@nostr-dev-kit/ndk";
import type { NDKProject } from "@nostr-dev-kit/ndk";
⋮----
// Mock dependencies
⋮----
// Create mock project
⋮----
// Create mock agents
⋮----
// Add agents to registry
⋮----
// Call republishAllAgentProfiles
⋮----
// Verify AgentPublisher was created
⋮----
// Verify publishAgentProfile was called for each agent
⋮----
// Mock project context
⋮----
// Create mock agent
⋮----
// Add agent to registry
⋮----
// Call republishAllAgentProfiles without project
⋮----
// Verify publishAgentProfile was called with context values
⋮----
// Create mock agent
⋮----
// Add agent to registry
⋮----
// Call republishAllAgentProfiles without project
⋮----
// Verify publishAgentProfile was NOT called
⋮----
// Create mock project
⋮----
// Create mock agents
⋮----
// Add agents to registry
⋮----
// Make first call fail
⋮----
// Call republishAllAgentProfiles
⋮----
// Verify both agents were attempted
</file>

<file path="src/agents/__tests__/AgentRegistry.test.ts">
import { describe, it, expect, beforeEach, mock } from "bun:test";
import { AgentRegistry } from "../AgentRegistry";
import type { AgentConfig } from "@/agents/types";
import { NDKPrivateKeySigner } from "@nostr-dev-kit/ndk";
⋮----
import path from "node:path";
import { configService } from "@/services";
import { nip19 } from "nostr-tools";
⋮----
// Mock file system
⋮----
// Mock config service
⋮----
// Mock built-in agents to avoid them being loaded during tests
⋮----
// Reset mocks
⋮----
// Ensure agent developer
⋮----
// Ensure agent reviewer
⋮----
// Should not throw, but set empty registry
⋮----
nsec: "", // Empty nsec
⋮----
// Clear write mock to check it's not called again
⋮----
// Check that writeJsonFile was called for the tester agent
⋮----
// Ensure a developer agent
⋮----
// Ensure agents
</file>

<file path="src/agents/__tests__/mcp-integration.test.ts">
import { describe, it, expect, beforeEach, afterEach, mock, spyOn } from "bun:test";
import { AgentExecutor } from "../execution/AgentExecutor";
import type { Agent } from "../types";
import { MCPService } from "@/services/mcp/MCPService";
import { ConversationManager } from "@/conversations/ConversationManager";
import { loadLLMRouter } from "@/llm";
import type { Tool } from "@/tools/types";
import type { TenexMCP } from "@/services/config/types";
import { configService } from "@/services";
import { NDKPrivateKeySigner } from "@nostr-dev-kit/ndk";
⋮----
// Mock modules
// Mock llm router
⋮----
// Reset MCP service singleton
⋮----
// Create test agent
⋮----
tools: ["read_path"], // Native tool
⋮----
// Create conversation manager
⋮----
// Create executor
⋮----
// Mock MCP configuration
⋮----
// Mock MCP service to return test tools
⋮----
// Get available tools through executor
⋮----
// Should include both native and MCP tools
expect(tools.some((t) => t.name === "read_path")).toBe(true); // Native tool
⋮----
// Should only have native tools
⋮----
expect(tools.every((t) => !t.name.startsWith("mcp__"))).toBe(true); // No MCP tools
⋮----
// Mock MCP tool
⋮----
// Mock LLM response that uses the MCP tool
⋮----
// Verify tool was called
⋮----
// Should handle error gracefully
⋮----
// First LLM call
⋮----
// Second LLM call (after tools)
⋮----
// Native tool and MCP tool with potential conflict
⋮----
name: "mcp__server__read_file", // Namespaced to avoid conflict with native "read_path"
⋮----
// Should have both tools with different names
⋮----
// All MCP tools should have namespace format
⋮----
// Capture the messages sent to LLM
⋮----
// Check that system message includes MCP tools
⋮----
// Tools should be grouped by server in the prompt
</file>

<file path="src/agents/__tests__/tool-assignment.test.ts">
import { describe, it, expect, beforeEach, mock } from "bun:test";
import { AgentRegistry } from "../AgentRegistry";
import { getDefaultToolsForAgent } from "../constants";
import { getBuiltInAgents } from "../builtInAgents";
⋮----
// Both agents get default tools from constants.ts
⋮----
// Planner gets the same default tools
⋮----
// Note: AgentRegistry.ts will remove all tools from these agents
// since they use claude backend, but getDefaultToolsForAgent
// returns the default set for non-orchestrator built-in agents
⋮----
// This test verifies that when creating an agent,
// the isBuiltIn status is determined BEFORE calling getDefaultToolsForAgent
⋮----
// Verify orchestrator is in built-in agents
⋮----
// The fix in AgentRegistry.ts line 212 ensures isBuiltIn is determined before tool assignment
</file>

<file path="src/agents/built-in/executor.ts">
import type { BuiltInAgentDefinition } from "../builtInAgents";
</file>

<file path="src/agents/built-in/orchestrator.ts">
import type { StoredAgentData } from "../types";
⋮----
/**
 * Default orchestrator agent definition
 * This agent represents the orchestrator and has special capabilities
 * like phase transitions and project coordination.
 * Tools are assigned dynamically in AgentRegistry based on isOrchestrator flag
 */
</file>

<file path="src/agents/built-in/planner.ts">
import type { BuiltInAgentDefinition } from "../builtInAgents";
</file>

<file path="src/agents/built-in/project-manager.ts">
import type { StoredAgentData } from "../types";
⋮----
/**
 * Default project manager agent definition
 * This agent represents the project manager focused on deep project knowledge
 * and understanding the project's architecture, dependencies, and context.
 */
</file>

<file path="src/agents/execution/__tests__/AgentExecutor-simple.test.ts">
import { describe, it, expect, beforeEach, mock } from "bun:test";
import { AgentExecutor } from "../AgentExecutor";
import type { Agent } from "@/agents/types";
⋮----
// Create mock agent
⋮----
// Mock the backend modules
⋮----
// Access private method via prototype
⋮----
// Mock the backend modules
⋮----
// Access private method via prototype
⋮----
// Mock the backend modules
⋮----
// Access private method via prototype
</file>

<file path="src/agents/execution/__tests__/AgentExecutor.test.ts">
import { describe, it, expect, beforeEach, afterEach, mock } from "bun:test";
import { AgentExecutor } from "../AgentExecutor";
import { createMockLLMService, MockFactory } from "@/test-utils";
import type { ExecutionContext } from "../types";
import type { Agent } from "@/agents/types";
import type { Message } from "@/llm/types";
import type { ConversationManager } from "@/conversations/ConversationManager";
import { NDK } from "@nostr-dev-kit/ndk";
⋮----
// Mock required modules
⋮----
// Create mock LLM service
⋮----
// Create mock NDK
⋮----
// Create mock conversation manager
⋮----
// Create mock agent
⋮----
// Create mock execution context
⋮----
// Mock the backend modules
⋮----
async execute(messages: Message[], tools: any[], context: ExecutionContext)
⋮----
// Update agent to use reason-act backend
⋮----
// Mock the backend modules
⋮----
// Update agent to use routing backend
⋮----
// Mock required modules
⋮----
constructor(private llm: any, private conversationManager: any)
⋮----
// Mock backend to throw error
⋮----
async execute()
⋮----
expect(true).toBe(false); // Should not reach here
⋮----
// Update agent with unknown backend
⋮----
expect(true).toBe(false); // Should not reach here
⋮----
// Mock tool registry
⋮----
// Mock backend that checks tools
</file>

<file path="src/agents/execution/__tests__/ClaudeBackend.test.ts">
import { describe, expect, test, beforeEach, mock, Mock } from "bun:test";
import type { ExecutionContext } from "../types";
import type { NostrPublisher } from "@/nostr/NostrPublisher";
import type { ConversationManager } from "@/conversations/ConversationManager";
import type { Agent } from "@/agents/types";
import type { Tool } from "@/tools/types";
import type { Message } from "multi-llm-ts";
import type { NDKEvent } from "@nostr-dev-kit/ndk";
⋮----
// Mock dependencies
⋮----
// Mock modules
⋮----
// Import ClaudeBackend after mocks are set up
import { ClaudeBackend } from "../ClaudeBackend";
⋮----
// Reset all mocks
⋮----
// Setup mock agent
⋮----
// Setup mock triggering event
⋮----
// Setup mock conversation manager
⋮----
// Setup mock execution context
⋮----
// Setup mock publisher
⋮----
// Reset orchestrator mock to default success response
⋮----
// Verify orchestrator was called with correct parameters
⋮----
// Verify completion handler was called
⋮----
// Verify session ID was stored
⋮----
// Setup context with existing session ID
⋮----
// Verify orchestrator was called with resumeSessionId
⋮----
// Setup orchestrator to return failure
⋮----
// Setup orchestrator without finalResponse
⋮----
// No finalResponse field
⋮----
// Verify completion handler was called with task content
⋮----
// Setup conversation manager to return null for agent context
⋮----
// Should still complete successfully but not save session ID
⋮----
// saveConversation should not be called when context is null
</file>

<file path="src/agents/execution/__tests__/complete-reminder.test.ts">
import { describe, expect, it, mock, beforeEach } from "bun:test";
import { ReasonActLoop } from "../ReasonActLoop";
import type { LLMService, StreamEvent } from "@/llm/types";
import type { Agent } from "@/agents/types";
import { Message } from "multi-llm-ts";
import { PHASES } from "@/conversations/phases";
⋮----
// Create a mock LLM service
⋮----
// Mock the initial stream response (no complete tool call)
⋮----
// Mock the reminder stream response (with complete tool call)
⋮----
// Set up the mock to return different streams
⋮----
// Create a non-orchestrator agent context
⋮----
// Execute the streaming
⋮----
// Verify that the LLM was called twice
⋮----
// Verify the second call included the reminder message
⋮----
// Mock stream response for orchestrator
⋮----
// Create an orchestrator agent context in CHAT phase
⋮----
phase: PHASES.CHAT, // Chat phase - no reminder expected
⋮----
// Execute the streaming
⋮----
// consume events
⋮----
// Verify that the LLM was only called once (no reminder)
⋮----
// Mock the initial stream response (no complete tool call)
⋮----
// Mock the reminder stream response (still no complete tool call - stubborn agent)
⋮----
// Set up the mock to return different streams
⋮----
// Create a non-orchestrator agent context
⋮----
// Execute the streaming
⋮----
// Verify that the LLM was called twice
⋮----
// Verify that auto-completion was triggered
⋮----
// Mock stream response with complete tool call
⋮----
// Create a non-orchestrator agent context
⋮----
// Execute the streaming
⋮----
// consume events
⋮----
// Verify that the LLM was only called once (no reminder needed)
</file>

<file path="src/agents/execution/__tests__/completionHandler.test.ts">
import { describe, it, expect, beforeEach, mock } from "bun:test";
import { handleAgentCompletion } from "../completionHandler";
import type { NostrPublisher } from "@/nostr/NostrPublisher";
import type { Agent } from "@/agents/types";
import type { NDKEvent } from "@nostr-dev-kit/ndk";
⋮----
// Reset mocks
⋮----
// Setup mock orchestrator agent
⋮----
// Mock ProjectContext
⋮----
// Mock logger
⋮----
// Setup mock publisher
⋮----
// Setup mock agent
⋮----
// Verify the result
⋮----
// Verify publishResponse was called correctly
⋮----
// Verify the result uses the custom summary
⋮----
// Verify publishResponse was called with custom summary
⋮----
// Verify it retrieved the orchestrator
⋮----
// Verify it published to orchestrator's pubkey
⋮----
// The triggering event is passed but not used in the current implementation
// This test ensures it doesn't break when provided
⋮----
// Make publisher throw an error
⋮----
// The function should throw the error up
⋮----
// Should still work with empty response
⋮----
const response = "A".repeat(10000); // Very long response
⋮----
// Should handle long responses without truncation
⋮----
// Verify the exact structure of completeMetadata
</file>

<file path="src/agents/execution/__tests__/ReasonActLoop.errorRecovery.test.ts">
import { describe, it, expect, beforeEach, mock } from "bun:test";
import { ReasonActLoop } from "../ReasonActLoop";
import type { LLMService, StreamEvent } from "@/llm/types";
import { serializeToolResult } from "@/llm/ToolResult";
import type { ToolError } from "@/tools/core";
⋮----
// Reset mocks
⋮----
// Create mocks
⋮----
// Use reflection to access private method
⋮----
// Simulate a tool_complete event with a complex error
⋮----
// Verify publishError was called with the formatted error
⋮----
// Test system error
⋮----
// Reset mock
⋮----
// Test complex error object
⋮----
// Should not throw even if publisher fails
⋮----
// Verify error was logged
⋮----
field: "unknown", // Field is not serialized, so it becomes "unknown"
⋮----
// Test with null result
⋮----
// parseToolResult is called synchronously from handleToolCompleteEvent
// and should throw immediately
⋮----
// Test with missing __typedResult
⋮----
// Test non-orchestrator agent
⋮----
// Reset
⋮----
// Test orchestrator agent (should not add to stream)
⋮----
// Test string error
⋮----
// Test error with message
⋮----
// Test validation error
⋮----
// Test execution error
⋮----
// Test unparseable object
⋮----
// Test other types
</file>

<file path="src/agents/execution/__tests__/ReasonActLoop.orchestrator-reminder.test.ts">
import { describe, it, expect, beforeEach, mock } from "bun:test";
import { ReasonActLoop } from "../ReasonActLoop";
import type { LLMService, StreamEvent } from "@/llm/types";
import type { Agent } from "@/agents/types";
import type { Conversation } from "@/conversations/types";
import { Message } from "multi-llm-ts";
⋮----
// First call returns content without terminal tools
⋮----
// Second call (reminder) should use continue
⋮----
// Set up the mock to return different streams on consecutive calls
⋮----
// Should have called stream twice
⋮----
// Second call should have the reminder
⋮----
// Result should be from the continue tool
⋮----
phase: "chat", // Chat phase - no reminder needed
⋮----
// Should only call stream once (no reminder)
</file>

<file path="src/agents/execution/__tests__/ReasonActLoop.toolError.test.ts">
import { describe, it, expect, mock } from "bun:test";
import type { NostrPublisher } from "@/nostr/NostrPublisher";
import { createTracingLogger } from "@/tracing";
import { ReasonActLoop } from "../ReasonActLoop";
import { serializeToolResult } from "@/llm/ToolResult";
⋮----
// Use reflection to access private method
⋮----
// Simulate a tool_complete event with an error
⋮----
// Verify publishError was called with the correct error message
⋮----
// Use reflection to access private method
⋮----
// Simulate a tool_complete event with success
⋮----
// Verify publishError was NOT called
</file>

<file path="src/agents/execution/__tests__/RoutingBackend.test.ts">
import { describe, expect, it, mock, beforeEach } from "bun:test";
import type { LLMService } from "@/llm/types";
import type { ConversationManager } from "@/conversations/ConversationManager";
import { RoutingBackend } from "../RoutingBackend";
import type { ExecutionContext } from "../types";
import { Message } from "multi-llm-ts";
⋮----
// Mock LLM service
⋮----
// Mock conversation manager
⋮----
// Mock agent executor
⋮----
// Mock project context
⋮----
// Verify LLM was called
⋮----
// Verify agent executor was called
⋮----
phase: "plan", // Different phase
⋮----
phase: "chat", // Current phase
⋮----
// Mock project context
⋮----
// Verify phase transition was called
⋮----
// Mock project context
⋮----
// Verify agent executor was called with executor
⋮----
// Mock project context
⋮----
// Verify both agents were executed
</file>

<file path="src/agents/execution/AgentExecutor.ts">
import type { ConversationManager } from "@/conversations/ConversationManager";
import type { LLMService } from "@/llm/types";
import { NostrPublisher } from "@/nostr";
import { buildSystemPrompt } from "@/prompts/utils/systemPromptBuilder";
import { getProjectContext } from "@/services";
import { mcpService } from "@/services/mcp/MCPService";
import {
    type TracingContext,
    createAgentExecutionContext,
    createTracingContext,
} from "@/tracing";
import { logger } from "@/utils/logger";
import type NDK from "@nostr-dev-kit/ndk";
import type { NDKEvent } from "@nostr-dev-kit/ndk";
import type { Agent } from "@/agents/types";
import { Message } from "multi-llm-ts";
import { ReasonActLoop } from "./ReasonActLoop";
import { ClaudeBackend } from "./ClaudeBackend";
import { RoutingBackend } from "./RoutingBackend";
import type { ExecutionBackend } from "./ExecutionBackend";
import type { ExecutionContext } from "./types";
⋮----
import { startExecutionTime, stopExecutionTime } from "@/conversations/executionTime";
import { createExecutionLogger } from "@/logging/ExecutionLogger";
import type { NDKAgentLesson } from "@/events/NDKAgentLesson";
⋮----
export class AgentExecutor
⋮----
constructor(
⋮----
/**
     * Get the appropriate execution backend based on agent configuration
     */
private getBackend(agent: Agent): ExecutionBackend
⋮----
/**
     * Execute an agent's assignment for a conversation with streaming
     */
async execute(context: ExecutionContext, parentTracingContext?: TracingContext): Promise<void>
⋮----
// Create agent execution tracing context
⋮----
// Build messages first to get the Claude session ID
⋮----
// Get the Claude session ID from the conversation state
⋮----
// Ensure context has publisher and conversationManager
⋮----
agentExecutor: this, // Pass this AgentExecutor instance for continue() tool
claudeSessionId, // Pass the determined session ID
⋮----
// Get fresh conversation data for execution time tracking
⋮----
// Start execution time tracking
⋮----
// Log execution flow start
⋮----
// Publish typing indicator start
⋮----
// Log execution flow complete
⋮----
// Stop typing indicator after successful execution
⋮----
// Clean up the publisher resources
⋮----
// Conversation updates are now handled by NostrPublisher
⋮----
// Log execution flow failure
⋮----
// Stop execution time tracking even on error
⋮----
// Conversation saving is now handled by NostrPublisher
⋮----
// Ensure typing indicator is stopped even on error
⋮----
// Clean up the publisher resources
⋮----
/**
     * Build the messages array for the agent execution
     */
private async buildMessages(
        context: ExecutionContext,
        _triggeringEvent: NDKEvent
): Promise<Message[]>
⋮----
// Get fresh conversation data
⋮----
// Create tag map for efficient lookup
⋮----
// Get all available agents for handoffs
⋮----
// Get MCP tools for the prompt
⋮----
// Build system prompt using the shared function
// Only pass the current agent's lessons
⋮----
// Use the new unified buildAgentMessages method
⋮----
// Add the agent's messages
⋮----
/**
     * Execute with streaming support
     */
private async executeWithStreaming(
        context: ExecutionContext,
        messages: Message[],
        _tracingContext: TracingContext
): Promise<void>
⋮----
// Get tools for response processing - use agent's configured tools
⋮----
// Add MCP tools if available and agent has MCP access
⋮----
// Get the appropriate backend for this agent
⋮----
// Execute using the backend - all backends now use the same interface
</file>

<file path="src/agents/execution/ClaudeBackend.ts">
import type { Tool } from "@/tools/types";
import type { ExecutionBackend } from "./ExecutionBackend";
import type { ExecutionContext } from "./types";
import { handleAgentCompletion } from "./completionHandler";
import { ClaudeTaskOrchestrator } from "@/claude/orchestrator";
import { TaskPublisher } from "@/nostr/TaskPublisher";
import { getNDK } from "@/nostr/ndkClient";
import type { NostrPublisher } from "@/nostr/NostrPublisher";
import { logger } from "@/utils/logger";
import type { Message } from "multi-llm-ts";
⋮----
/**
 * ClaudeBackend executes tasks by directly calling Claude Code
 * and then uses the same completion logic as the complete() tool to return
 * control to the orchestrator.
 */
export class ClaudeBackend implements ExecutionBackend
⋮----
async execute(
        messages: Array<Message>,
        tools: Tool[],
        context: ExecutionContext,
        publisher: NostrPublisher
): Promise<void>
⋮----
// Extract the system prompt from messages
⋮----
// Extract the prompt from the last user message
⋮----
// Create instances for direct Claude Code execution
⋮----
// Create abort controller for this execution
⋮----
// Log if we have a claude session ID to resume
⋮----
// Execute Claude Code directly
⋮----
// Store the Claude session ID using the new updateAgentState method
⋮----
// Use Claude's final response instead of the original task content
⋮----
// Use the same completion handler as the complete() tool
// This will publish the completion event
</file>

<file path="src/agents/execution/completionHandler.ts">
import { getProjectContext } from "@/services/ProjectContext";
import { logger } from "@/utils/logger";
import type { Complete } from "@/tools/types";
import type { NostrPublisher } from "@/nostr/NostrPublisher";
import type { Agent } from "@/agents/types";
import type { NDKEvent } from "@nostr-dev-kit/ndk";
⋮----
/**
 * Shared completion logic used by both the complete() tool and ClaudeBackend
 * Ensures consistent behavior when agents complete their tasks
 */
⋮----
export interface CompletionOptions {
    response: string;
    summary?: string;
    agent: Agent;
    conversationId: string;
    publisher: NostrPublisher;
    triggeringEvent?: NDKEvent;
}
⋮----
/**
 * Handle agent task completion by publishing to orchestrator and logging
 * This is the core logic extracted from the complete() tool
 */
export async function handleAgentCompletion(options: CompletionOptions): Promise<Complete>
⋮----
// Always route completions to the orchestrator for phase control
⋮----
// Publish the completion event
⋮----
// Return the Complete termination
</file>

<file path="src/agents/execution/constants.ts">
/**
 * Configuration constants for agent execution
 */
⋮----
/** Maximum number of attempts to enforce proper termination */
⋮----
/** Delay in milliseconds after publishing typing indicator */
⋮----
/** Default duration for tool execution when not tracked */
</file>

<file path="src/agents/execution/control-flow-types.ts">
import type { ContinueFlow, Complete, EndConversation, RoutingDecision, CompletionSummary, ConversationResult } from "@/tools/core";
⋮----
// Type guards for tool outputs
export function isContinueFlow(output: unknown): output is ContinueFlow
⋮----
export function isRoutingDecision(routing: unknown): routing is RoutingDecision
⋮----
export function isComplete(output: unknown): output is Complete
⋮----
export function isCompletionSummary(completion: unknown): completion is CompletionSummary
⋮----
export function isEndConversation(output: unknown): output is EndConversation
⋮----
export function isConversationResult(result: unknown): result is ConversationResult
</file>

<file path="src/agents/execution/ExecutionBackend.ts">
import type { ExecutionContext } from "./types";
import type { Tool } from "@/tools/types";
import type { NostrPublisher } from "@/nostr/NostrPublisher";
import type { Message } from "multi-llm-ts";
⋮----
/**
 * Interface for agent execution backends.
 * Different backends can implement different execution strategies
 * (e.g., reason-act loops, direct tool execution, etc.)
 */
export interface ExecutionBackend {
    /**
     * Execute the agent's task
     * @param messages - The messages to send to the LLM
     * @param tools - The tools available to the agent
     * @param context - The execution context
     * @param publisher - The NostrPublisher for publishing events
     */
    execute(
        messages: Array<Message>,
        tools: Tool[],
        context: ExecutionContext,
        publisher: NostrPublisher
    ): Promise<void>;
}
⋮----
/**
     * Execute the agent's task
     * @param messages - The messages to send to the LLM
     * @param tools - The tools available to the agent
     * @param context - The execution context
     * @param publisher - The NostrPublisher for publishing events
     */
execute(
        messages: Array<Message>,
        tools: Tool[],
        context: ExecutionContext,
        publisher: NostrPublisher
    ): Promise<void>;
</file>

<file path="src/agents/execution/index.ts">

</file>

<file path="src/agents/execution/ReasonActLoop-old.ts">
import type { CompletionResponse, LLMService, Tool } from "@/llm/types";
import type { StreamEvent } from "@/llm/types";
import type { NostrPublisher } from "@/nostr/NostrPublisher";
import { StreamPublisher } from "@/nostr/NostrPublisher";
import { buildLLMMetadata } from "@/prompts/utils/llmMetadata";
import type { ToolExecutionResult } from "@/tools/types";
import type { TracingContext, TracingLogger } from "@/tracing";
import { createTracingLogger, createTracingContext } from "@/tracing";
import { Message } from "multi-llm-ts";
import { deserializeToolResult, isSerializedToolResult } from "@/llm/ToolResult";
import { getProjectContext } from "@/services/ProjectContext";
import type { ExecutionBackend } from "./ExecutionBackend";
import type { ExecutionContext } from "./types";
import { logger } from "@/utils/logger";
import { createExecutionLogger, type ExecutionLogger } from "@/logging/ExecutionLogger";
import { StreamStateManager } from "./StreamStateManager";
import { isContinueFlow, isComplete, isEndConversation } from "./control-flow-types";
import { formatToolError } from "@/utils/error-formatter";
import { ExecutionConfig } from "./constants";
⋮----
export class ReasonActLoop implements ExecutionBackend
⋮----
constructor(
⋮----
/**
     * ExecutionBackend interface implementation
     */
async execute(
        messages: Array<import("multi-llm-ts").Message>,
        tools: Tool[],
        context: ExecutionContext,
        publisher: NostrPublisher
): Promise<void>
⋮----
// Create tracing context
⋮----
// Create execution logger for structured event logging
⋮----
// Execute the streaming loop and collect results
⋮----
// Drain the generator to make it execute
⋮----
// Execution is complete - all state updates have been handled by the publisher
⋮----
async *executeStreamingInternal(
        context: ExecutionContext,
        messages: Message[],
        tracingContext: TracingContext,
        publisher?: NostrPublisher,
        tools?: Tool[]
): AsyncGenerator<StreamEvent, void, unknown>
⋮----
// Check if this agent requires termination enforcement
⋮----
// Allow up to MAX_TERMINATION_ATTEMPTS attempts for proper termination
⋮----
// Create stream for this attempt
⋮----
: stateManager.getStreamPublisher(); // Reuse existing stream publisher for reminder
⋮----
// Reset state for new attempt (but keep streamPublisher)
⋮----
// Process the stream
⋮----
// Finalize the stream
⋮----
// Check if termination is correct
⋮----
// If terminated properly, we're done
⋮----
// If this is the last attempt, auto-complete
⋮----
// Publish the auto-generated termination event
⋮----
// Otherwise, prepare reminder for next attempt
⋮----
// Execution is complete - all state updates have been handled by the publisher
⋮----
private logStreamingStart(
        tracingLogger: TracingLogger,
        context: ExecutionContext,
        tools?: Tool[],
        stateManager?: StreamStateManager
): void
⋮----
private createLLMStream(
        context: ExecutionContext,
        messages: Message[],
        tools?: Tool[],
        publisher?: NostrPublisher
): ReturnType<LLMService["stream"]>
⋮----
private setupStreamPublisher(
        publisher: NostrPublisher | undefined,
        _tracingLogger: TracingLogger,
        _context: ExecutionContext
): StreamPublisher | undefined
⋮----
private async *processStreamEvents(
        stream: AsyncIterable<StreamEvent>,
        stateManager: StreamStateManager,
        streamPublisher: StreamPublisher | undefined,
        publisher: NostrPublisher | undefined,
        tracingLogger: TracingLogger,
        context: ExecutionContext
): AsyncGenerator<StreamEvent>
⋮----
// If this was a terminal tool, we should stop processing
⋮----
// All terminal tools should stop processing here
// The continue() tool will execute agents in finalizeStream
⋮----
private handleContentEvent(
        event: { content: string },
        stateManager: StreamStateManager,
        streamPublisher?: StreamPublisher,
        context?: ExecutionContext
): void
⋮----
// Extract and log reasoning if present
⋮----
// Orchestrator should remain silent - don't add content to stream
⋮----
private extractAndLogReasoning(content: string, context?: ExecutionContext): void
⋮----
// Extract thinking content
⋮----
// Process each thinking block (in case there are multiple)
⋮----
// Parse structured reasoning
⋮----
// Log agent thinking
⋮----
private parseReasoningContent(content: string):
⋮----
// If no structured reasoning found, use the whole content
⋮----
private async handleToolStartEvent(
        streamPublisher: StreamPublisher | undefined,
        publisher: NostrPublisher | undefined,
        toolName: string,
        toolArgs: Record<string, unknown>,
        stateManager: StreamStateManager,
        tracingLogger: TracingLogger,
        context?: ExecutionContext
): Promise<void>
⋮----
// Create a unique ID for this tool call (based on tool name and current timestamp)
⋮----
// Log tool execution start with ExecutionLogger
⋮----
// Publish typing indicator with tool information
⋮----
// Get the appropriate description function
⋮----
private getToolDescriptions(toolName: string): Record<string, (args: Record<string, unknown>) => string>
⋮----
// File operations
⋮----
// Git operations
⋮----
// Web operations
⋮----
// Documentation
⋮----
// Analysis
⋮----
// Control flow
⋮----
// MCP tools
⋮----
// For MCP tools, try to create a descriptive message
⋮----
private async handleToolCompleteEvent(
        event: { tool: string; result: unknown },
        stateManager: StreamStateManager,
        streamPublisher: StreamPublisher | undefined,
        publisher: NostrPublisher | undefined,
        tracingLogger: TracingLogger,
        context: ExecutionContext
): Promise<boolean>
⋮----
// Check if this tool never sent a tool_start event
⋮----
// Get the tool description function
⋮----
// Note: We don't have args for tools that skip start, so pass empty object
⋮----
// Brief delay to ensure the typing indicator is visible
⋮----
// Log tool execution complete with ExecutionLogger
⋮----
// We don't have the exact start time, so use a reasonable estimate
const duration = ExecutionConfig.DEFAULT_TOOL_DURATION_MS; // Default 1 second, could be improved with tracking
⋮----
// Check if tool execution failed and publish error
⋮----
// Format error message based on error type
⋮----
// Check if this is a terminal tool
⋮----
private parseToolResult(event:
⋮----
// Check if we have a typed result from ToolPlugin
⋮----
// Tool results must include the typed result
⋮----
private processToolResult(
        toolResult: ToolExecutionResult,
        stateManager: StreamStateManager,
        tracingLogger: TracingLogger,
        context: ExecutionContext
): void
⋮----
// Check if it's a continue flow
⋮----
// Only process the first continue
⋮----
// Log routing decision with ExecutionLogger
⋮----
confidence: 0.85 // Could be extracted from reasoning
⋮----
// Increment continue call count
⋮----
// Check if it's a termination (complete or end_conversation)
⋮----
// Log completion decision
⋮----
// Log end conversation decision
⋮----
private isTerminalResult(result: ToolExecutionResult): boolean
⋮----
// Check if it's a control flow or termination
⋮----
private handleDoneEvent(
        event: { response?: CompletionResponse },
        stateManager: StreamStateManager,
        _tracingLogger: TracingLogger
): void
⋮----
private handleErrorEvent(
        event: { error: string },
        stateManager: StreamStateManager,
        streamPublisher: StreamPublisher | undefined,
        tracingLogger: TracingLogger,
        context?: ExecutionContext
): void
⋮----
// Orchestrator should remain silent - don't add error content to stream
⋮----
private async finalizeStream(
        streamPublisher: StreamPublisher | undefined,
        stateManager: StreamStateManager,
        context: ExecutionContext,
        messages: Message[],
        _tracingLogger: TracingLogger,
        _publisher?: NostrPublisher
): Promise<void>
⋮----
// Convert flow/termination to metadata for finalization
⋮----
// Orchestrator should remain silent - only finalize if there's metadata (terminal tools)
// Skip finalization if orchestrator only has content with no terminal tool
⋮----
private getReminderMessage(context: ExecutionContext): string
⋮----
private autoCompleteTermination(
        stateManager: StreamStateManager,
        context: ExecutionContext,
        tracingLogger: TracingLogger
): void
⋮----
// For orchestrator, we need to auto-route somewhere
// This is a fallback - orchestrator should always use continue()
⋮----
// We can't auto-complete for orchestrator since it needs to route
⋮----
// For non-orchestrator, complete back to orchestrator
⋮----
private createFinalEvent(stateManager: StreamStateManager): StreamEvent
⋮----
// Add additional properties that AgentExecutor expects
⋮----
private async *handleStreamingError(
        error: unknown,
        publisher: NostrPublisher | undefined,
        streamPublisher: StreamPublisher | undefined,
        tracingLogger: TracingLogger,
        context: ExecutionContext
): AsyncGenerator<StreamEvent>
</file>

<file path="src/agents/execution/ReasonActLoop.ts">
import type { CompletionResponse, LLMService, Tool } from "@/llm/types";
import type { StreamEvent } from "@/llm/types";
import type { NostrPublisher } from "@/nostr/NostrPublisher";
import { StreamPublisher } from "@/nostr/NostrPublisher";
import { buildLLMMetadata } from "@/prompts/utils/llmMetadata";
import type { TracingContext, TracingLogger } from "@/tracing";
import { createTracingLogger, createTracingContext } from "@/tracing";
import { Message } from "multi-llm-ts";
import type { ConversationManager } from "@/conversations/ConversationManager";
import type { ExecutionBackend } from "./ExecutionBackend";
import type { ExecutionContext } from "./types";
import { logger } from "@/utils/logger";
import { createExecutionLogger, type ExecutionLogger } from "@/logging/ExecutionLogger";
import { StreamStateManager } from "./StreamStateManager";
import { ToolStreamHandler } from "./ToolStreamHandler";
import { TerminationHandler } from "./TerminationHandler";
import { ExecutionConfig } from "./constants";
⋮----
/**
 * Simplified ReasonActLoop implementation using extracted handlers.
 * Orchestrates the main LLM streaming loop without complex nested logic.
 */
export class ReasonActLoop implements ExecutionBackend
⋮----
constructor(
⋮----
/**
     * ExecutionBackend interface implementation
     */
async execute(
        messages: Array<Message>,
        tools: Tool[],
        context: ExecutionContext,
        publisher: NostrPublisher
): Promise<void>
⋮----
// Execute the streaming loop
⋮----
// Drain the generator
⋮----
async *executeStreamingInternal(
        context: ExecutionContext,
        messages: Message[],
        tracingContext: TracingContext,
        publisher?: NostrPublisher,
        tools?: Tool[]
): AsyncGenerator<StreamEvent, void, unknown>
⋮----
// Initialize handlers
⋮----
// Main termination loop
⋮----
// Reset state for retry (but keep stream publisher)
⋮----
// Create and process stream
⋮----
// Process stream events
⋮----
// Finalize stream
⋮----
// Check if should retry for termination
⋮----
// Prepare for retry with reminder
⋮----
private async *processStream(
        stream: AsyncIterable<StreamEvent>,
        stateManager: StreamStateManager,
        toolHandler: ToolStreamHandler,
        streamPublisher: StreamPublisher | undefined,
        publisher: NostrPublisher | undefined,
        tracingLogger: TracingLogger,
        context: ExecutionContext
): AsyncGenerator<StreamEvent>
⋮----
private handleContentEvent(
        event: { content: string },
        stateManager: StreamStateManager,
        streamPublisher?: StreamPublisher,
        context?: ExecutionContext
): void
⋮----
// Extract and log reasoning if present
⋮----
// Orchestrator should remain silent
⋮----
private handleErrorEvent(
        event: { error: string },
        stateManager: StreamStateManager,
        streamPublisher: StreamPublisher | undefined,
        tracingLogger: TracingLogger,
        context?: ExecutionContext
): void
⋮----
// Orchestrator should remain silent
⋮----
private async finalizeStream(
        streamPublisher: StreamPublisher | undefined,
        stateManager: StreamStateManager,
        context: ExecutionContext,
        messages: Message[],
        _tracingLogger: TracingLogger
): Promise<void>
⋮----
// Build metadata for finalization
⋮----
// Only finalize if there's content or metadata
⋮----
private createLLMStream(
        context: ExecutionContext,
        messages: Message[],
        tools?: Tool[],
        publisher?: NostrPublisher
): ReturnType<LLMService["stream"]>
⋮----
private createStreamPublisher(publisher: NostrPublisher | undefined): StreamPublisher | undefined
⋮----
private createFinalEvent(stateManager: StreamStateManager): StreamEvent
⋮----
// Add additional properties for AgentExecutor
⋮----
private async *handleError(
        error: unknown,
        publisher: NostrPublisher | undefined,
        stateManager: StreamStateManager,
        tracingLogger: TracingLogger,
        context: ExecutionContext
): AsyncGenerator<StreamEvent>
⋮----
private logExecutionStart(
        tracingLogger: TracingLogger,
        context: ExecutionContext,
        tools?: Tool[]
): void
⋮----
private extractAndLogReasoning(content: string, context?: ExecutionContext, stateManager?: StreamStateManager): void
⋮----
// Extract thinking content
⋮----
// Process each thinking block
⋮----
// Check if this block has already been logged
⋮----
return; // Skip already logged blocks
⋮----
// Mark this block as logged
⋮----
private parseReasoningContent(content: string):
⋮----
// If no structured reasoning found, use the whole content
</file>

<file path="src/agents/execution/RoutingBackend.ts">
import type { LLMService } from "@/llm/types";
import type { NostrPublisher, TenexLogData } from "@/nostr/NostrPublisher";
import type { Tool } from "@/tools/types";
import { getProjectContext } from "@/services/ProjectContext";
import { createTracingLogger } from "@/tracing";
import { Message } from "multi-llm-ts";
import { z } from "zod";
import type { ConversationManager } from "@/conversations/ConversationManager";
import type { ExecutionBackend } from "./ExecutionBackend";
import type { ExecutionContext } from "./types";
import { createExecutionLogger, type ExecutionLogger } from "@/logging/ExecutionLogger";
⋮----
// Schema for routing decisions
⋮----
type RoutingDecision = z.infer<typeof RoutingDecisionSchema>;
⋮----
export class RoutingBackend implements ExecutionBackend
⋮----
constructor(
⋮----
async execute(
        messages: Message[],
        tools: Tool[], // Ignored - routing backend doesn't use tools
        context: ExecutionContext,
        publisher: NostrPublisher
): Promise<void>
⋮----
tools: Tool[], // Ignored - routing backend doesn't use tools
⋮----
// Log routing analysis start
⋮----
// Get routing decision from LLM
⋮----
// Log routing decision with ExecutionLogger
⋮----
// Update phase if transitioning
⋮----
routingDecision.phase as any, // TODO: proper phase type
⋮----
// Get the AgentExecutor from context
⋮----
// Execute target agents
⋮----
// Handle special END agent to cleanly terminate conversation
⋮----
// Log the end of conversation
⋮----
// Don't execute any more agents
⋮----
// Find agent by slug (case-insensitive)
⋮----
// If not found, try case-insensitive search
⋮----
// Create a new publisher for the target agent
⋮----
// Continue with other agents even if one fails
⋮----
private async getRoutingDecision(
        messages: Message[], 
        context: ExecutionContext,
        tracingLogger: any,
        executionLogger?: ExecutionLogger
): Promise<RoutingDecision>
⋮----
// Add instruction to return JSON only
⋮----
// Use regular completion but parse the JSON response
⋮----
temperature: 0.3, // Lower temperature for consistent routing
⋮----
// Extract and log reasoning if present in the response
⋮----
// Parse the JSON response
⋮----
// Extract JSON if it's wrapped in markdown code blocks
⋮----
// Validate with Zod schema
</file>

<file path="src/agents/execution/StreamStateManager.ts">
import type { ToolExecutionResult, ContinueFlow, Complete, EndConversation } from "@/tools/types";
import type { CompletionResponse } from "@/llm/types";
import type { StreamPublisher } from "@/nostr/NostrPublisher";
⋮----
/**
 * Represents the mutable state during stream processing
 */
export interface StreamingState {
    allToolResults: ToolExecutionResult[];
    continueFlow: ContinueFlow | undefined;
    termination: Complete | EndConversation | undefined;
    finalResponse: CompletionResponse | undefined;
    fullContent: string;
    streamPublisher: StreamPublisher | undefined;
    startedTools: Set<string>;
    loggedThinkingBlocks: Set<string>;
}
⋮----
/**
 * Manages the mutable state during LLM stream processing.
 * Provides controlled access and modifications to the streaming state.
 */
export class StreamStateManager
⋮----
constructor()
⋮----
/**
     * Create a fresh initial state
     */
private createInitialState(): StreamingState
⋮----
/**
     * Reset the state to initial values
     */
reset(): void
⋮----
/**
     * Reset state for a retry attempt (keeps streamPublisher)
     */
resetForRetry(): void
⋮----
/**
     * Append content to the accumulated full content
     */
appendContent(content: string): void
⋮----
/**
     * Get the current full content
     */
getFullContent(): string
⋮----
/**
     * Add a tool execution result
     */
addToolResult(result: ToolExecutionResult): void
⋮----
/**
     * Get all tool results
     */
getToolResults(): ToolExecutionResult[]
⋮----
/**
     * Set the continue flow (only if not already set)
     */
setContinueFlow(flow: ContinueFlow): boolean
⋮----
return false; // Already set, ignore
⋮----
/**
     * Get the continue flow
     */
getContinueFlow(): ContinueFlow | undefined
⋮----
/**
     * Set the termination (complete or end_conversation)
     */
setTermination(termination: Complete | EndConversation): void
⋮----
/**
     * Get the termination
     */
getTermination(): Complete | EndConversation | undefined
⋮----
/**
     * Check if the stream has terminated (either continue flow or termination)
     */
hasTerminated(): boolean
⋮----
/**
     * Set the final response from the LLM
     */
setFinalResponse(response: CompletionResponse): void
⋮----
/**
     * Get the final response
     */
getFinalResponse(): CompletionResponse | undefined
⋮----
/**
     * Set the stream publisher
     */
setStreamPublisher(publisher: StreamPublisher): void
⋮----
/**
     * Get the stream publisher
     */
getStreamPublisher(): StreamPublisher | undefined
⋮----
/**
     * Mark a tool as started
     */
markToolStarted(toolCallId: string): void
⋮----
/**
     * Check if a tool has been started
     */
hasToolStarted(toolNamePattern: string): boolean
⋮----
/**
     * Mark a thinking block as logged (using its content hash)
     */
markThinkingBlockLogged(blockContent: string): void
⋮----
/**
     * Check if a thinking block has already been logged
     */
hasThinkingBlockBeenLogged(blockContent: string): boolean
⋮----
/**
     * Get the raw state (use sparingly, prefer specific methods)
     */
getState(): Readonly<StreamingState>
⋮----
/**
     * Get a summary of the current state for logging
     */
getStateSummary(): Record<string, unknown>
</file>

<file path="src/agents/execution/TerminationHandler.ts">
import type { TracingLogger } from "@/tracing";
import type { ExecutionContext } from "./types";
import type { Message } from "multi-llm-ts";
import { StreamStateManager } from "./StreamStateManager";
import { ExecutionConfig } from "./constants";
import { getProjectContext } from "@/services/ProjectContext";
import { logger } from "@/utils/logger";
⋮----
/**
 * Handles termination logic for agent execution.
 * Responsible for enforcing proper termination, generating reminder messages,
 * and auto-completing when agents fail to terminate properly.
 */
export class TerminationHandler
⋮----
constructor(private stateManager: StreamStateManager)
⋮----
/**
     * Check if termination is required and enforce it if necessary
     * @returns true if should continue with another attempt, false if properly terminated
     */
shouldRetryForTermination(
        context: ExecutionContext,
        attempt: number,
        tracingLogger: TracingLogger
): boolean
⋮----
// Check if this agent requires termination enforcement
⋮----
// If terminated properly or termination not required, we're done
⋮----
// If we haven't reached max attempts, retry
⋮----
// Max attempts reached - auto-complete
⋮----
/**
     * Get reminder message for agents that didn't terminate properly
     */
getReminderMessage(context: ExecutionContext): string
⋮----
/**
     * Prepare messages for retry attempt with reminder
     */
prepareRetryMessages(
        currentMessages: Message[],
        context: ExecutionContext,
        tracingLogger: TracingLogger
): Message[]
⋮----
/**
     * Auto-complete termination when agent fails to call terminal tool
     */
private autoCompleteTermination(
        context: ExecutionContext,
        tracingLogger: TracingLogger
): void
⋮----
// For orchestrator, we can't auto-complete since it needs to route
// This is a critical error - orchestrator must always route
⋮----
// For non-orchestrator, complete back to orchestrator
⋮----
/**
     * Check if the current phase/agent combination requires termination enforcement
     */
requiresTerminationEnforcement(context: ExecutionContext): boolean
⋮----
/**
     * Log termination attempt information
     */
logTerminationAttempt(
        attempt: number,
        context: ExecutionContext,
        tracingLogger: TracingLogger
): void
</file>

<file path="src/agents/execution/ToolStreamHandler.ts">
import type { NostrPublisher, StreamPublisher } from "@/nostr/NostrPublisher";
import type { TracingLogger } from "@/tracing";
import type { ExecutionContext } from "./types";
import type { ExecutionLogger } from "@/logging/ExecutionLogger";
import type { ToolExecutionResult } from "@/tools/executor";
import { StreamStateManager } from "./StreamStateManager";
import { ExecutionConfig } from "./constants";
import { formatToolError } from "@/utils/error-formatter";
import { deserializeToolResult, isSerializedToolResult } from "@/llm/ToolResult";
import { isContinueFlow, isComplete, isEndConversation } from "./control-flow-types";
import { logger } from "@/utils/logger";
⋮----
/**
 * Handles tool-related events in the LLM stream.
 * Responsible for processing tool_start and tool_complete events,
 * managing tool descriptions, publishing typing indicators, and error handling.
 */
export class ToolStreamHandler
⋮----
constructor(
⋮----
/**
     * Handle a tool_start event
     */
async handleToolStartEvent(
        streamPublisher: StreamPublisher | undefined,
        publisher: NostrPublisher | undefined,
        toolName: string,
        toolArgs: Record<string, unknown>,
        tracingLogger: TracingLogger,
        context?: ExecutionContext
): Promise<void>
⋮----
// Create a unique ID for this tool call
⋮----
// Log tool execution start
⋮----
// Flush stream for non-continue tools
⋮----
// Publish typing indicator with tool information
⋮----
/**
     * Handle a tool_complete event
     * @returns true if this was a terminal tool (continue, complete, end_conversation)
     */
async handleToolCompleteEvent(
        event: { tool: string; result: unknown },
        streamPublisher: StreamPublisher | undefined,
        publisher: NostrPublisher | undefined,
        tracingLogger: TracingLogger,
        context: ExecutionContext
): Promise<boolean>
⋮----
// Parse the tool result first to get metadata
⋮----
// Check if this tool never sent a tool_start event
// Pass the tool result so we can use metadata if available
⋮----
// Add result to state
⋮----
// Log tool execution complete
⋮----
// Publish error if tool failed
⋮----
// Process the tool result (update state with continue/termination)
⋮----
// Flush stream and stop typing indicator
⋮----
// Check if this is a terminal tool
⋮----
/**
     * Check if tool never sent a start event and handle it
     */
private async handleMissingToolStart(
        toolName: string,
        toolResult: ToolExecutionResult,
        publisher: NostrPublisher | undefined,
        tracingLogger: TracingLogger,
        context: ExecutionContext
): Promise<void>
⋮----
// First try to use metadata from the tool result
⋮----
// Try to generate a message from executed args
⋮----
// Fall back to generic message
⋮----
// Brief delay to ensure the typing indicator is visible
⋮----
/**
     * Parse tool result from event
     */
private parseToolResult(event:
⋮----
// Tool results must include the typed result
⋮----
/**
     * Log tool completion with ExecutionLogger
     */
private logToolComplete(
        toolResult: ToolExecutionResult,
        toolName: string,
        context: ExecutionContext
): void
⋮----
// We don't have the exact start time, so use a reasonable estimate
⋮----
/**
     * Publish tool error if execution failed
     */
private async publishToolError(
        toolResult: ToolExecutionResult,
        toolName: string,
        publisher: NostrPublisher | undefined,
        tracingLogger: TracingLogger
): Promise<void>
⋮----
/**
     * Process tool result and update state
     */
private processToolResult(
        toolResult: ToolExecutionResult,
        tracingLogger: TracingLogger,
        context: ExecutionContext
): void
⋮----
// Check if it's a continue flow
⋮----
// Log routing decision
⋮----
// Check if it's a termination
⋮----
/**
     * Check if tool result is terminal (continue, complete, or end_conversation)
     */
private isTerminalResult(result: ToolExecutionResult): boolean
⋮----
/**
     * Get human-readable description for a tool
     */
private getToolDescription(toolName: string, args: Record<string, unknown>): string
⋮----
/**
     * Tool description generators
     */
private getToolDescriptions(): Record<string, (args: Record<string, unknown>) => string>
⋮----
// File operations
⋮----
// Git operations
⋮----
// Web operations
⋮----
// Documentation
⋮----
// Analysis
⋮----
// Control flow
⋮----
// MCP tools
⋮----
// For MCP tools, try to create a descriptive message
</file>

<file path="src/agents/execution/types.ts">
import type { Agent } from "@/agents/types";
import type { Phase } from "@/conversations/phases";
import type { PhaseTransition } from "@/conversations/types";
import type { ToolExecutionResult } from "@/tools/types";
import type { NDKEvent } from "@nostr-dev-kit/ndk";
import type { NostrPublisher } from "@/nostr/NostrPublisher";
import type { ConversationManager } from "@/conversations/ConversationManager";
import type { AgentExecutor } from "@/agents/execution/AgentExecutor";
⋮----
export interface ExecutionContext {
    agent: Agent;
    conversationId: string;
    phase: Phase;
    projectPath: string;
    triggeringEvent: NDKEvent;
    publisher: NostrPublisher;
    conversationManager: ConversationManager;
    previousPhase?: Phase;
    handoff?: PhaseTransition;
    claudeSessionId?: string;
    agentExecutor?: AgentExecutor;
}
⋮----
export interface AgentExecutionResult {
    success: boolean;
    response?: string;
    toolExecutions?: ToolExecutionResult[];
    error?: string;
}
</file>

<file path="src/agents/AgentPublisher.ts">
import type { AgentConfig } from "@/agents/types";
import { EVENT_KINDS } from "@/llm";
import { logger } from "@/utils/logger";
import type NDK from "@nostr-dev-kit/ndk";
import { NDKEvent, type NDKPrivateKeySigner } from "@nostr-dev-kit/ndk";
⋮----
/**
 * Service for publishing agent-related Nostr events
 */
export class AgentPublisher
⋮----
constructor(private ndk: NDK)
⋮----
/**
     * Publishes a kind:0 profile event for an agent
     */
async publishAgentProfile(
        signer: NDKPrivateKeySigner,
        agentName: string,
        agentRole: string,
        projectName: string,
        projectPubkey: string
): Promise<void>
⋮----
// Generate random dicebear avatar
const avatarStyle = "bottts"; // Using bottts style for agents
const seed = signer.pubkey; // Use pubkey as seed for consistent avatar
⋮----
/**
     * Publishes an agent request event
     */
async publishAgentRequest(
        signer: NDKPrivateKeySigner,
        agentConfig: Omit<AgentConfig, "nsec">,
        projectPubkey: string,
        ndkAgentEventId?: string
): Promise<NDKEvent>
⋮----
// Only add e-tag if this agent was created from an NDKAgent event
⋮----
// Add agent metadata tags
⋮----
/**
     * Publishes all agent-related events when creating a new agent
     */
async publishAgentCreation(
        signer: NDKPrivateKeySigner,
        agentConfig: Omit<AgentConfig, "nsec">,
        projectName: string,
        projectPubkey: string,
        ndkAgentEventId?: string
): Promise<void>
⋮----
// Publish profile event
⋮----
// Publish request event
</file>

<file path="src/agents/AgentRegistry.ts">
import fs from "node:fs/promises";
import path from "node:path";
import { AgentPublisher } from "@/agents/AgentPublisher";
import type { Agent, AgentConfig, AgentConfigOptionalNsec, StoredAgentData } from "@/agents/types";
import { ensureDirectory, fileExists, readFile, writeJsonFile } from "@/lib/fs";
import { DEFAULT_AGENT_LLM_CONFIG } from "@/llm/constants";
import { getNDK } from "@/nostr";
import { configService } from "@/services";
import { getProjectContext, isProjectContextInitialized } from "@/services";
import type { TenexAgents } from "@/services/config/types";
import { logger } from "@/utils/logger";
import { NDKPrivateKeySigner } from "@nostr-dev-kit/ndk";
import type { NDKProject } from "@nostr-dev-kit/ndk";
import { getBuiltInAgents } from "./builtInAgents";
import { getDefaultToolsForAgent } from "./constants";
import { isToollessBackend } from "./utils";
⋮----
export class AgentRegistry
⋮----
constructor(
        private basePath: string,
        isGlobal = false
)
⋮----
// If basePath already includes .tenex, use it as is
⋮----
async loadFromProject(ndkProject?: NDKProject): Promise<void>
⋮----
// Ensure .tenex directory exists
⋮----
// Load agents using ConfigService
⋮----
// Load global agents first if we're in a project context
⋮----
// Load project/local agents
⋮----
// Load global agents first (if in project context)
⋮----
// Load project/local agents (these can override global ones)
⋮----
// Load built-in agents
⋮----
async ensureAgent(
        name: string,
        config: AgentConfigOptionalNsec,
        ndkProject?: NDKProject
): Promise<Agent>
⋮----
// Check if agent already exists
⋮----
// Check if we have it in registry
⋮----
// Generate new nsec for agent
⋮----
// Create new registry entry
⋮----
// Only add eventId if it exists
⋮----
// Check if this is a built-in agent
⋮----
// Save agent definition to file
⋮----
// Only include tools if explicitly provided
⋮----
// For built-in agents, we don't save instructions to the JSON file
// This ensures built-in agents always use the up-to-date instructions
// from the code rather than potentially outdated instructions in the file
⋮----
// Publish kind:0 and request events for new agent
⋮----
// Load agent definition from file
⋮----
// For built-in agents, merge missing fields from TypeScript definition before validation
⋮----
// Fill in missing required fields from built-in definition
⋮----
// Check if this is a built-in agent
⋮----
// Fallback: create definition from config if file doesn't exist
⋮----
// Create NDKPrivateKeySigner
⋮----
// Determine agent name - use project name for project-manager agent
⋮----
// If project context not available, use default name
⋮----
// Keep support for custom orchestrator agents using project name
⋮----
// If project context not available, use default name
⋮----
// Determine if this is a built-in agent early
⋮----
// Create Agent instance with all properties set
⋮----
tools: [], // Will be set next
mcp: agentDefinition.mcp ?? !registryEntry.orchestratorAgent, // Default to true for non-orchestrator agents
⋮----
isBuiltIn: isBuiltIn, // Set the isBuiltIn property here
backend: agentDefinition.backend, // Propagate backend configuration
⋮----
// Set tools - use explicit tools if configured, otherwise use defaults
⋮----
// Claude and routing backend agents don't use tools through the traditional tool system
⋮----
// Convert tool names to Tool instances
⋮----
// Store in both maps
⋮----
getAgent(name: string): Agent | undefined
⋮----
getAgentByPubkey(pubkey: string): Agent | undefined
⋮----
getAllAgents(): Agent[]
⋮----
getAllAgentsMap(): Map<string, Agent>
⋮----
getAgentByName(name: string): Agent | undefined
⋮----
private async saveRegistry(): Promise<void>
⋮----
/**
     * Remove an agent by its event ID
     * This removes the agent from memory and deletes its definition file
     */
async removeAgentByEventId(eventId: string): Promise<boolean>
⋮----
// Find the agent with this event ID
⋮----
// Don't allow removing built-in agents
⋮----
// Remove from memory
⋮----
// Remove from registry
⋮----
// Delete the agent definition file
⋮----
// Remove from registry and save
⋮----
/**
     * Remove an agent by its slug
     * This removes the agent from memory and deletes its definition file
     */
async removeAgentBySlug(slug: string): Promise<boolean>
⋮----
// Don't allow removing built-in agents
⋮----
// Remove from memory
⋮----
// Remove from registry
⋮----
// Delete the agent definition file
⋮----
// Remove from registry and save
⋮----
/**
     * Get the orchestrator agent if one exists
     */
getOrchestratorAgent(): Agent | undefined
⋮----
// Look for the orchestrator by slug first (for built-in orchestrator)
⋮----
// Fallback to looking for any agent marked as orchestrator
⋮----
private async publishAgentEvents(
        signer: NDKPrivateKeySigner,
        config: Omit<AgentConfig, "nsec">,
        ndkAgentEventId?: string,
        ndkProject?: NDKProject
): Promise<void>
⋮----
// Use passed NDKProject if available, otherwise fall back to ProjectContext
⋮----
// Check if project context is initialized
⋮----
// Get project context for project pubkey and name
⋮----
// Create agent publisher
⋮----
// Publish agent profile (kind:0) and request event
⋮----
// Don't throw - agent creation should succeed even if publishing fails
⋮----
async loadAgentBySlug(slug: string, fromGlobal = false): Promise<Agent | null>
⋮----
// Determine the correct agents directory
⋮----
// Load agent definition from file
⋮----
// For built-in agents, merge missing fields from TypeScript definition before validation
⋮----
// Fill in missing required fields from built-in definition
⋮----
// Create AgentConfig from definition
⋮----
tools: agentDefinition.tools, // Preserve explicit tools configuration
mcp: agentDefinition.mcp, // Preserve MCP configuration
⋮----
/**
     * Validate an agent definition has all required fields
     */
private validateAgentDefinition(definition: unknown): asserts definition is StoredAgentData
⋮----
// Optional fields with type validation
// Note: instructions is optional for built-in agents
⋮----
/**
     * Ensure built-in agents are loaded
     */
private async ensureBuiltInAgents(
        ndkProject?: NDKProject
): Promise<void>
⋮----
// Check if this is the orchestrator
⋮----
// Use ensureAgent just like any other agent
⋮----
mcp: !isOrchestrator, // Default: true for all agents except orchestrator
⋮----
// Mark as built-in and set orchestrator flag
⋮----
// Update registry to mark orchestrator
⋮----
/**
     * Republish kind:0 events for all agents
     * This is called when the project boots to ensure agents are discoverable
     */
async republishAllAgentProfiles(ndkProject?: NDKProject): Promise<void>
⋮----
// Use passed NDKProject if available, otherwise fall back to ProjectContext
⋮----
// Check if project context is initialized
⋮----
// Get project context for project pubkey and name
⋮----
// Republish kind:0 for each agent
⋮----
// Continue with other agents even if one fails
</file>

<file path="src/agents/builtInAgents.ts">
import { DEFAULT_AGENT_LLM_CONFIG } from "@/llm/constants";
import { EXECUTOR_AGENT } from "./built-in/executor";
import { ORCHESTRATOR_AGENT_DEFINITION } from "./built-in/orchestrator";
import { PLANNER_AGENT } from "./built-in/planner";
import { PROJECT_MANAGER_AGENT_DEFINITION } from "./built-in/project-manager";
⋮----
export interface BuiltInAgentDefinition {
    name: string;
    slug: string;
    role: string;
    instructions: string;
    llmConfig?: string;
    backend?: "reason-act-loop" | "claude" | "routing";
    useCriteria?: string;
}
⋮----
export function getBuiltInAgents(): BuiltInAgentDefinition[]
</file>

<file path="src/agents/constants.ts">
import type { Agent } from "./types";
import { analyze } from "../tools/implementations/analyze";
import { continueTool } from "../tools/implementations/continue";
import { generateInventoryTool } from "../tools/implementations/generateInventory";
import { learnTool } from "../tools/implementations/learn";
import { readPathTool } from "../tools/implementations/readPath";
import { writeContextFileTool } from "@/tools/implementations/writeContextFile";
import { completeTool } from "../tools/implementations/complete";
import { shellTool } from "../tools/implementations/shell";
⋮----
/**
 * Get all available tools for an agent based on their role
 * All agents now have access to all tools except orchestrator-only tools
 */
export function getDefaultToolsForAgent(agent: Agent): string[]
⋮----
// Built-in agents
⋮----
// Orchestrator with routing backend doesn't need any tools
⋮----
// Legacy orchestrator with reason-act-loop gets continue tool
⋮----
// Other non-orchestrator agents use complete tool to signal task completion
⋮----
// Custom agents default to complete tool
</file>

<file path="src/agents/index.ts">

</file>

<file path="src/agents/types.ts">
import type { Phase } from "@/conversations/phases";
import type { Conversation } from "@/conversations/types";
import type { Tool } from "@/tools/types";
import type { NDKEvent, NDKPrivateKeySigner } from "@nostr-dev-kit/ndk";
⋮----
export interface AgentSummary {
    name: string;
    role: string;
    pubkey: string;
}
⋮----
export interface Agent {
    name: string;
    pubkey: string;
    signer: NDKPrivateKeySigner;
    role: string;
    description?: string; // Agent description from NDKAgent event
    instructions?: string;
    useCriteria?: string; // Criteria for when this agent should be selected
    llmConfig: string;
    tools: Tool[]; // Actual tool instances
    mcp?: boolean; // Whether this agent has access to MCP tools (defaults to true except for orchestrator)
    eventId?: string; // NDKAgent event ID
    slug: string; // Agent slug/key from agents.json
    isOrchestrator?: boolean; // Whether this agent is the orchestrator agent
    isBuiltIn?: boolean; // Whether this is a built-in agent (executor, planner)
    backend?: "reason-act-loop" | "claude" | "routing"; // Execution backend to use (defaults to 'reason-act-loop')
}
⋮----
description?: string; // Agent description from NDKAgent event
⋮----
useCriteria?: string; // Criteria for when this agent should be selected
⋮----
tools: Tool[]; // Actual tool instances
mcp?: boolean; // Whether this agent has access to MCP tools (defaults to true except for orchestrator)
eventId?: string; // NDKAgent event ID
slug: string; // Agent slug/key from agents.json
isOrchestrator?: boolean; // Whether this agent is the orchestrator agent
isBuiltIn?: boolean; // Whether this is a built-in agent (executor, planner)
backend?: "reason-act-loop" | "claude" | "routing"; // Execution backend to use (defaults to 'reason-act-loop')
⋮----
export interface ToolCallArguments {
    // Common tool arguments
    command?: string; // For shell tools
    path?: string; // For file tools
    mode?: string; // For claude_code tool
    prompt?: string; // For claude_code tool

    // Allow other tool arguments
    [key: string]: string | number | boolean | undefined;
}
⋮----
// Common tool arguments
command?: string; // For shell tools
path?: string; // For file tools
mode?: string; // For claude_code tool
prompt?: string; // For claude_code tool
⋮----
// Allow other tool arguments
⋮----
export interface ToolCall {
    tool: string;
    args: ToolCallArguments;
    id?: string;
}
⋮----
/**
 * Configuration load options
 */
export interface ConfigurationLoadOptions {
    skipGlobal?: boolean;
}
⋮----
/**
 * Agent data stored in JSON files (.tenex/agents/*.json)
 */
export interface StoredAgentData {
    name: string;
    role: string;
    description?: string;
    instructions?: string;
    useCriteria?: string;
    llmConfig?: string;
    tools?: string[]; // Tool names in storage - converted to Tool instances at runtime
    mcp?: boolean; // Whether this agent has access to MCP tools
    backend?: "reason-act-loop" | "claude" | "routing"; // Execution backend to use (defaults to 'reason-act-loop')
}
⋮----
tools?: string[]; // Tool names in storage - converted to Tool instances at runtime
mcp?: boolean; // Whether this agent has access to MCP tools
backend?: "reason-act-loop" | "claude" | "routing"; // Execution backend to use (defaults to 'reason-act-loop')
⋮----
/**
 * Agent configuration including sensitive data from registry
 */
export interface AgentConfig extends StoredAgentData {
    nsec: string; // Private key from agents.json registry
    eventId?: string; // NDKAgent event ID if created from Nostr event
    pubkey?: string; // Public key derived from nsec
}
⋮----
nsec: string; // Private key from agents.json registry
eventId?: string; // NDKAgent event ID if created from Nostr event
pubkey?: string; // Public key derived from nsec
⋮----
/**
 * Agent config for creation with optional nsec
 */
export interface AgentConfigOptionalNsec extends StoredAgentData {
    nsec?: string; // Optional during creation
    eventId?: string;
    pubkey?: string;
}
⋮----
nsec?: string; // Optional during creation
⋮----
/**
 * Agent configuration for orchestration system
 */
export interface AgentConfiguration {
    name: string;
    nsec: string;
    eventId?: string;
    role?: string;
}
⋮----
/**
 * Project agents configuration
 */
export interface ProjectAgentsConfig {
    agents: Record<string, AgentConfiguration>;
}
</file>

<file path="src/agents/utils.ts">
import type { Agent } from "./types";
⋮----
export const isClaudeBackend = (agent: Agent): boolean
⋮----
export const isRoutingBackend = (agent: Agent): boolean
⋮----
export const isToollessBackend = (agent: Agent): boolean
</file>

<file path="src/claude/executor.ts">
import { logger } from "@/utils/logger";
import { type SDKMessage, query } from "@anthropic-ai/claude-code";
import type { ContentBlock, TextBlock } from "@anthropic-ai/sdk/resources/messages/messages";
⋮----
export interface ClaudeCodeExecutorOptions {
    prompt: string;
    projectPath: string;
    systemPrompt?: string;
    timeout?: number;
    abortSignal?: AbortSignal;
    resumeSessionId?: string;
}
⋮----
export interface ClaudeCodeResult {
    success: boolean;
    sessionId?: string;
    totalCost: number;
    messageCount: number;
    duration: number;
    assistantMessages: string[];
    error?: string;
}
⋮----
/**
 * Low-level executor for Claude Code SDK
 * Single Responsibility: Execute Claude Code and stream raw SDK messages
 * Has NO knowledge of Nostr or tasks
 */
export class ClaudeCodeExecutor
⋮----
constructor(private options: ClaudeCodeExecutorOptions)
⋮----
// Link external abort signal if provided
⋮----
/**
     * Execute Claude Code and stream SDK messages
     * @yields Raw SDKMessage events from the Claude Code SDK
     * @returns Final execution result with metrics
     */
async *execute(): AsyncGenerator<SDKMessage, ClaudeCodeResult, unknown>
⋮----
// Set timeout if specified
⋮----
// Clear timeout if aborted early
⋮----
// Log resume session ID if present
⋮----
// Stream messages from Claude Code SDK
⋮----
// Extract metrics from messages
⋮----
// Yield the message to the caller
⋮----
// Clear timeout on success
⋮----
/**
     * Extract text content from an assistant message
     */
private extractTextContent(message: SDKMessage): string
⋮----
kill(): void
⋮----
isRunning(): boolean
</file>

<file path="src/claude/index.ts">

</file>

<file path="src/claude/orchestrator.ts">
import { startExecutionTime, stopExecutionTime } from "@/conversations/executionTime";
import type { Conversation } from "@/conversations/types";
import type { TaskPublisher } from "@/nostr/TaskPublisher";
import { logger } from "@/utils/logger";
import type { ContentBlock, TextBlock } from "@anthropic-ai/sdk/resources/messages/messages";
import type { NDKTask, NDKEvent, NDKSubscription } from "@nostr-dev-kit/ndk";
import { ClaudeCodeExecutor } from "./executor";
import { getNDK } from "@/nostr/ndkClient";
⋮----
export interface ClaudeTaskOptions {
    prompt: string;
    systemPrompt?: string;
    projectPath: string;
    title: string;
    branch?: string;
    conversationRootEventId?: string;
    conversation?: Conversation;
    abortSignal?: AbortSignal;
    resumeSessionId?: string;
}
⋮----
export interface ClaudeTaskResult {
    task: NDKTask;
    sessionId?: string;
    totalCost: number;
    messageCount: number;
    duration: number;
    success: boolean;
    error?: string;
    finalResponse?: string;
}
⋮----
/**
 * Orchestrates Claude Code execution with Nostr task tracking
 * Single Responsibility: Coordinate Claude SDK execution with task lifecycle and Nostr publishing
 */
export class ClaudeTaskOrchestrator
⋮----
constructor(private taskPublisher: TaskPublisher)
⋮----
async execute(options: ClaudeTaskOptions): Promise<ClaudeTaskResult>
⋮----
// Create task
⋮----
// Log if we're resuming a session
⋮----
// Create executor
⋮----
// Set up abort event subscription
⋮----
// Subscribe to ephemeral abort events targeting this task
⋮----
kinds: [24133], // Ephemeral event for task abort
"#e": [task.id], // Events e-tagging this task
⋮----
// Abort the executor
⋮----
// Update task status to interrupted
⋮----
// Start execution timing
⋮----
// Track the last assistant message for final response
⋮----
// Execute and stream messages
⋮----
// The value is the final ClaudeCodeResult
⋮----
// Stop timing
⋮----
// Complete task
⋮----
// Capture session ID when it becomes available
⋮----
// Process SDK message and publish progress updates
⋮----
// Publish progress update using TaskPublisher with session ID
⋮----
// Stop timing on error
⋮----
// Check if this was an abort
⋮----
// Mark task as failed or interrupted
⋮----
// Clean up abort subscription
</file>

<file path="src/commands/agent/add.ts">
import { AgentRegistry } from "@/agents/AgentRegistry";
import { DEFAULT_AGENT_LLM_CONFIG } from "@/llm/constants";
import { configService } from "@/services/ConfigService";
import { logger } from "@/utils/logger";
import { confirm, input } from "@inquirer/prompts";
import { Command } from "commander";
⋮----
interface AddOptions {
    project?: boolean;
    global?: boolean;
}
⋮----
// Determine where to save
⋮----
// Default: use project if in one, otherwise global
⋮----
// Interactive wizard
⋮----
// Determine the base path for the registry
⋮----
// Load existing registry
⋮----
// Check if agent already exists
⋮----
// If creating a project agent, check if it would shadow a global one
⋮----
// If we can't load global agents, continue anyway
⋮----
// Create agent config
⋮----
// Use AgentRegistry to ensure agent (this handles all file operations and Nostr publishing)
</file>

<file path="src/commands/agent/index.ts">
import { Command } from "commander";
import { agentAddCommand } from "./add";
import { agentListCommand } from "./list";
import { agentRemoveCommand } from "./remove";
</file>

<file path="src/commands/agent/list.ts">
import { AgentRegistry } from "@/agents/AgentRegistry";
import type { Agent } from "@/agents/types";
import { configService } from "@/services/ConfigService";
import { logger } from "@/utils/logger";
import { Command } from "commander";
⋮----
interface ListOptions {
    project?: boolean;
    global?: boolean;
    all?: boolean;
}
⋮----
// Default to showing all agents
⋮----
// Validate options
⋮----
// Load and display global agents
⋮----
// Load and display project agents
⋮----
// Load global agents to check for overrides
⋮----
// Load project registry
⋮----
// Categorize agents
⋮----
// Show project-specific agents first
⋮----
// Show overridden agents
</file>

<file path="src/commands/agent/remove.ts">
import { AgentRegistry } from "@/agents/AgentRegistry";
import { configService } from "@/services/ConfigService";
import { logger } from "@/utils/logger";
import { confirm } from "@inquirer/prompts";
import { Command } from "commander";
⋮----
interface RemoveOptions {
    project?: boolean;
    global?: boolean;
    force?: boolean;
}
⋮----
// Determine where to remove from
⋮----
// Default: try project first if in one, otherwise global
⋮----
// Load the appropriate registry
⋮----
// Find the agent
⋮----
// If we defaulted to project, suggest checking global
⋮----
// Check if it's a built-in agent
⋮----
// Confirm deletion unless --force is used
⋮----
// Remove the agent
</file>

<file path="src/commands/debug/chat.ts">
import { AgentRegistry } from "@/agents/AgentRegistry";
import { AgentExecutor } from "@/agents/execution/AgentExecutor";
import type { ExecutionContext } from "@/agents/execution/types";
import type { Agent } from "@/agents/types";
import { ConversationManager } from "@/conversations/ConversationManager";
import type { Phase } from "@/conversations/phases";
import type { Conversation } from "@/conversations/types";
import { createAgentAwareLLMService, loadLLMRouter } from "@/llm";
import { DEFAULT_AGENT_LLM_CONFIG } from "@/llm/constants";
import { getNDK, initNDK } from "@/nostr/ndkClient";
import { PromptBuilder } from "@/prompts";
import { getProjectContext } from "@/services";
import { formatError } from "@/utils/errors";
import { logDebug, logError, logInfo } from "@/utils/logger";
import { ensureProjectInitialized } from "@/utils/projectInitialization";
import { NDKEvent } from "@nostr-dev-kit/ndk";
import chalk from "chalk";
import { v4 as uuidv4 } from "uuid";
⋮----
interface DebugChatOptions {
    systemPrompt?: boolean;
    message?: string;
    llm?: string | boolean;
}
⋮----
export async function runDebugChat(
    initialAgentName: string | undefined,
    options: DebugChatOptions
): Promise<void>
⋮----
// Parse LLM options - --llm should specify a preset name from llms.json
⋮----
// Initialize project context if needed
⋮----
// Create the LLM router
⋮----
// Get project context and create signer
⋮----
// Load agent from registry or create default
⋮----
// Use the existing agent but potentially override llmConfig if --llm specified
⋮----
// Create default debug agent
⋮----
// Create conversation state for AgentExecutor
⋮----
// Initialize NDK for AgentExecutor
⋮----
// Create agent-aware LLM service that routes based on agent's llmConfig
⋮----
// Initialize AgentExecutor
⋮----
// Track messages separately for interactive mode
⋮----
// Show system prompt if requested
⋮----
// Handle single message mode
⋮----
// Create a mock NDKEvent for the debug session
⋮----
mockEvent.kind = 9000; // Tenex conversation event kind
⋮----
// Create execution context
⋮----
// Execute using AgentExecutor without parent tracing context
⋮----
// Interactive REPL mode
⋮----
// Handle special commands
⋮----
// Skip empty inputs
⋮----
// Show thinking indicator
⋮----
// Add user message to tracking
⋮----
// Create a mock NDKEvent for the user message
⋮----
mockEvent.kind = 9000; // Tenex conversation event kind
⋮----
// Create execution context
⋮----
// Execute using AgentExecutor without parent tracing context
⋮----
// Clear thinking indicator
⋮----
// Add assistant response to tracking
⋮----
// Display response
⋮----
// Clear thinking indicator
⋮----
console.log(); // Empty line for readability
</file>

<file path="src/commands/debug/claudeCode.ts">
import { ClaudeCodeExecutor } from "@/claude/executor";
import type { SDKMessage } from "@anthropic-ai/claude-code";
import { formatError } from "@/utils/errors";
import { logError, logInfo, logDebug } from "@/utils/logger";
import chalk from "chalk";
import { colorizeJSON } from "@/utils/formatting";
⋮----
interface DebugClaudeCodeOptions {
    timeout?: number;
}
⋮----
export async function runDebugClaudeCode(
    prompt: string,
    options: DebugClaudeCodeOptions
): Promise<void>
⋮----
// Create executor with options
⋮----
// Track message types for summary
⋮----
// Execute and stream messages
⋮----
// Create the generator
⋮----
// Process messages using the same pattern as ClaudeTaskOrchestrator
⋮----
// The value is the final ClaudeCodeResult
⋮----
// value is an SDKMessage
⋮----
// Track message type counts
⋮----
// Display message based on type
⋮----
// Track last assistant message for summary
⋮----
// Display execution summary
⋮----
// Display message type breakdown
⋮----
// Display last assistant message if available
⋮----
/**
 * Display an SDK message with appropriate formatting
 */
function displaySDKMessage(message: SDKMessage): void
⋮----
// Truncate very long results
⋮----
// Log unknown message types for debugging
⋮----
console.log(); // Empty line for readability
</file>

<file path="src/commands/debug/conversation.ts">
import { getNDK } from "@/nostr/ndkClient";
import { fetchConversation } from "@/utils/conversationFetcher";
import { formatError } from "@/utils/errors";
import { logError, logInfo } from "@/utils/logger";
import { ensureProjectInitialized } from "@/utils/projectInitialization";
import chalk from "chalk";
⋮----
export async function runDebugConversation(nevent: string)
⋮----
// Initialize project context
⋮----
// Get NDK instance
⋮----
// Fetch and format conversation
⋮----
// Display the conversation
</file>

<file path="src/commands/debug/conversationSelector.ts">
import inquirer from "inquirer";
import chalk from "chalk";
import type { ConversationManager } from "@/conversations/ConversationManager";
import type { Conversation } from "@/conversations/types";
⋮----
interface ConversationChoice {
    name: string;
    value: string;
    short: string;
}
⋮----
export async function selectConversation(conversationManager: ConversationManager): Promise<string | null>
⋮----
// Sort conversations by last event timestamp (most recent first)
⋮----
// Create choices for inquirer
⋮----
// Create a display name with metadata
⋮----
// Add separator and cancel option
⋮----
// Handle Ctrl+C
⋮----
function formatDuration(ms: number): string
</file>

<file path="src/commands/debug/index.ts">
import { AgentRegistry } from "@/agents/AgentRegistry";
import { ALL_PHASES, type Phase } from "@/conversations/phases";
import { buildSystemPrompt } from "@/prompts/utils/systemPromptBuilder";
import { getProjectContext } from "@/services";
import { mcpService } from "@/services/mcp/MCPService";
import type { Tool } from "@/tools/types";
import { formatError } from "@/utils/errors";
import { logError, logInfo } from "@/utils/logger";
import { ensureProjectInitialized } from "@/utils/projectInitialization";
import chalk from "chalk";
import { formatMarkdown, colorizeJSON } from "@/utils/formatting";
import type { NDKAgentLesson } from "@/events/NDKAgentLesson";
⋮----
// Format content with enhancements
function formatContentWithEnhancements(content: string, isSystemPrompt = false): string
⋮----
// Handle <tool_use> blocks
⋮----
interface DebugSystemPromptOptions {
    agent: string;
    phase: string;
}
⋮----
export async function runDebugSystemPrompt(options: DebugSystemPromptOptions)
⋮----
// Initialize project context if needed
⋮----
// Load agent from registry
⋮----
// Get all available agents for handoffs
⋮----
// Validate phase
⋮----
// Initialize MCP service to get tools
⋮----
// Continue without MCP tools - don't fail the whole debug command
⋮----
// Build system prompt using the shared function - exactly as production does
// Only pass the current agent's lessons
⋮----
conversation: undefined, // No conversation in debug mode
⋮----
// Format and display the system prompt with enhancements
</file>

<file path="src/commands/debug/timeline.ts">
import { promises as fs } from "node:fs";
import path from "node:path";
import chalk from "chalk";
import type { CommandModule } from "yargs";
import { ConversationManager } from "@/conversations/ConversationManager";
import type { LLMCallLogEntry } from "@/llm/callLogger";
import type { ToolCallLogEntry } from "@/tools/toolLogger";
import { getProjectContext } from "@/services/ProjectContext";
import { selectConversation } from "./conversationSelector";
⋮----
interface TimelineEvent {
    timestamp: number;
    type: "conversation_start" | "llm_call" | "tool_call" | "phase_transition" | "message" | "agent_handoff";
    agent?: string;
    description: string;
    details?: Record<string, unknown>;
    duration?: number;
}
⋮----
async handler(argv)
⋮----
// Initialize conversation manager
⋮----
// Get conversation ID either from argument or selector
⋮----
// Get conversation data
⋮----
// Collect all timeline events
⋮----
// Get the conversation start time from the first event
⋮----
// 1. Add conversation start
⋮----
// 2. Add phase transitions
⋮----
// 3. Load LLM calls
⋮----
// Only include entries for this conversation
⋮----
// Skip invalid lines
⋮----
// 4. Load tool calls
⋮----
// Only include entries for this conversation
⋮----
// Skip invalid lines
⋮----
// 5. Add messages from history
⋮----
// Sort events by timestamp
⋮----
// Display timeline
⋮----
// Format timestamp
⋮----
// Choose color based on event type
⋮----
// Main timeline entry
⋮----
// Agent info
⋮----
// Duration
⋮----
// Key details
⋮----
// Show reasoning separately if present
⋮----
// Time gap indicator
⋮----
// Summary statistics
⋮----
// Performance insights
⋮----
// Helper function to find log files
async function findLogFiles(logDir: string, conversationId: string): Promise<string[]>
⋮----
// Helper function to format duration
function formatDuration(ms: number): string
⋮----
// Helper function to extract reasoning from LLM response
function extractReasoning(content?: string): string | undefined
⋮----
// Extract key decision if present
⋮----
// Otherwise return first line of thinking
</file>

<file path="src/commands/inventory/index.ts">
import { generateInventory, updateInventory } from "@/utils/inventory";
import { logger } from "@/utils/logger";
import { ensureProjectInitialized } from "@/utils/projectInitialization";
import { Command } from "commander";
⋮----
// Initialize project context
⋮----
// Initialize project context
</file>

<file path="src/commands/mcp/__tests__/add.test.ts">
import { describe, it, expect, beforeEach, afterEach, mock } from "bun:test";
import { Command } from "commander";
import { addCommand } from "../add";
import type { TenexConfig } from "@/services/config/types";
⋮----
// Mock modules
⋮----
// Mock process.exit
⋮----
// Mock which command validation
⋮----
// Mock console methods
⋮----
// Reset mocks
⋮----
// Replace console methods
⋮----
// Default mock config
⋮----
// Setup default mocks
⋮----
// Create commander program with mcp subcommand
⋮----
program.exitOverride(); // Prevent process.exit during tests
⋮----
// Restore console methods
⋮----
process.cwd(), // The actual implementation uses process.cwd()
⋮----
// Reset mocks
⋮----
// Should not call which for special commands
</file>

<file path="src/commands/mcp/__tests__/list.test.ts">
import { describe, it, expect, beforeEach, afterEach, mock } from "bun:test";
import { Command } from "commander";
import { listCommand } from "../list";
import type { TenexConfig } from "@/services/config/types";
import chalk from "chalk";
⋮----
// Mock modules
⋮----
// Mock console methods
⋮----
// Helper to set up configService mocks
async function setupConfigServiceMocks()
⋮----
// Reset mocks
⋮----
// Default mock configs
⋮----
// Create commander program
⋮----
(configService.loadConfig as any).mockResolvedValue(mockProjectConfig); // Main config
⋮----
.mockResolvedValueOnce(mockGlobalConfig.mcp) // Global MCP
.mockResolvedValueOnce(mockProjectConfig.mcp); // Project MCP
⋮----
// Check that logger.info was called
⋮----
// Get the actual calls and convert to plain text
⋮----
// Check that key information is present in the output
⋮----
// Check global servers
⋮----
// Check project servers
⋮----
// Check status footer
⋮----
.mockResolvedValueOnce({}) // No global config
.mockResolvedValueOnce({}); // No project config
⋮----
// Should not show these optional fields
⋮----
// Missing args field
</file>

<file path="src/commands/mcp/add.ts">
import { which } from "@/lib/shell";
import { configService } from "@/services/ConfigService";
import type { MCPServerConfig } from "@/services/config/types";
import { logger } from "@/utils/logger";
import { Command } from "commander";
⋮----
interface AddOptions {
    project?: boolean;
    global?: boolean;
}
⋮----
interface AddOptionsWithPaths extends AddOptions {
    paths?: string;
    env?: string[];
}
⋮----
// Parse command and args from the array
⋮----
const command = commandArgs[0] as string; // Safe because we checked length above
⋮----
// Validate name
⋮----
// Validate command exists (skip for npx, npm, etc.)
⋮----
// Parse allowed paths if provided
⋮----
// Parse environment variables if provided
⋮----
// Create server config
⋮----
// Determine where to save
⋮----
// Default: use project if in one, otherwise global
⋮----
// Load existing MCP config
⋮----
// Check if server name already exists
⋮----
// Add new server
⋮----
// Save config
⋮----
// Exit successfully
</file>

<file path="src/commands/mcp/index.ts">
import { Command } from "commander";
import { addCommand } from "./add";
import { listCommand } from "./list";
import { removeCommand } from "./remove";
</file>

<file path="src/commands/mcp/list.ts">
import { configService } from "@/services/ConfigService";
import type { MCPServerConfig } from "@/services/config/types";
import { logger } from "@/utils/logger";
import chalk from "chalk";
import { Command } from "commander";
⋮----
interface ListOptions {
    project?: boolean;
    global?: boolean;
    all?: boolean;
}
⋮----
// Default to showing all servers
⋮----
// Validate options
⋮----
// Load configurations
⋮----
// Check if any servers exist
⋮----
// Display global servers
⋮----
// Display project servers
⋮----
// Categorize servers
⋮----
// Show project-specific servers first
⋮----
// Show overridden servers
⋮----
// Display status summary
⋮----
// Load merged config to show final status
⋮----
function displayServer(
    name: string,
    server: MCPServerConfig,
    isOverridden = false,
    isOverriding = false
): void
</file>

<file path="src/commands/mcp/remove.ts">
import { configService } from "@/services/ConfigService";
import { logger } from "@/utils/logger";
import { confirm } from "@inquirer/prompts";
import { Command } from "commander";
⋮----
interface RemoveOptions {
    project?: boolean;
    global?: boolean;
    force?: boolean;
}
⋮----
// Determine where to remove from
⋮----
// Default: try project first if in one, otherwise global
⋮----
// Load existing MCP config
⋮----
// Check if server exists
⋮----
// If we defaulted to project, suggest checking global
⋮----
// Confirm deletion unless --force is used
⋮----
// Remove the server
⋮----
// Save updated config
</file>

<file path="src/commands/project/index.ts">
import { projectInitCommand } from "@/commands/project/init";
import { projectRunCommand } from "@/commands/project/run";
import { Command } from "commander";
</file>

<file path="src/commands/project/init.ts">
import path from "node:path";
import { logger } from "@/utils/logger";
import { Command } from "commander";
import { ProjectManager } from "../../daemon/ProjectManager";
import { getNDK, initNDK, shutdownNDK } from "../../nostr/ndkClient";
import { formatError } from "../../utils/errors";
⋮----
// Initialize NDK and get singleton
⋮----
// Shutdown NDK
</file>

<file path="src/commands/project/run.ts">
import path from "node:path";
import { ProjectDisplay } from "@/commands/run/ProjectDisplay";
import { StatusPublisher } from "@/commands/run/StatusPublisher";
import { SubscriptionManager } from "@/commands/run/SubscriptionManager";
import { EventHandler } from "@/event-handler";
import { loadLLMRouter } from "@/llm";
import { getNDK, shutdownNDK } from "@/nostr/ndkClient";
import { getProjectContext } from "@/services";
import { mcpService } from "@/services/mcp/MCPService";
import { formatError } from "@/utils/errors";
import { logger } from "@/utils/logger";
import { setupGracefulShutdown } from "@/utils/process";
import { ensureProjectInitialized } from "@/utils/projectInitialization";
import type NDK from "@nostr-dev-kit/ndk";
import chalk from "chalk";
import { Command } from "commander";
⋮----
// Initialize project context (includes NDK setup)
⋮----
// Display project information
⋮----
// Start the project listener
⋮----
async function runProjectListener(projectPath: string, ndk: NDK)
⋮----
// Load LLM router
⋮----
// Initialize MCP service
⋮----
// Initialize event handler
⋮----
// Initialize subscription manager
⋮----
// Start status publisher
⋮----
// Set up graceful shutdown
⋮----
// Stop subscriptions first
⋮----
// Stop status publisher
⋮----
// Clean up event handler subscriptions
⋮----
// Shutdown MCP service
⋮----
// Shutdown NDK singleton
⋮----
// Keep the process running
⋮----
// This promise never resolves, keeping the listener active
</file>

<file path="src/commands/run/constants.ts">
import { EVENT_KINDS } from "@/llm/types";
⋮----
export const STATUS_INTERVAL_MS = 15000; // 15 seconds
⋮----
export function getEventKindName(kind: number): string
</file>

<file path="src/commands/run/processedEventTracking.ts">
import { existsSync } from "node:fs";
import { mkdir, readFile, writeFile } from "node:fs/promises";
import { join } from "node:path";
import { logger } from "@/utils/logger";
⋮----
// State management
⋮----
const SAVE_DEBOUNCE_MS = 1000; // Save at most once per second
⋮----
function getStorePath(projectPath: string): string
⋮----
export async function loadProcessedEvents(projectPath: string): Promise<void>
⋮----
// Ensure .tenex directory exists
⋮----
// Load existing processed event IDs if file exists
⋮----
// Continue with empty set on error
⋮----
export function hasProcessedEvent(eventId: string): boolean
⋮----
export function addProcessedEvent(projectPath: string, eventId: string): void
⋮----
function debouncedSave(projectPath: string): void
⋮----
// Clear existing timeout
⋮----
// Set new timeout
⋮----
async function saveProcessedEvents(projectPath: string): Promise<void>
⋮----
// Ensure directory exists before saving
⋮----
// Convert Set to Array for JSON serialization
⋮----
export async function flushProcessedEvents(projectPath: string): Promise<void>
⋮----
// Cancel any pending saves and save immediately
⋮----
export function clearProcessedEvents(): void
⋮----
export function getProcessedEventCount(): number
</file>

<file path="src/commands/run/ProjectDisplay.ts">
import { logger } from "@/utils/logger";
import type { NDKEvent } from "@nostr-dev-kit/ndk";
⋮----
import type { Agent } from "@/agents/types";
import { getProjectContext } from "@/services";
import chalk from "chalk";
⋮----
export class ProjectDisplay
⋮----
async displayProjectInfo(projectPath: string): Promise<void>
⋮----
// Note: Documentation display moved to after subscription EOSE
⋮----
private displayBasicInfo(projectPath: string): void
⋮----
private async displayAgentConfigurations(): Promise<void>
⋮----
// Debug logging
⋮----
private displayAgentBySlug(slug: string, agent: Agent): void
⋮----
// Display agent information
⋮----
private displayAgent(eventId: string, agents: Map<string, Agent>): void
⋮----
// Find agent by eventId
⋮----
// Display agent information with instance pubkey
</file>

<file path="src/commands/run/StatusPublisher.ts">
import { STATUS_INTERVAL_MS, STATUS_KIND } from "@/commands/run/constants";
import { getNDK } from "@/nostr/ndkClient";
import { configService, getProjectContext, isProjectContextInitialized } from "@/services";
import { formatError } from "@/utils/errors";
import { logWarning } from "@/utils/logger";
import { NDKEvent } from "@nostr-dev-kit/ndk";
⋮----
export class StatusPublisher
⋮----
async startPublishing(projectPath: string): Promise<void>
⋮----
stopPublishing(): void
⋮----
private async publishStatusEvent(projectPath: string): Promise<void>
⋮----
// Tag the project event properly
⋮----
// Sign the event with the project's signer
⋮----
private async addAgentPubkeys(event: NDKEvent, _projectPath: string): Promise<void>
⋮----
private async addModelTags(event: NDKEvent, projectPath: string): Promise<void>
⋮----
// Add model tags for each LLM configuration
⋮----
// Also check if there are agent-specific defaults
</file>

<file path="src/commands/run/SubscriptionManager.ts">
import {
    addProcessedEvent,
    clearProcessedEvents,
    flushProcessedEvents,
    hasProcessedEvent,
    loadProcessedEvents,
} from "@/commands/run/processedEventTracking";
import type { EventHandler } from "@/event-handler";
import { NDKAgentLesson } from "@/events/NDKAgentLesson";
import { EVENT_KINDS } from "@/llm/types";
import { getNDK } from "@/nostr/ndkClient";
import { getProjectContext } from "@/services";
import { logger } from "@/utils/logger";
import {
    type NDKEvent,
    type NDKFilter,
    type NDKSubscription,
    filterAndRelaySetFromBech32,
} from "@nostr-dev-kit/ndk";
import type { NDKKind } from "@nostr-dev-kit/ndk";
import chalk from "chalk";
⋮----
export class SubscriptionManager
⋮----
constructor(eventHandler: EventHandler, projectPath: string)
⋮----
async start(): Promise<void>
⋮----
// Load previously processed event IDs from disk
⋮----
// 1. Subscribe to project updates (NDKProject events)
⋮----
// 2. Subscribe to agent lessons
⋮----
// 3. Subscribe to all project-related events
⋮----
private async subscribeToProjectUpdates(): Promise<void>
⋮----
private async subscribeToAgentLessons(): Promise<void>
⋮----
// Get all agent pubkeys
⋮----
// Create filter for agent lessons
⋮----
"#a": [project.tagId()], // Scoped to this project
⋮----
// Convert to NDKAgentLesson
⋮----
// Add to project context
⋮----
// Log initial load completion
⋮----
// Log lesson distribution
⋮----
private async subscribeToProjectEvents(): Promise<void>
⋮----
// Filter for all events that tag this project
⋮----
private async handleIncomingEvent(event: NDKEvent, source: string): Promise<void>
⋮----
// Check for duplicate events
⋮----
// Mark as processed
⋮----
// Log receipt
⋮----
async stop(): Promise<void>
⋮----
// Flush any pending saves to disk before stopping
</file>

<file path="src/commands/setup/index.ts">
import { llmCommand } from "@/commands/setup/llm";
import { Command } from "commander";
</file>

<file path="src/commands/setup/llm.ts">
import os from "node:os";
import path from "node:path";
⋮----
import { LLMConfigEditor } from "@/llm/LLMConfigEditor";
import { logger } from "@/utils/logger";
import { Command } from "commander";
⋮----
// Project-specific configuration
⋮----
// Check if we're in a TENEX project
⋮----
// Global configuration
⋮----
// Ensure global config directory exists
</file>

<file path="src/commands/daemon.ts">
import path from "node:path";
import { EventMonitor } from "@/daemon/EventMonitor";
import { ProcessManager } from "@/daemon/ProcessManager";
import { ProjectManager } from "@/daemon/ProjectManager";
import { initNDK, shutdownNDK } from "@/nostr/ndkClient";
import { configService } from "@/services";
import { logger } from "@/utils/logger";
import { setupGracefulShutdown } from "@/utils/process";
import { runInteractiveSetup } from "@/utils/setup";
import { Command } from "commander";
⋮----
// Load configuration
⋮----
// Get whitelisted pubkeys
⋮----
// Check for required configurations
⋮----
// Run interactive setup
⋮----
// Save the setup configuration and reload
⋮----
// Initialize NDK and get singleton
⋮----
// Initialize core components
⋮----
// Set up graceful shutdown
⋮----
// Stop monitoring new events
⋮----
// Stop all running projects
⋮----
// Shutdown NDK singleton
⋮----
// Start monitoring without passing LLM configs - let projects load from global config with proper default detection
⋮----
// Keep the process alive
⋮----
// This promise never resolves, keeping the daemon running
</file>

<file path="src/conversations/__tests__/ConversationManager.integration.test.ts">
import { describe, it, expect, beforeEach, afterEach } from "bun:test";
import { ConversationManager } from "../ConversationManager";
import { FileSystemAdapter } from "../persistence";
import type { Conversation } from "../types";
import {
    createConversationEvent,
    createReplyEvent,
    createAgentMessageEvent,
} from "@/test-utils/mocks/events";
import { ensureDirectory, removeDirectory, fileExists, readFile } from "@/lib/fs";
import path from "node:path";
import fs from "node:fs/promises";
import os from "node:os";
⋮----
// Create temporary test directory
⋮----
// Initialize manager with real file system
⋮----
// Clean up
⋮----
// Remove test directory
⋮----
// Ignore cleanup errors
⋮----
// Verify conversation file exists
⋮----
// Read and verify content
⋮----
// Add events and update phase
⋮----
// Force save
⋮----
// Read from disk
⋮----
// Create conversations with first manager
⋮----
// Update one conversation
⋮----
// Create new manager instance
⋮----
// Verify conversations were loaded
⋮----
// Add some history
⋮----
// Archive the conversation
⋮----
// Verify moved to archive
⋮----
// Verify conversation is no longer in memory
⋮----
// Verify archived content is intact
⋮----
// Create and archive a conversation
⋮----
// Create active conversation
⋮----
// New manager instance
⋮----
// Create test conversations
⋮----
// Search for "Express"
⋮----
// Clear initial save
⋮----
// Add an event but don't save manually
⋮----
// Fast-forward 30 seconds to trigger autosave
⋮----
// Wait for autosave to complete
⋮----
// Check if file was updated
⋮----
// Verify the new message was saved
⋮----
// Create 10 conversations concurrently
⋮----
// Verify all were created
⋮----
// Verify all exist in manager
⋮----
// Verify all were persisted
⋮----
// Perform concurrent updates
⋮----
expect(conversation?.history).toHaveLength(6); // Original + 5 updates
⋮----
// Create a corrupted file
⋮----
// Create a valid conversation
⋮----
// Reload manager - should skip corrupted file
⋮----
// Make directory read-only to cause save error
⋮----
// Try to save - should not throw
⋮----
// Expected to fail, but shouldn't crash
⋮----
// Restore permissions
⋮----
// Conversation should still be in memory
⋮----
// Add various metadata
⋮----
// Load in new manager
</file>

<file path="src/conversations/__tests__/ConversationManager.synchronizeAgentContext.test.ts">
import { describe, it, expect, beforeEach, mock } from "bun:test";
import { ConversationManager } from "../ConversationManager";
import type { NDKEvent } from "@nostr-dev-kit/ndk";
import { Message } from "multi-llm-ts";
import type { AgentContext } from "../types";
⋮----
// Mock the fs module
⋮----
// Mock the persistence module
⋮----
// Mock nostr utils
⋮----
// Mock services
⋮----
// Helper to create mock events
const createMockEvent = (id: string, content: string, created_at: number): NDKEvent => (
⋮----
// Create conversation
⋮----
// Create agent context
⋮----
// Update last update time to be after all events
⋮----
// Synchronize context
⋮----
// Should not have added any new messages
⋮----
// Create conversation
⋮----
// Create agent context with old lastUpdate
⋮----
agentContext.lastUpdate = new Date((initialTime - 3600) * 1000); // 1 hour ago
⋮----
// Add some events that happened after agent's last update
⋮----
// Mock agent detection
⋮----
// Synchronize context
⋮----
// Should have added one system message with historical context
⋮----
// Add events including one from the agent itself
⋮----
// Mock agent detection
⋮----
if (event.id === "event-2") return "agent-1"; // Agent's own message
⋮----
// Should only include agent-2's message, not agent-1's own message
⋮----
// Create a new triggering event from user
⋮----
// Should have added the user message
⋮----
// Create a triggering event from another agent
⋮----
// Should have added one system message with proper attribution
⋮----
// Add a missed event
⋮----
// Create a new triggering event
⋮----
return event.id !== "event-2"; // event-2 is from an agent
⋮----
// Should have: historical context, separator, new message
⋮----
// Add triggering event to history first
⋮----
// Add a newer event after the triggering event
⋮----
// Set lastUpdate between triggering event and newer event
agentContext.lastUpdate = new Date((initialTime - 1200) * 1000); // After triggering event but before newer event
⋮----
// Synchronize with the old event as triggering
⋮----
// Should only add the newer event to historical context, not the triggering event as action
⋮----
// Add event with content and event without content
⋮----
// Should only include the valid event in historical context
⋮----
expect(agentContext.messages[0].content).not.toContain("has content"); // null content excluded
⋮----
// Add event from unknown agent
⋮----
// Mock getProjectContext to not have this agent
⋮----
agents: new Map(), // Empty agents map
⋮----
// Should attribute to "Another agent"
⋮----
// Wait a bit to ensure time difference
</file>

<file path="src/conversations/__tests__/ConversationManager.test.ts">
import { describe, it, expect, beforeEach, mock } from "bun:test";
import { ConversationManager } from "../ConversationManager";
import type { NDKEvent } from "@nostr-dev-kit/ndk";
import type { Phase, PhaseTransition } from "../types";
⋮----
// Mock the fs module
⋮----
// Mock the persistence module
⋮----
// Create a conversation
⋮----
// Perform phase transition
⋮----
// First transition: chat -> plan
⋮----
// Second transition: plan -> execute
⋮----
// Third transition: execute -> verification
⋮----
// Verify transition history
⋮----
// Try to transition to the same phase (handoff)
⋮----
// Verify handoff transition details
⋮----
// Verify save was called with conversation including transitions
</file>

<file path="src/conversations/__tests__/executionTime.test.ts">
import { describe, it, expect, beforeEach, afterEach } from "bun:test";
import {
    startExecutionTime,
    stopExecutionTime,
    getTotalExecutionTimeSeconds,
    isExecutionActive,
    initializeExecutionTime,
    ensureExecutionTimeInitialized,
} from "../executionTime";
import type { Conversation, Phase } from "../types";
⋮----
// Mock Date.now for controlled time testing
⋮----
// Helper to create test conversation
function createTestConversation(id: string): Conversation
⋮----
startExecutionTime(conversation); // Try to start again
⋮----
// Should not change the start time
⋮----
mockTime += 5000; // 5 seconds later
⋮----
expect(duration).toBe(5000); // Duration in milliseconds
⋮----
conversation.executionTime.totalSeconds = 10; // Previous sessions
⋮----
mockTime += 5000; // 5 seconds of active session
⋮----
expect(totalSeconds).toBe(15); // 10 + 5
⋮----
// First session: 5 seconds
⋮----
// Second session: 3 seconds
mockTime += 10000; // User thinking time (not counted)
⋮----
// Third session: 7 seconds
mockTime += 5000; // User thinking time (not counted)
⋮----
expect(conversation.executionTime.totalSeconds).toBe(15); // 5 + 3 + 7
⋮----
mockTime += 365 * 24 * 60 * 60 * 1000; // 1 year in milliseconds
⋮----
expect(conversation.executionTime.totalSeconds).toBeGreaterThan(0); // No overflow
⋮----
mockTime += 1499; // Just under 1.5 seconds
⋮----
expect(conversation.executionTime.totalSeconds).toBe(1); // Rounds down
⋮----
mockTime += 1500; // Exactly 1.5 seconds
⋮----
expect(conversation.executionTime.totalSeconds).toBe(3); // 1 + 2 (rounds up)
⋮----
executionTime: undefined as any, // Simulate missing executionTime
⋮----
conversation.executionTime.currentSessionStart = mockTime - 45 * 60 * 1000; // 45 minutes ago
⋮----
conversation.executionTime.currentSessionStart = mockTime - 10 * 60 * 1000; // 10 minutes ago
⋮----
expect(conversation.executionTime.isActive).toBe(true); // Still active
⋮----
} as any, // Missing fields
⋮----
expect(conversation.executionTime.totalSeconds).toBe(100); // Preserved
⋮----
// lastUpdated is only set if we need to reset stale sessions
⋮----
// Active session adds 8 more seconds
⋮----
expect(netTime).toBe(50); // 42 + 8
⋮----
// Simulate rapid tool executions with longer durations
⋮----
mockTime += 1000; // 1 second per tool
⋮----
mockTime += 50; // 50ms between tools (not counted)
⋮----
expect(conversation.executionTime.totalSeconds).toBe(5); // 5 * 1s = 5s
</file>

<file path="src/conversations/__tests__/orchestrator-marker-simple.test.ts">
import { describe, expect, test } from "bun:test";
import { Message } from "multi-llm-ts";
⋮----
// Test the logic without full initialization
⋮----
// The condition from our fix
⋮----
// Simulate what the orchestrator would receive
⋮----
// Add summary context
⋮----
// Add NEW INTERACTION marker
⋮----
// Add the request/completion as user message
⋮----
// Verify structure
⋮----
// Find marker position
⋮----
// User message should come after marker
</file>

<file path="src/conversations/persistence/__tests__/FileSystemAdapter.integration.test.ts">
import { describe, it, expect, beforeEach, afterEach, mock } from "bun:test";
import { FileSystemAdapter } from "../FileSystemAdapter";
import { createTempDir, cleanupTempDir } from "@/test-utils";
import { logger } from "@/utils/logger";
⋮----
import type { Conversation } from "@/conversations/types";
⋮----
// Mock NDK to avoid initialization errors
⋮----
// Create test directories
⋮----
// Initialize adapter
⋮----
// Cleanup
⋮----
history: [], // Empty history
⋮----
// Save conversation
⋮----
// Verify file was created (files are saved as {id}.json, not in subdirectories)
⋮----
// Load conversation
⋮----
expect(loaded?.phase).toBe("chat"); // Schema transforms to lowercase
⋮----
// Verify agentContexts Map is properly restored
⋮----
// Save multiple conversations
⋮----
// List conversations
⋮----
// Save conversations with different titles
⋮----
// Search for authentication
⋮----
// Save and then archive
⋮----
// Verify it's marked as archived
⋮----
// Verify archive file exists (archived files are in archive/{id}.json)
⋮----
// Verify metadata shows it as archived
⋮----
// Save initial version
⋮----
// Simulate concurrent updates
⋮----
// Wait for all saves
⋮----
// Load and verify final state
⋮----
// Should have one of the concurrent values (race condition is expected)
</file>

<file path="src/conversations/persistence/__tests__/FileSystemAdapter.state-persistence.test.ts">
import { describe, it, expect, beforeEach, afterEach, mock } from "bun:test";
import { FileSystemAdapter } from "../FileSystemAdapter";
import { createTempDir, cleanupTempDir, createMockNDKEvent } from "@/test-utils";
⋮----
import type { Conversation, AgentContext } from "@/conversations/types";
import { EVENT_KINDS } from "@/llm/types";
⋮----
// Mock NDK to avoid initialization errors
⋮----
// Mock NDKEvent to handle deserialization
⋮----
// Create test directories
⋮----
// Initialize adapter
⋮----
// Cleanup
⋮----
// Create a complex conversation with multiple agent contexts
⋮----
phaseStartedAt: Date.now() - 60000, // Started 1 minute ago
⋮----
// Save conversation
⋮----
// Verify file was created
⋮----
// Load conversation
⋮----
// Verify basic properties
⋮----
// Verify history is preserved
⋮----
// Verify agent contexts are preserved
⋮----
// Note: toolCalls are not currently preserved in FileSystemAdapter
// This would require updating the adapter to handle toolCalls in message reconstruction
⋮----
// Note: toolCalls are not currently preserved in FileSystemAdapter
⋮----
// Verify metadata is preserved
⋮----
// Verify the history events are preserved
⋮----
// Verify phase transitions are preserved
⋮----
// Verify execution time is preserved
⋮----
// Note: Custom metrics property is not part of standard Conversation interface
// and would require schema updates to persist
⋮----
// Create multiple conversations
⋮----
// Verify all conversations can be loaded
⋮----
// Verify files exist on disk
⋮----
// Filter to only count the conversation files we created in this test
⋮----
// Create initial conversation
⋮----
// Save initial state
⋮----
// Update conversation
⋮----
// Save updated state
⋮----
// Load and verify updates
⋮----
// Save and load
⋮----
// Verify special characters are preserved
</file>

<file path="src/conversations/persistence/__tests__/FileSystemAdapter.test.ts">
import { describe, it, expect, beforeEach, afterEach } from "bun:test";
import { FileSystemAdapter } from "../FileSystemAdapter";
import { createTempDir, cleanupTempDir, createMockConversation } from "@/test-utils";
import path from "node:path";
⋮----
// Initialize again - should not throw
⋮----
// Check the conversations subdirectory
⋮----
// Check base directory
⋮----
// The adapter might sanitize the filename
⋮----
// Update and save again
⋮----
// Should sanitize the filename
⋮----
// Add a non-JSON file
⋮----
// Verify it exists
⋮----
// Delete it
⋮----
// Verify it's gone
⋮----
// Should not throw
⋮----
// Add lots of phase transitions
⋮----
// Save 10 conversations concurrently
</file>

<file path="src/conversations/persistence/FileSystemAdapter.ts">
import { promises as fs } from "node:fs";
import path from "node:path";
import type { Phase } from "@/conversations/phases";
import { ensureDirectory, fileExists, readJsonFile, writeJsonFile } from "@/lib/fs";
import { getNDK } from "@/nostr/ndkClient";
import { logger } from "@/utils/logger";
import { NDKEvent } from "@nostr-dev-kit/ndk";
import { Message } from "multi-llm-ts";
import type { AgentState, ConversationMetadata as ConvMetadata, Conversation } from "../types";
import {
    type AgentStateSchema,
    MetadataFileSchema,
    SerializedConversationSchema,
} from "./schemas";
import type { z } from "zod";
import type {
    ConversationMetadata,
    ConversationPersistenceAdapter,
    ConversationSearchCriteria,
} from "./types";
⋮----
export class FileSystemAdapter implements ConversationPersistenceAdapter
⋮----
constructor(projectPath: string)
⋮----
async initialize(): Promise<void>
⋮----
// Initialize metadata file if it doesn't exist
⋮----
async save(conversation: Conversation): Promise<void>
⋮----
// Convert agentStates Map to a plain object for serialization
⋮----
// Serialize NDKEvents to a storable format
⋮----
// Update metadata
⋮----
async load(conversationId: string): Promise<Conversation | null>
⋮----
// Check archive
⋮----
// Validate the loaded data with Zod
⋮----
// Reconstruct conversation with validated data
⋮----
// Reconstruct agentStates Map
⋮----
async delete(conversationId: string): Promise<void>
⋮----
// Remove from metadata
⋮----
async list(): Promise<ConversationMetadata[]>
⋮----
async search(criteria: ConversationSearchCriteria): Promise<ConversationMetadata[]>
⋮----
async archive(conversationId: string): Promise<void>
⋮----
// Update metadata with lock
⋮----
async restore(conversationId: string): Promise<void>
⋮----
// Update metadata with lock
⋮----
private getConversationPath(conversationId: string): string
⋮----
private getArchivePath(conversationId: string): string
⋮----
private async loadMetadata(): Promise<
⋮----
// Validate with Zod
⋮----
private async saveMetadata(metadata:
⋮----
private async updateMetadata(conversation: Conversation): Promise<void>
⋮----
// Serialize metadata updates to prevent race conditions
⋮----
private async removeFromMetadata(conversationId: string): Promise<void>
⋮----
// Serialize metadata updates to prevent race conditions
</file>

<file path="src/conversations/persistence/index.ts">

</file>

<file path="src/conversations/persistence/schemas.ts">
import { z } from "zod";
⋮----
// Enhanced handoff fields
⋮----
// Simplified agent state schema
⋮----
agentStates: z.record(z.string(), AgentStateSchema).optional(), // Map serialized as object
⋮----
export type SerializedConversation = z.infer<typeof SerializedConversationSchema>;
export type MetadataFile = z.infer<typeof MetadataFileSchema>;
</file>

<file path="src/conversations/persistence/types.ts">
import type { Conversation } from "../types";
⋮----
export interface ConversationMetadata {
    id: string;
    title: string;
    createdAt: number;
    updatedAt: number;
    phase: string;
    eventCount: number;
    agentCount: number;
    archived?: boolean;
}
⋮----
export interface ConversationSearchCriteria {
    title?: string;
    phase?: string;
    dateFrom?: number;
    dateTo?: number;
    agentPubkey?: string;
    archived?: boolean;
}
⋮----
export interface ConversationPersistenceAdapter {
    initialize(): Promise<void>;
    save(conversation: Conversation): Promise<void>;
    load(conversationId: string): Promise<Conversation | null>;
    delete(conversationId: string): Promise<void>;
    list(): Promise<ConversationMetadata[]>;
    search(criteria: ConversationSearchCriteria): Promise<ConversationMetadata[]>;
    archive(conversationId: string): Promise<void>;
    restore(conversationId: string): Promise<void>;
}
⋮----
initialize(): Promise<void>;
save(conversation: Conversation): Promise<void>;
load(conversationId: string): Promise<Conversation | null>;
delete(conversationId: string): Promise<void>;
list(): Promise<ConversationMetadata[]>;
search(criteria: ConversationSearchCriteria): Promise<ConversationMetadata[]>;
archive(conversationId: string): Promise<void>;
restore(conversationId: string): Promise<void>;
</file>

<file path="src/conversations/ConversationManager.ts">
import path from "node:path";
import type { Phase } from "@/conversations/phases";
import type { AgentState, PhaseTransition } from "@/conversations/types";
import { ensureDirectory } from "@/lib/fs";
import { getAgentSlugFromEvent, isEventFromUser } from "@/nostr/utils";
import { getProjectContext } from "@/services";
import {
    type TracingContext,
    createPhaseExecutionContext,
    createTracingContext,
    createTracingLogger,
} from "@/tracing";
import { logger } from "@/utils/logger";
import { NDKArticle, type NDKEvent } from "@nostr-dev-kit/ndk";
import { Message } from "multi-llm-ts";
import { ensureExecutionTimeInitialized } from "./executionTime";
import { FileSystemAdapter } from "./persistence";
import type { ConversationPersistenceAdapter } from "./persistence/types";
import type { Conversation, ConversationMetadata } from "./types";
import { getNDK } from "@/nostr";
import { createExecutionLogger } from "@/logging/ExecutionLogger";
import type { Agent } from "@/agents/types";
⋮----
export class ConversationManager
⋮----
constructor(
        private projectPath: string, 
        persistence?: ConversationPersistenceAdapter
)
⋮----
getProjectPath(): string
⋮----
async initialize(): Promise<void>
⋮----
// Load existing conversations
⋮----
async createConversation(event: NDKEvent): Promise<Conversation>
⋮----
// Create tracing context for this conversation
⋮----
// Log conversation start
⋮----
// Check for 30023 tags (NDKArticle references)
⋮----
// Parse the article reference (format: 30023:pubkey:dtag)
⋮----
phase: "chat", // All conversations start in chat phase
⋮----
agentStates: new Map<string, AgentState>(), // Initialize empty agent states
⋮----
phaseTransitions: [], // Initialize empty phase transitions array
⋮----
// Save immediately after creation
⋮----
getConversation(id: string): Conversation | undefined
⋮----
async updatePhase(
        id: string,
        phase: Phase,
        message: string,
        agentPubkey: string,
        agentName: string,
        reason?: string,
        summary?: string
): Promise<void>
⋮----
// Get or create tracing context
⋮----
// Create phase execution context
⋮----
// Log phase transition
⋮----
// Create transition record even for same-phase handoffs
⋮----
// Update conversation phase if it changed
⋮----
// Clear readFiles when transitioning from REFLECTION back to CHAT
⋮----
// Always push the transition (handoff) record
⋮----
// Save after phase update
⋮----
async incrementContinueCallCount(conversationId: string, phase: Phase): Promise<void>
⋮----
// Initialize continueCallCounts if not exists
⋮----
// Increment the count for the current phase
⋮----
// Save after updating count
⋮----
getContinueCallCount(conversationId: string, phase: Phase): number
⋮----
async addEvent(conversationId: string, event: NDKEvent): Promise<void>
⋮----
// Update the conversation summary to include the latest message
⋮----
// Save after adding event
⋮----
async updateMetadata(
        conversationId: string,
        metadata: Partial<ConversationMetadata>
): Promise<void>
⋮----
getPhaseHistory(conversationId: string): NDKEvent[]
⋮----
// Return events from current phase only
// For now, return all events - phase filtering can be added later
⋮----
getAllConversations(): Conversation[]
⋮----
getConversationByEvent(eventId: string): Conversation | undefined
⋮----
// Find conversation that contains this event
⋮----
/**
     * Build messages for an agent using simplified conversation context.
     * This is the SINGLE method for building agent context.
     */
async buildAgentMessages(
        conversationId: string,
        targetAgent: Agent,
        triggeringEvent?: NDKEvent,
        handoff?: PhaseTransition
): Promise<
⋮----
// Get or initialize the agent's state
⋮----
// === 1. Add historical context (messages the agent has not yet seen) ===
⋮----
// Add handoff summary if provided
⋮----
if (sender) { // Don't include the agent's own previous messages
⋮----
// === 2. Add "NEW INTERACTION" marker (if applicable) ===
// Always for orchestrator, or for any agent if there was historical context
⋮----
// === 3. Add the current triggering event as the primary message ===
⋮----
// If from another agent, attribute it as a system message
⋮----
// If no explicit triggering event content, but a handoff message exists
⋮----
// === 4. Update agent's state for next turn ===
⋮----
// Update Claude session ID from the triggering event if available
⋮----
// Save the updated conversation state
⋮----
/**
     * Update an agent's state (e.g., to store Claude session ID)
     */
async updateAgentState(
        conversationId: string, 
        agentSlug: string, 
        updates: Partial<AgentState>
): Promise<void>
⋮----
/**
     * Helper to determine who sent an event
     */
private getEventSender(event: NDKEvent, agentSlug: string): string | null
⋮----
// Skip the agent's own previous messages
⋮----
// Persistence methods
private async loadConversations(): Promise<void>
⋮----
// Ensure execution time is initialized for loaded conversations
⋮----
// Initialize agentStates as a Map if not present
⋮----
// Convert from plain object to Map after deserialization
⋮----
async saveConversation(conversationId: string): Promise<void>
⋮----
async archiveConversation(conversationId: string): Promise<void>
⋮----
async searchConversations(query: string): Promise<Conversation[]>
⋮----
async cleanup(): Promise<void>
⋮----
// Save all conversations before cleanup
⋮----
/**
     * Get the tracing context for a conversation
     */
getTracingContext(conversationId: string): TracingContext | undefined
⋮----
/**
     * Clean up conversation metadata that's no longer needed
     */
cleanupConversationMetadata(conversationId: string): void
⋮----
// Clear readFiles tracking
⋮----
/**
     * Complete a conversation and clean up its resources
     */
async completeConversation(conversationId: string): Promise<void>
⋮----
// Clean up metadata
⋮----
// Remove from active conversations
⋮----
// Save final state
⋮----
// DEPRECATED: Temporary method for backward compatibility during migration
getAgentContext(conversationId: string, agentSlug: string):
⋮----
// Return a minimal object that satisfies the claudeSessionId check
</file>

<file path="src/conversations/executionTime.ts">
/**
 * Utility functions for managing conversation execution time
 * Works directly with Conversation objects, following DRY principle
 */
⋮----
import type { Conversation } from "./types";
⋮----
/**
 * Start tracking execution time for a conversation
 */
export function startExecutionTime(conversation: Conversation): void
⋮----
// Already active - don't restart
⋮----
/**
 * Stop tracking execution time and add duration to total
 * @returns Duration of this session in milliseconds
 */
export function stopExecutionTime(conversation: Conversation): number
⋮----
/**
 * Get total execution time in seconds (including current session if active)
 */
export function getTotalExecutionTimeSeconds(conversation: Conversation): number
⋮----
// If currently executing, include current session time
⋮----
/**
 * Check if conversation is currently tracking execution time
 */
export function isExecutionActive(conversation: Conversation): boolean
⋮----
/**
 * Initialize execution time for a new conversation
 */
export function initializeExecutionTime(conversation: Conversation): void
⋮----
/**
 * Ensure conversation has execution time initialized (for loaded conversations)
 */
export function ensureExecutionTimeInitialized(conversation: Conversation): void
⋮----
// Crash recovery: if execution was active but daemon restarted,
// reset the active state as the session was lost
⋮----
const maxSessionTime = 30 * 60 * 1000; // 30 minutes max reasonable session
⋮----
// Consider the session lost and reset state
</file>

<file path="src/conversations/index.ts">

</file>

<file path="src/conversations/phases.ts">
export type Phase =
    | "chat"
    | "brainstorm"
    | "plan"
    | "execute"
    | "verification"
    | "chores"
    | "reflection";
⋮----
export interface PhaseDefinition {
    description: string;
    goal: string;
    whenToUse: string[];
    doNot?: string[];
    constraints: string[];
}
⋮----
export function isValidPhase(phase: string): phase is Phase
⋮----
export function getValidTransitions(currentPhase: Phase): readonly Phase[]
⋮----
export function canTransitionTo(currentPhase: Phase, targetPhase: Phase): boolean
</file>

<file path="src/conversations/types.ts">
import type { NDKEvent } from "@nostr-dev-kit/ndk";
import type { Message } from "multi-llm-ts";
import type { Phase } from "./phases";
⋮----
// Simplified agent state to track what an agent has seen
export interface AgentState {
    lastProcessedMessageIndex: number; // Index into Conversation.history
    claudeSessionId?: string; // Claude Code session ID (if per-agent per-conversation)
}
⋮----
lastProcessedMessageIndex: number; // Index into Conversation.history
claudeSessionId?: string; // Claude Code session ID (if per-agent per-conversation)
⋮----
export interface Conversation {
    id: string;
    title: string;
    phase: Phase;
    history: NDKEvent[]; // The SINGLE source of truth for all events/messages
    agentStates: Map<string, AgentState>; // Track what each agent has seen in 'history'
    phaseStartedAt?: number;
    metadata: ConversationMetadata;
    phaseTransitions: PhaseTransition[]; // First-class phase transition history

    // Execution time tracking
    executionTime: {
        totalSeconds: number;
        currentSessionStart?: number;
        isActive: boolean;
        lastUpdated: number;
    };
}
⋮----
history: NDKEvent[]; // The SINGLE source of truth for all events/messages
agentStates: Map<string, AgentState>; // Track what each agent has seen in 'history'
⋮----
phaseTransitions: PhaseTransition[]; // First-class phase transition history
⋮----
// Execution time tracking
⋮----
export interface ConversationMetadata {
    branch?: string; // Git branch for execution phase
    summary?: string; // Current understanding/summary
    requirements?: string; // Captured requirements
    plan?: string; // Approved plan
    readFiles?: string[]; // Files read during this conversation (for write_context_file security)
    continueCallCounts?: Record<Phase, number>; // Track continue calls per phase
    projectPath?: string; // Project path for debug commands
    last_user_message?: string; // Last message from the user
    referencedArticle?: {
        title: string;
        content: string;
        dTag: string;
    }; // NDKArticle referenced by kind:11 event (30023)
}
⋮----
branch?: string; // Git branch for execution phase
summary?: string; // Current understanding/summary
requirements?: string; // Captured requirements
plan?: string; // Approved plan
readFiles?: string[]; // Files read during this conversation (for write_context_file security)
continueCallCounts?: Record<Phase, number>; // Track continue calls per phase
projectPath?: string; // Project path for debug commands
last_user_message?: string; // Last message from the user
⋮----
}; // NDKArticle referenced by kind:11 event (30023)
⋮----
export interface PhaseTransition {
    from: Phase;
    to: Phase;
    message: string; // Comprehensive context from the transition
    timestamp: number;
    agentPubkey: string; // Track which agent initiated
    agentName: string; // Human-readable agent name
    reason?: string; // Brief description (optional)

    // Enhanced handoff fields
    summary?: string; // State summary for receiving agent
}
⋮----
message: string; // Comprehensive context from the transition
⋮----
agentPubkey: string; // Track which agent initiated
agentName: string; // Human-readable agent name
reason?: string; // Brief description (optional)
⋮----
// Enhanced handoff fields
summary?: string; // State summary for receiving agent
</file>

<file path="src/daemon/__tests__/EventMonitor.test.ts">
import { describe, it, expect, beforeEach, afterEach, mock, spyOn } from "bun:test";
import type { NDKEvent, NDKFilter, NDKSubscription } from "@nostr-dev-kit/ndk";
import { EventMonitor } from "../EventMonitor";
import type { IProcessManager } from "../ProcessManager";
import type { IProjectManager } from "../ProjectManager";
⋮----
import { logger } from "@/utils/logger";
⋮----
// Mock nostr-tools at module level
⋮----
// Mock types
interface MockNDK {
    subscribe: (filter: NDKFilter, options?: any) => NDKSubscription;
}
⋮----
interface MockSubscription extends NDKSubscription {
    on: (event: string, handler: (event: NDKEvent) => void) => void;
    stop: () => void;
    eventHandlers: Map<string, ((event: NDKEvent) => void)[]>;
}
⋮----
// Extend EventMonitor to expose private method for testing
class TestableEventMonitor extends EventMonitor
⋮----
public async testHandleEvent(event: NDKEvent): Promise<void>
⋮----
// Create mock implementations
⋮----
// Create mock subscription with event handlers
⋮----
// Create mock NDK
⋮----
// Mock getNDK to return our mock
⋮----
// Spy on logger methods
⋮----
// Create EventMonitor instance
⋮----
// Don't start, just stop
⋮----
const createMockEvent = (tags: string[][], pubkey: string = "testpubkey"): NDKEvent =>
⋮----
// Add debug spy for getNDK
⋮----
// Verify ensureProjectExists was called with correct naddr format
⋮----
expect.stringMatching(/^naddr1/), // Should start with naddr1
⋮----
// First call returns false, second returns true (simulating race condition)
⋮----
// Trigger both events concurrently
⋮----
// Should only spawn once due to the isProjectRunning check
⋮----
// Get the event handler
⋮----
// Create an event that will cause an error
⋮----
// Mock handleEvent to throw
⋮----
// The handler should catch the error
⋮----
// Wait for async error handling
⋮----
// Error should be logged
</file>

<file path="src/daemon/__tests__/ProcessManager.test.ts">
import { describe, it, expect, beforeEach, afterEach, mock } from "bun:test";
import { ProcessManager } from "../ProcessManager";
import { createTempDir, cleanupTempDir } from "@/test-utils";
⋮----
// Track mock processes by PID
⋮----
// Store reference to the original process.kill
⋮----
// Override process.kill globally
⋮----
// Just checking if process exists
⋮----
// Process doesn't exist - throw ESRCH error
⋮----
// Actually killing the process
⋮----
// For SIGTERM, trigger the mock process kill method
⋮----
// For other signals (like SIGKILL), just remove from tracking
⋮----
// Mock child_process module
⋮----
// Trigger exit handlers
⋮----
// Track the process
⋮----
// Mock logger
⋮----
// Clear all tracking
⋮----
// Clean up any remaining processes
⋮----
// Clean up temp directory
⋮----
// Clear all tracking
⋮----
// Check that a process was spawned
⋮----
// Check that the manager tracks it as running
⋮----
// Try to spawn again
⋮----
// Should still be just one process
⋮----
// The process should be tracked by customId in ProcessManager
⋮----
// But not by the path basename
⋮----
// Get the spawned process
⋮----
// Manually trigger process exit
⋮----
// Call the exit handler
⋮----
// Process should be removed from tracking
⋮----
// Temporarily override spawn to create an error process
⋮----
// Trigger error immediately
⋮----
// Clean up
⋮----
// Track it briefly
⋮----
// Return to default behavior for other tests
⋮----
// Wait for error handler
⋮----
// Process should not be running
⋮----
// Restore original module mock
⋮----
// Verify it's running first
⋮----
// Get the PID and remove it from active PIDs
⋮----
// The ProcessManager uses process.kill(pid, 0) to check if process exists
// Our mock will return false since we removed it from activePids
⋮----
// Should detect process is dead
⋮----
// Wait for process to exit
⋮----
// Should not throw
⋮----
// Get the mock process and make it not respond to SIGTERM
⋮----
// Override kill to not trigger exit
⋮----
// Don't trigger exit handlers
⋮----
// Override once to capture the handler but not remove the PID
⋮----
// Start the stop operation
⋮----
// Wait a bit then trigger timeout by calling process.kill with SIGKILL
⋮----
// ProcessManager should try SIGKILL after timeout
// We'll simulate the process finally dying
⋮----
// Call any exit handlers that were registered
⋮----
// Start multiple projects
⋮----
// Wait for all processes to exit
⋮----
// Should not throw when no processes
⋮----
// Stop only project1
⋮----
// Start and stop multiple times
</file>

<file path="src/daemon/__tests__/ProjectManager.test.ts">
import { describe, it, expect, beforeEach, afterEach, mock, spyOn } from "bun:test";
import fs from "node:fs/promises";
import { existsSync } from "node:fs";
import path from "node:path";
import { ProjectManager } from "../ProjectManager";
import type { ProjectData } from "../ProjectManager";
import type NDK from "@nostr-dev-kit/ndk";
import type { NDKProject } from "@nostr-dev-kit/ndk";
import { logger } from "@/utils/logger";
⋮----
import { configService } from "@/services";
⋮----
// Mock modules
⋮----
// Mock node:util promisify to work with our mock exec
⋮----
// Mock child_process exec
⋮----
// Create temp directory for tests
⋮----
// Create mock project
⋮----
// Create mock NDK
⋮----
// Spy on logger
⋮----
// Create ProjectManager instance
⋮----
// Clean up temp directory
⋮----
// Mock git functions
⋮----
// Mock agent fetcher
⋮----
// Mock MCP installer
⋮----
// Verify project directory was created
⋮----
// Verify git was not initialized (repo was cloned)
⋮----
// Mock project without repo
⋮----
// Mock git functions
⋮----
// Verify git was initialized
⋮----
// Make fetchEvent throw an error
⋮----
// Mock config without projectNaddr
⋮----
// Create existing project structure
⋮----
// Should not initialize (already exists)
⋮----
// Mock git functions
⋮----
// Simulate concurrent initialization attempts
⋮----
// Both should return the same path
</file>

<file path="src/daemon/EventMonitor.ts">
import { getNDK } from "@/nostr/ndkClient";
import { logger } from "@/utils/logger";
import type { NDKEvent, NDKFilter, NDKSubscription } from "@nostr-dev-kit/ndk";
import { nip19 } from "nostr-tools";
import type { IProcessManager } from "./ProcessManager";
import type { IProjectManager } from "./ProjectManager";
⋮----
export interface IEventMonitor {
    start(whitelistedPubkeys: string[]): Promise<void>;
    stop(): Promise<void>;
}
⋮----
start(whitelistedPubkeys: string[]): Promise<void>;
stop(): Promise<void>;
⋮----
export class EventMonitor implements IEventMonitor
⋮----
constructor(
⋮----
async start(whitelistedPubkeys: string[]): Promise<void>
⋮----
async stop(): Promise<void>
⋮----
private async handleEvent(event: NDKEvent): Promise<void>
⋮----
// Check if event has project "a" tag
⋮----
// Check if project is already running
⋮----
// Ensure project exists and get path
⋮----
// Spawn project run process
⋮----
private getProjectTag(event: NDKEvent): string | undefined
⋮----
private extractProjectIdentifier(aTag: string): string | undefined
⋮----
// Format: kind:pubkey:identifier
⋮----
private reconstructNaddr(aTag: string, eventPubkey: string): string
⋮----
// Parse the a tag to get project details
⋮----
// Validate that kind is present
⋮----
// Use the pubkey from the a tag if available, otherwise use event pubkey
⋮----
// Encode as naddr
</file>

<file path="src/daemon/ProcessManager.ts">
import { type ChildProcess, spawn } from "node:child_process";
import path from "node:path";
import { logger } from "@/utils/logger";
⋮----
export interface IProcessManager {
    spawnProjectRun(projectPath: string, projectId?: string): Promise<void>;
    isProjectRunning(projectId: string): Promise<boolean>;
    stopProject(projectId: string): Promise<void>;
    stopAll(): Promise<void>;
}
⋮----
spawnProjectRun(projectPath: string, projectId?: string): Promise<void>;
isProjectRunning(projectId: string): Promise<boolean>;
stopProject(projectId: string): Promise<void>;
stopAll(): Promise<void>;
⋮----
interface ProcessInfo {
    process: ChildProcess;
    projectPath: string;
    startedAt: Date;
}
⋮----
export class ProcessManager implements IProcessManager
⋮----
async spawnProjectRun(projectPath: string, projectId?: string): Promise<void>
⋮----
// Check if already running
⋮----
// Get the CLI binary path
⋮----
// Spawn the process
⋮----
stdio: "inherit", // Let output pass through directly
⋮----
// Handle process exit
⋮----
// Handle errors
⋮----
// Store process info
⋮----
async isProjectRunning(projectId: string): Promise<boolean>
⋮----
// Check if process is still alive
⋮----
// Process doesn't exist
⋮----
async stopProject(projectId: string): Promise<void>
⋮----
// Try graceful shutdown first
⋮----
// Wait for process to exit
⋮----
// Force kill if not exited
⋮----
async stopAll(): Promise<void>
⋮----
getRunningProjects(): Array<
</file>

<file path="src/daemon/ProjectManager.ts">
import { exec } from "node:child_process";
import fs from "node:fs/promises";
import path from "node:path";
import { promisify } from "node:util";
import type { Agent } from "@/agents/types";
import { LLMConfigEditor } from "@/llm/LLMConfigEditor";
import { configService, setProjectContext } from "@/services";
import type { TenexConfig } from "@/services/config/types";
import { initializeToolLogger } from "@/tools/toolLogger";
import { ensureTenexInGitignore, initializeGitRepository } from "@/utils/git";
import { logger } from "@/utils/logger";
import { toKebabCase } from "@/utils/string";
import { fetchAgentDefinition } from "@/utils/agentFetcher";
import { installMCPServerFromEvent } from "@/services/mcp/mcpInstaller";
// createAgent functionality has been moved to AgentRegistry
import type NDK from "@nostr-dev-kit/ndk";
import { NDKEvent, NDKPrivateKeySigner } from "@nostr-dev-kit/ndk";
import type { NDKProject } from "@nostr-dev-kit/ndk";
import { NDKMCPTool } from "@/events/NDKMCPTool";
import chalk from "chalk";
⋮----
export interface ProjectData {
    identifier: string;
    pubkey: string;
    naddr: string;
    title: string;
    description?: string;
    repoUrl?: string;
    hashtags: string[];
    agentEventIds: string[];
    mcpEventIds: string[];
    createdAt?: number;
    updatedAt?: number;
}
⋮----
export interface IProjectManager {
    initializeProject(projectPath: string, naddr: string, ndk: NDK): Promise<ProjectData>;
    loadProject(projectPath: string): Promise<ProjectData>;
    ensureProjectExists(identifier: string, naddr: string, ndk: NDK): Promise<string>;
    loadAndInitializeProjectContext(projectPath: string, ndk: NDK): Promise<void>;
}
⋮----
initializeProject(projectPath: string, naddr: string, ndk: NDK): Promise<ProjectData>;
loadProject(projectPath: string): Promise<ProjectData>;
ensureProjectExists(identifier: string, naddr: string, ndk: NDK): Promise<string>;
loadAndInitializeProjectContext(projectPath: string, ndk: NDK): Promise<void>;
⋮----
export class ProjectManager implements IProjectManager
⋮----
constructor(projectsPath?: string)
async initializeProject(projectPath: string, naddr: string, ndk: NDK): Promise<ProjectData>
⋮----
// Fetch project from Nostr
⋮----
// Clone repository if provided, otherwise create directory and init git
⋮----
// Create project directory and initialize git
⋮----
// Ensure .tenex is in .gitignore
⋮----
// Create project structure (without nsec in config)
⋮----
// Initialize agent registry
⋮----
// Fetch and save agent and MCP definitions
⋮----
// Load all agents including built-ins, passing project
⋮----
// Now set the project context once with all agents loaded
⋮----
// Republish kind:0 events for all agents
⋮----
// Check if LLM configuration is needed
⋮----
async loadProject(projectPath: string): Promise<ProjectData>
⋮----
// For now, return a simplified version without decoding naddr
// The identifier and pubkey will be filled when the project is fetched from Nostr
⋮----
identifier: config.projectNaddr, // Use naddr as identifier temporarily
pubkey: "", // Will be filled when fetched from Nostr
⋮----
title: "Untitled Project", // This should come from NDKProject
⋮----
hashtags: [], // This should come from NDKProject
⋮----
createdAt: undefined, // This should come from NDKProject
updatedAt: undefined, // This should come from NDKProject
⋮----
async ensureProjectExists(identifier: string, naddr: string, ndk: NDK): Promise<string>
⋮----
// Check if project already exists
⋮----
// Initialize the project
⋮----
async loadAndInitializeProjectContext(projectPath: string, ndk: NDK): Promise<void>
⋮----
// Load project configuration
⋮----
// Fetch project from Nostr
⋮----
// Load agents using AgentRegistry
⋮----
// Get all agents from registry
⋮----
// Set slug on each agent
⋮----
// Initialize ProjectContext
⋮----
// Republish kind:0 events for all agents on project load
⋮----
// Initialize tool logger for tracing tool executions
⋮----
private async fetchProject(naddr: string, ndk: NDK): Promise<NDKProject>
⋮----
private projectToProjectData(project: NDKProject): ProjectData
⋮----
private async cloneRepository(repoUrl: string, projectPath: string): Promise<void>
⋮----
private async createProjectStructure(
        projectPath: string,
        projectData: ProjectData
): Promise<void>
⋮----
// Create project config (without nsec - it's now in agents.json)
⋮----
private async fetchAndSaveCapabilities(
        projectPath: string,
        project: ProjectData,
        ndk: NDK,
        ndkProject?: NDKProject
): Promise<void>
⋮----
// Load the existing agents registry (which should contain the PM agent)
⋮----
// Process agent tags
⋮----
// Generate a slug for the agent (kebab-case of the name)
⋮----
// Use AgentRegistry.ensureAgent to handle all file operations
⋮----
// Process MCP tags
⋮----
private async projectExists(projectPath: string): Promise<boolean>
⋮----
private async checkAndRunLLMConfigWizard(projectPath: string): Promise<void>
⋮----
// Check if there are any LLM configurations
⋮----
// Don't throw - LLM configuration is not critical for project initialization
</file>

<file path="src/event-handler/__tests__/newConversation.test.ts">
import { describe, it, expect, beforeEach, afterEach, mock } from "bun:test";
import { handleNewConversation } from "../newConversation";
import { MockFactory } from "@/test-utils";
import type { NDKEvent } from "@nostr-dev-kit/ndk";
⋮----
// Create mock event
⋮----
// Create mock agent registry
⋮----
// Create mock conversation manager
⋮----
// Create mock agent executor
⋮----
// Mock modules
⋮----
constructor()
⋮----
async publishResponse()
async publishError()
⋮----
// Remove agent tag
⋮----
// Should not throw
⋮----
// Should not throw
⋮----
// Should not throw
⋮----
// Should not throw
⋮----
// Should still create conversation with empty content
</file>

<file path="src/event-handler/__tests__/reply.test.ts">
import { describe, it, expect, beforeEach, afterEach, mock } from "bun:test";
import { handleReply } from "../reply";
import { MockFactory } from "@/test-utils";
import type { NDKEvent } from "@nostr-dev-kit/ndk";
⋮----
// Create mock conversation
⋮----
// Create mock event
⋮----
// Create mock agent registry
⋮----
// Create mock conversation manager
⋮----
// Create mock agent executor
⋮----
// Mock modules
⋮----
constructor()
⋮----
async publishResponse()
async publishError()
⋮----
// Remove agent tag
⋮----
// Should not throw
⋮----
// Should not attempt to execute
⋮----
// Should not throw
⋮----
// Should not attempt to execute
⋮----
// Should not throw
⋮----
// Should not throw but should not execute
⋮----
// Should not throw
⋮----
// Should not attempt to get conversation
⋮----
// Should still add message with empty content
</file>

<file path="src/event-handler/__tests__/task.test.ts">
import { describe, it, expect, beforeEach, mock } from "bun:test";
import { NDKTask } from "@nostr-dev-kit/ndk";
import { handleTask } from "../task";
import type { AgentExecutor } from "@/agents/execution/AgentExecutor";
import type { ConversationManager } from "@/conversations";
import type { Agent } from "@/agents/types";
import { createMockAgent } from "@/test-utils";
⋮----
// Reset mocks
⋮----
// Create mock agents
⋮----
// Mock project context
⋮----
// Mock getProjectContext
⋮----
// Mock NostrPublisher
⋮----
// Create mock conversation manager
⋮----
// Create mock agent executor
⋮----
// Create mock event
⋮----
// Verify conversation was created
⋮----
// Verify executor was called with orchestrator
⋮----
// Add p-tag for specific agent
⋮----
// Verify executor was called with the p-tagged agent
⋮----
// Add p-tag for unknown agent
⋮----
// Verify conversation was created but no execution happened
⋮----
// Add multiple p-tags
⋮----
// Should route to first matching agent (TestAgent)
⋮----
// Make conversation creation fail
⋮----
// Should not throw
⋮----
// Executor should not be called
⋮----
// Make agent execution fail
⋮----
// Should not throw
⋮----
// Conversation should still be created
⋮----
// Remove claude-session tag
⋮----
// Create long content
⋮----
// Should still process normally
⋮----
// Track NostrPublisher constructor calls
⋮----
// Verify NostrPublisher was created with correct params
</file>

<file path="src/event-handler/index.ts">
import { type NDKEvent, type NDKKind, NDKTask } from "@nostr-dev-kit/ndk";
import type NDK from "@nostr-dev-kit/ndk";
import chalk from "chalk";
import { AgentExecutor } from "../agents/execution/AgentExecutor";
import { getEventKindName } from "../commands/run/constants";
import { ConversationManager } from "../conversations/ConversationManager";
import type { LLMService } from "../llm/types";
import { EVENT_KINDS } from "../llm/types";
import { logger } from "../utils/logger";
import { handleNewConversation } from "./newConversation";
import { handleProjectEvent } from "./project";
import { handleChatMessage } from "./reply";
import { handleTask } from "./task";
⋮----
export class EventHandler
⋮----
constructor(
⋮----
async initialize(): Promise<void>
⋮----
// Initialize components directly
⋮----
// Initialize components
⋮----
async handleEvent(event: NDKEvent): Promise<void>
⋮----
// Ignore kind 24010 (project status), 24111 (typing indicator), and 24112 (typing stop) events
⋮----
private handleDefaultEvent(event: NDKEvent): void
⋮----
async cleanup(): Promise<void>
⋮----
// Save all conversations before shutting down
</file>

<file path="src/event-handler/newConversation.ts">
import type { NDKEvent } from "@nostr-dev-kit/ndk";
import chalk from "chalk";
import type { AgentExecutor } from "../agents/execution/AgentExecutor";
import type { ConversationManager } from "../conversations";
import { getProjectContext } from "../services";
import { formatError } from "../utils/errors";
import { logger } from "../utils/logger";
⋮----
interface EventHandlerContext {
    conversationManager: ConversationManager;
    agentExecutor: AgentExecutor;
}
⋮----
export const handleNewConversation = async (
    event: NDKEvent,
    context: EventHandlerContext
): Promise<void> =>
⋮----
// Create conversation
⋮----
// Get orchestrator agent directly from project context
⋮----
// Check for p-tags to determine target agent
⋮----
let targetAgent = orchestratorAgent; // Default to orchestrator agent
⋮----
// If there are p-tags, check if any match system agents
⋮----
// Execute with the appropriate agent
</file>

<file path="src/event-handler/project.ts">
import fs from "node:fs/promises";
import path from "node:path";
import type { NDKEvent, NDKProject } from "@nostr-dev-kit/ndk";
import { AgentRegistry } from "../agents/AgentRegistry";
import type { Agent } from "../agents/types";
import { getNDK } from "../nostr";
import { getProjectContext, isProjectContextInitialized } from "../services/ProjectContext";
import { fetchAgentDefinition } from "../utils/agentFetcher";
import { logger } from "../utils/logger";
import { toKebabCase } from "../utils/string";
⋮----
/**
 * Handles project update events by syncing agent definitions.
 * When a project event is received, this function:
 * 1. Checks if the event is for the currently loaded project
 * 2. Identifies new agents that have been added to the project
 * 3. Fetches agent definitions from Nostr for new agents
 * 4. Saves agent definitions to disk and registers them in AgentRegistry
 * 5. Updates the ProjectContext with the new agent configuration
 */
export async function handleProjectEvent(event: NDKEvent, projectPath: string): Promise<void>
⋮----
// Extract agent event IDs from the project
⋮----
// Only process if project context is initialized (daemon is running)
⋮----
// Check if this is the same project that's currently loaded
⋮----
// Load agent registry
⋮----
// Track which agents need to be added or updated
⋮----
// Find new agents that need to be fetched
⋮----
// Find agents that need to be removed (exist locally but not in the project)
⋮----
// Handle agent removals first
⋮----
// Fetch and save new agent definitions
⋮----
// Save agent definition file
⋮----
// Generate a slug for the agent
⋮----
// Ensure the agent is registered
⋮----
// Reload the agent registry to get all agents including new ones
⋮----
// Update the project context with new agents
⋮----
// Create NDKProject from the event
⋮----
// Update the existing project context atomically
</file>

<file path="src/event-handler/reply.ts">
import type { NDKEvent } from "@nostr-dev-kit/ndk";
import chalk from "chalk";
import { Message } from "multi-llm-ts";
import type { AgentExecutor } from "../agents/execution/AgentExecutor";
import type { ExecutionContext } from "../agents/execution/types";
import type { ConversationManager } from "../conversations";
import { NostrPublisher } from "../nostr";
import { isEventFromUser } from "../nostr/utils";
import { getProjectContext } from "../services";
import { formatError } from "../utils/errors";
import { logger } from "../utils/logger";
⋮----
interface EventHandlerContext {
    conversationManager: ConversationManager;
    agentExecutor: AgentExecutor;
}
⋮----
export const handleChatMessage = async (
    event: NDKEvent,
    context: EventHandlerContext
): Promise<void> =>
⋮----
// Extract p-tags to identify mentioned agents
⋮----
// Check if this message is directed to the system (project or agents)
⋮----
// This is a reply within an existing conversation
⋮----
async function handleReplyLogic(
    event: NDKEvent,
    { conversationManager, agentExecutor }: EventHandlerContext,
    mentionedPubkeys: string[]
): Promise<void>
⋮----
// Find the conversation this reply belongs to
⋮----
// If no conversation found and this is a reply to an NDKTask (K tag = 1934)
⋮----
// The task itself is the conversation root, so look for it directly
⋮----
// Add event to conversation history
⋮----
// Get PM agent directly from project context
⋮----
// Determine which agent should handle this event
let targetAgent = orchestratorAgent; // Default to orchestrator agent
⋮----
// Check for p-tagged agents regardless of sender
⋮----
// Find the first p-tagged system agent
⋮----
// For non-user events, skip if agent is the author (prevent loops)
⋮----
// For non-user events without valid p-tags, skip processing
⋮----
// Check for recent phase transition that might be a handoff for this agent
⋮----
// If this transition was very recent (within last 30 seconds) and has handoff info
⋮----
// Extract claude-session from the event
⋮----
// Execute with the appropriate agent
⋮----
agentExecutor, // Pass the executor so continue() can use it
⋮----
// Add handoff if available
⋮----
// Don't pre-add user messages to agent context - let the agent executor handle this
// to ensure proper bootstrapping for newly mentioned agents via p-tags
⋮----
// Check if it's an insufficient credits error
⋮----
// Create NostrPublisher to publish error
</file>

<file path="src/event-handler/task.ts">
import type { NDKEvent, NDKTask } from "@nostr-dev-kit/ndk";
import chalk from "chalk";
import type { AgentExecutor } from "../agents/execution/AgentExecutor";
import type { ConversationManager } from "../conversations";
import { getProjectContext } from "../services";
import { formatError } from "../utils/errors";
import { logger } from "../utils/logger";
import { Agent } from "@/agents";
⋮----
interface EventHandlerContext {
    conversationManager: ConversationManager;
    agentExecutor: AgentExecutor;
}
⋮----
export const handleTask = async (event: NDKTask, context: EventHandlerContext): Promise<void> =>
⋮----
// Extract p-tags to identify mentioned agents
⋮----
// Create conversation from NDKTask
⋮----
// Log the claude-session tag if present
⋮----
// Get orchestrator agent directly from project context
⋮----
// If there are p-tags, check if any match system agents
⋮----
// Execute with the appropriate agent
</file>

<file path="src/events/index.ts">

</file>

<file path="src/events/NDKAgent.ts">
import { NDKEvent, type NDKRawEvent } from "@nostr-dev-kit/ndk";
import type NDK from "@nostr-dev-kit/ndk";
⋮----
export class NDKAgent extends NDKEvent
⋮----
constructor(ndk?: NDK, event?: NDKEvent | NDKRawEvent)
⋮----
static from(event: NDKEvent): NDKAgent
⋮----
get name(): string | undefined
⋮----
set name(value: string | undefined)
⋮----
get title(): string | undefined
⋮----
set title(value: string | undefined)
⋮----
get description(): string | undefined
⋮----
/**
     * A one-liner description of the agent's purpose or functionality.
     */
set description(value: string | undefined)
⋮----
get role(): string | undefined
⋮----
/**
     * The expertise and personality for this agent.
     * This shapes how the agent interacts with users and other agents.
     */
set role(value: string | undefined)
⋮----
get instructions(): string | undefined
⋮----
/**
     * Detailed instructions or guidelines for the agent's operation.
     */
set instructions(value: string | undefined)
⋮----
get version(): number
⋮----
if (val === undefined) return 1; // Default version if not specified
⋮----
set version(value: number)
⋮----
get useCriteria(): string | undefined
⋮----
/**
     * Criteria for when this agent should be selected or used.
     * This helps with agent routing and selection.
     */
set useCriteria(value: string | undefined)
</file>

<file path="src/events/NDKAgentLesson.ts">
import { NDKEvent, type NDKRawEvent } from "@nostr-dev-kit/ndk";
import type NDK from "@nostr-dev-kit/ndk";
⋮----
export enum LessonQuality {
    TRIVIAL = "trivial",
    VALUABLE = "valuable",
    CRITICAL = "critical"
}
⋮----
export class NDKAgentLesson extends NDKEvent
⋮----
constructor(ndk?: NDK, event?: NDKEvent | NDKRawEvent)
⋮----
static from(event: NDKEvent): NDKAgentLesson
⋮----
get title(): string | undefined
⋮----
/**
     * Title/description of what this lesson is about.
     */
set title(value: string | undefined)
⋮----
// Alias for title
get description(): string | undefined
⋮----
set description(value: string | undefined)
⋮----
/**
     * The lesson content - what the agent learned.
     * This is stored in the event content.
     */
get lesson(): string
⋮----
set lesson(value: string)
⋮----
/**
     * Set the agent that this lesson belongs to.
     * @param agentEvent The NDKAgent event to reference
     */
set agent(agentEvent: NDKEvent)
⋮----
/**
     * Get the agent event ID this lesson belongs to.
     */
get agentId(): string | undefined
⋮----
/**
     * Quality assessment of the lesson
     */
get quality(): LessonQuality | undefined
⋮----
set quality(value: LessonQuality | undefined)
⋮----
/**
     * Metacognition reasoning - why this lesson is worth learning
     */
get metacognition(): string | undefined
⋮----
set metacognition(value: string | undefined)
</file>

<file path="src/events/NDKMCPTool.ts">
import { NDKEvent, type NDKRawEvent } from "@nostr-dev-kit/ndk";
import type NDK from "@nostr-dev-kit/ndk";
⋮----
export class NDKMCPTool extends NDKEvent
⋮----
constructor(ndk?: NDK, event?: NDKEvent | NDKRawEvent)
⋮----
static from(event: NDKEvent): NDKMCPTool
⋮----
get name(): string | undefined
⋮----
set name(value: string | undefined)
⋮----
get description(): string | undefined
⋮----
set description(value: string | undefined)
⋮----
get command(): string | undefined
⋮----
set command(value: string | undefined)
⋮----
get image(): string | undefined
⋮----
set image(value: string | undefined)
⋮----
get slug(): string
</file>

<file path="src/lib/fs/filesystem.ts">
import type { Stats } from "node:fs";
⋮----
import { formatError } from "@/utils/errors";
import { logError } from "@/utils/logger";
⋮----
/**
 * Unified file system utilities combining patterns from CLI and shared packages
 * Provides both sync and async operations with consistent error handling
 */
⋮----
// Path utilities
// File operations
export async function readFile(filePath: string, encoding?: BufferEncoding): Promise<string>;
export async function readFile(filePath: string, encoding: null): Promise<Buffer>;
export async function readFile(
    filePath: string,
    encoding?: BufferEncoding | null
): Promise<string | Buffer>
⋮----
export function expandHome(filePath: string): string
⋮----
export function resolvePath(filePath: string): string
⋮----
// Directory operations
export async function ensureDirectory(dirPath: string): Promise<void>
⋮----
export function ensureDirectorySync(dirPath: string): void
⋮----
export async function directoryExists(dirPath: string): Promise<boolean>
⋮----
export function directoryExistsSync(dirPath: string): boolean
⋮----
// File operations
export async function fileExists(filePath: string): Promise<boolean>
⋮----
export function fileExistsSync(filePath: string): boolean
⋮----
// JSON operations with error handling
export async function readJsonFile<T>(filePath: string): Promise<T | null>
⋮----
export function readJsonFileSync<T>(filePath: string): T | null
⋮----
export async function writeJsonFile<T>(
    filePath: string,
    data: T,
    options?: { spaces?: number }
): Promise<void>
⋮----
export function writeJsonFileSync<T>(
    filePath: string,
    data: T,
    options?: { spaces?: number }
): void
⋮----
// Text file operations
export async function readTextFile(filePath: string): Promise<string | null>
⋮----
export function readTextFileSync(filePath: string): string | null
⋮----
export async function writeTextFile(filePath: string, content: string): Promise<void>
⋮----
export function writeTextFileSync(filePath: string, content: string): void
⋮----
// Directory listing
export async function listDirectory(dirPath: string): Promise<string[]>
⋮----
export function listDirectorySync(dirPath: string): string[]
⋮----
// File copying
export async function copyFile(src: string, dest: string): Promise<void>
⋮----
export function copyFileSync(src: string, dest: string): void
⋮----
// File deletion
export async function deleteFile(filePath: string): Promise<void>
⋮----
// File doesn't exist, that's fine
⋮----
export function deleteFileSync(filePath: string): void
⋮----
// File doesn't exist, that's fine
⋮----
// Directory deletion
export async function deleteDirectory(
    dirPath: string,
    options?: { recursive?: boolean }
): Promise<void>
⋮----
// Directory doesn't exist, that's fine
⋮----
export function deleteDirectorySync(dirPath: string, options?:
⋮----
// Directory doesn't exist, that's fine
⋮----
// File stats
export async function getFileStats(filePath: string): Promise<Stats | null>
⋮----
export function getFileStatsSync(filePath: string): Stats | null
</file>

<file path="src/lib/fs/index.ts">

</file>

<file path="src/lib/fs/tenex.ts">
import path from "node:path";
import { directoryExists, ensureDirectory } from "./filesystem.js";
⋮----
/**
 * Get paths for common .tenex files
 */
export function getTenexPaths(projectPath: string)
⋮----
// Configuration operations removed - use ConfigService from @/services instead
⋮----
/**
 * Check if a project has been initialized (has .tenex directory)
 */
export async function isProjectInitialized(projectPath: string): Promise<boolean>
⋮----
/**
 * Initialize .tenex directory structure
 */
export async function initializeTenexDirectory(projectPath: string): Promise<void>
⋮----
// Create main .tenex directory
⋮----
// Create subdirectories
</file>

<file path="src/lib/shell.ts">
import { exec } from "node:child_process";
import { promisify } from "node:util";
⋮----
/**
 * Find the full path of a command using the system's which/where command
 */
export async function which(command: string): Promise<string | null>
⋮----
const path = stdout.trim().split("\n")[0]; // Get first result if multiple
</file>

<file path="src/llm/__tests__/pricing.test.ts">
/**
 * Tests for OpenRouter pricing service
 */
⋮----
import { OpenRouterPricingService } from "../pricing";
⋮----
// Mock fetch for testing
⋮----
// Expected: (10000/1M * 0.0000003) + (5000/1M * 0.0000025) = 0.000000003 + 0.0000000125 = 0.0000000155
⋮----
// Expected: (10000 + 5000) / 1M * 1.0 = 0.015
</file>

<file path="src/llm/callLogger.ts">
import { promises as fs } from "node:fs";
import { join } from "node:path";
import type { CompletionRequest, CompletionResponse, LLMConfig } from "./types";
⋮----
export interface LLMCallLogEntry {
    timestamp: string;
    timestampMs: number;
    requestId: string;
    duration?: number;
    durationMs?: number;

    // Configuration context
    configKey: string;
    config: {
        provider: string;
        model: string;
        baseUrl?: string;
        enableCaching?: boolean;
        temperature?: number;
        maxTokens?: number;
    };

    // Request context
    agentName?: string;
    context?: {
        configName?: string;
        agentName?: string;
    };

    // Complete request data
    request: {
        messages: Array<{
            role: string;
            content: string;
            contentLength: number;
        }>;
        options?: Record<string, unknown>;
        messageCount: number;
        totalRequestLength: number;
    };

    // Complete response data (if successful)
    response?: {
        content?: string;
        contentLength?: number;
        toolCalls?: Array<{
            name: string;
            params: unknown;
            paramsLength: number;
        }>;
        toolCallCount: number;
        usage?: {
            promptTokens?: number;
            completionTokens?: number;
            totalTokens?: number;
            cost?: number;
        };
    };

    // Error data (if failed)
    error?: {
        message: string;
        stack?: string;
        type: string;
    };

    // Status
    status: "success" | "error";

    // Performance metrics
    performance: {
        startTime: number;
        endTime?: number;
        durationMs?: number;
        tokensPerSecond?: number;
    };
}
⋮----
// Configuration context
⋮----
// Request context
⋮----
// Complete request data
⋮----
// Complete response data (if successful)
⋮----
// Error data (if failed)
⋮----
// Status
⋮----
// Performance metrics
⋮----
export class LLMCallLogger
⋮----
constructor(projectPath: string)
⋮----
private async ensureLogDirectory(): Promise<void>
⋮----
// Ignore if directory already exists
⋮----
private getLogFileName(agentName?: string): string
⋮----
const date = new Date().toISOString().split("T")[0]; // YYYY-MM-DD
⋮----
// Sanitize agent name for filename
⋮----
private getLogFilePath(agentName?: string): string
⋮----
private generateRequestId(configKey: string): string
⋮----
private calculateTokensPerSecond(
        usage?: { completionTokens?: number },
        durationMs?: number
): number | undefined
⋮----
async logLLMCall(
        configKey: string,
        config: LLMConfig,
        request: CompletionRequest,
        result: { response?: CompletionResponse; error?: Error },
        performance: { startTime: number; endTime: number }
): Promise<void>
⋮----
// Calculate total request length
⋮----
// Build log entry
⋮----
duration: Math.round((durationMs / 1000) * 100) / 100, // seconds with 2 decimals
⋮----
// Add response data if successful
⋮----
cost: undefined, // Cost calculation would need to be implemented based on model pricing
⋮----
// Add error data if failed
⋮----
// Write to JSONL file
⋮----
// Don't let logging errors break the main flow
⋮----
// Singleton instance
⋮----
export function initializeLLMLogger(projectPath: string): LLMCallLogger
⋮----
export function getLLMLogger(): LLMCallLogger | null
</file>

<file path="src/llm/constants.ts">
/**
 * LLM Configuration Constants
 */
⋮----
// Default configuration keys used in llms.json
⋮----
// Default fallback key for agents when no llmConfig is specified
</file>

<file path="src/llm/index.ts">
// Export types
⋮----
// Export router and utilities
⋮----
// Export model utilities
⋮----
// Re-export commonly used multi-llm-ts types
</file>

<file path="src/llm/LLMConfigEditor.ts">
import os from "node:os";
import path from "node:path";
import type { LLMConfig, LLMProvider } from "@/llm/types";
import { configService } from "@/services";
import type { TenexLLMs } from "@/services/config/types";
import { logger } from "@/utils/logger";
import search from "@inquirer/search";
import chalk from "chalk";
import inquirer from "inquirer";
import type { ModelsList } from "multi-llm-ts";
import { Message, igniteEngine } from "multi-llm-ts";
import { LLM_DEFAULTS } from "./constants";
import { getAllModels, getModelsForProvider } from "./models";
⋮----
type LLMConfigWithName = LLMConfig & {
    name: string;
};
⋮----
async function selectModelWithSearch(provider: string, models: string[]): Promise<string>
⋮----
async function selectOpenRouterModelWithPricing(
    models: string[]
): Promise<
⋮----
const formatModelChoice = (model: string) =>
⋮----
supportsCaching: false, // We don't have this info from multi-llm-ts
⋮----
export class LLMConfigEditor
⋮----
constructor(configPath: string, isGlobal = true)
⋮----
async showMainMenu(): Promise<void>
⋮----
// Show menu again after action
⋮----
async runOnboardingFlow(): Promise<void>
⋮----
private async loadConfig(): Promise<TenexLLMs>
⋮----
// Ensure defaults exists
⋮----
private async saveConfig(config: TenexLLMs): Promise<void>
⋮----
private getConfigList(llmsConfig: TenexLLMs): LLMConfigWithName[]
⋮----
private getExistingApiKeys(llmsConfig: TenexLLMs, provider: LLMProvider): string[]
⋮----
// Check auth for API keys
⋮----
private async addConfiguration(llmsConfig: TenexLLMs): Promise<void>
⋮----
// Get API key if available for fetching models
⋮----
// Fetch models dynamically based on provider
⋮----
// Default configuration name based on provider and model
⋮----
// Handle caching based on provider
⋮----
// Type assertion needed due to inquirer's complex prompt type system
⋮----
// Add apiKey for testing (if provided)
⋮----
// Add baseUrl for openrouter
⋮----
// Test the configuration BEFORE saving it
⋮----
// Create a temporary llmsConfig with the new credentials for testing
const testLlmsConfig = JSON.parse(JSON.stringify(llmsConfig)); // Deep copy
⋮----
// Test the configuration
⋮----
// Only save if test passes
⋮----
// If this is global config and a new API key was entered, save it to credentials
⋮----
// Remove API key from individual config in global mode
// API key is stored in auth, not on config
⋮----
private async editConfiguration(llmsConfig: TenexLLMs): Promise<void>
⋮----
// Get API key if available for fetching models
⋮----
// Fetch models dynamically based on provider
⋮----
// API key is stored in auth, not on config
⋮----
// Update defaults if needed
⋮----
private async removeConfiguration(llmsConfig: TenexLLMs): Promise<void>
⋮----
// Update defaults if needed
⋮----
private async setDefaultConfiguration(
        llmsConfig: TenexLLMs,
        defaultType: string
): Promise<void>
⋮----
private async testExistingConfiguration(llmsConfig: TenexLLMs): Promise<void>
⋮----
/**
     * Test an LLM configuration by sending a test message
     */
private async testLLMConfig(config: LLMConfig): Promise<boolean>
⋮----
// Use the multi-llm-ts v4.0 API
⋮----
// Find the specific model - handle both string and ChatModel types
</file>

<file path="src/llm/models.ts">
import { logger } from "@/utils/logger";
import { type ModelsList, loadModels, loadOpenRouterModels } from "multi-llm-ts";
import type { LLMProvider } from "./types";
⋮----
/**
 * Get available models for a provider
 */
export async function getModelsForProvider(
    provider: LLMProvider,
    apiKey?: string
): Promise<ModelsList | null>
⋮----
/**
 * Get all available models grouped by provider
 */
export async function getAllModels(
    credentials?: Record<string, string>
): Promise<Record<string, ModelsList>>
⋮----
// Handle OpenRouter separately if credentials are provided
</file>

<file path="src/llm/pricing.ts">
/**
 * OpenRouter pricing service for dynamic LLM cost calculation
 */
⋮----
import { logger } from "@/utils/logger";
⋮----
interface OpenRouterPricing {
    prompt: string;
    completion: string;
    request: string;
    image: string;
    web_search: string;
    internal_reasoning: string;
}
⋮----
interface OpenRouterModel {
    id: string;
    name: string;
    pricing: OpenRouterPricing;
}
⋮----
interface OpenRouterResponse {
    data: OpenRouterModel[];
}
⋮----
interface ModelPricing {
    prompt: number;
    completion: number;
}
⋮----
export class OpenRouterPricingService
⋮----
private readonly cacheValidityMs = 60 * 60 * 1000; // 1 hour
⋮----
/**
     * Get pricing for a specific model
     */
async getModelPricing(modelId: string): Promise<ModelPricing | null>
⋮----
/**
     * Calculate cost for token usage
     */
async calculateCost(
        modelId: string,
        promptTokens: number,
        completionTokens: number
): Promise<number>
⋮----
// Return a minimal default cost
return ((promptTokens + completionTokens) / 1_000_000) * 1.0; // $1 per 1M tokens
⋮----
/**
     * Get all available models with pricing
     */
async getAllModelPricing(): Promise<Map<string, ModelPricing>>
⋮----
/**
     * Force refresh the pricing cache
     */
async refreshCache(): Promise<void>
⋮----
// Clear existing cache
⋮----
// Populate cache with new data
⋮----
// Only cache models with valid pricing
⋮----
/**
     * Ensure cache is fresh, refresh if needed
     */
private async ensureFreshCache(): Promise<void>
⋮----
/**
     * Find best matching model ID for partial model names
     * This helps with cases where the model name doesn't exactly match OpenRouter's ID
     */
async findModelId(partialModelName: string): Promise<string | null>
⋮----
// Exact match first
⋮----
// Partial match
⋮----
// Export singleton instance
</file>

<file path="src/llm/router.ts">
import { configService } from "@/services";
import { logger } from "@/utils/logger";
import { igniteEngine, loadModels } from "multi-llm-ts";
import { ToolPlugin } from "./ToolPlugin";
import { getLLMLogger, initializeLLMLogger } from "./callLogger";
import type {
    CompletionRequest,
    CompletionResponse,
    LLMConfig,
    LLMService,
    StreamEvent,
} from "./types";
⋮----
export interface LLMRouterConfig {
    configs: Record<string, LLMConfig>;
    defaults: {
        agents?: string;
        analyze?: string;
        orchestrator?: string;
        [key: string]: string | undefined;
    };
}
⋮----
/**
 * Simple LLM router that manages multiple LLM instances
 */
export class LLMRouter implements LLMService
⋮----
constructor(private config: LLMRouterConfig)
⋮----
/**
     * Resolve which configuration to use based on context
     */
private resolveConfigKey(context?:
⋮----
// Check if configName is a defaults reference (e.g., "defaults.analyze")
⋮----
// If the default key doesn't exist or point to a valid config, continue to other logic
⋮----
// Check if configName is a default key (e.g., "agents", "analyze", "orchestrator")
⋮----
// Direct config name takes precedence
⋮----
// Fallback to first available config
⋮----
/**
     * Get available configuration keys
     */
getConfigKeys(): string[]
⋮----
/**
     * Complete a request using the appropriate LLM
     */
async complete(request: CompletionRequest): Promise<CompletionResponse>
⋮----
// Extract context from request options
⋮----
// Get the configuration key
⋮----
// Trace system prompt if present
⋮----
// Trace all messages
⋮----
// Use the multi-llm-ts v4 API
⋮----
// Register tools as plugins if provided
⋮----
// Find the specific model - handle both string and ChatModel types
⋮----
// Execute completion with API
⋮----
// Model metadata is available in the model object if needed for future use
// Not mutating the response object to maintain clean types
⋮----
// Trace response content
⋮----
// Trace tool calls if present
⋮----
// Log to comprehensive JSONL logger
⋮----
// Log to comprehensive JSONL logger
⋮----
async *stream(request: CompletionRequest): AsyncIterable<StreamEvent>
⋮----
// Register tools as plugins if provided
⋮----
// Find the specific model
⋮----
// Use generate() for streaming
⋮----
// Log chunk metadata
⋮----
// Build the final response
⋮----
// Log to comprehensive JSONL logger
⋮----
// Log to comprehensive JSONL logger
⋮----
/**
 * Load LLM router from configuration file
 */
export async function loadLLMRouter(projectPath: string): Promise<LLMRouter>
⋮----
// Initialize comprehensive LLM logger
⋮----
// Use configService to load merged global and project-specific configuration
⋮----
// Transform TenexLLMs structure to LLMRouterConfig
⋮----
// For each configuration, merge in the credentials
⋮----
/**
 * Create an agent-aware LLM service that automatically routes based on agent
 */
export function createAgentAwareLLMService(router: LLMRouter, agentName: string): LLMService
⋮----
// Inject agent name into options
⋮----
// Inject agent name into options
</file>

<file path="src/llm/ToolPlugin.ts">
import { getToolLogger } from "@/tools/toolLogger";
import type {
    Tool,
    ExecutionContext,
    ToolExecutor,
    ToolError,
    ToolExecutionResult,
} from "@/tools/types";
import { createToolExecutor } from "@/tools/types";
import { logger } from "@/utils/logger";
import {
    Plugin,
    type PluginExecutionContext,
    type PluginParameter as MultiLLMPluginParameter,
} from "multi-llm-ts";
import { serializeToolResult } from "./ToolResult";
⋮----
/**
 * Adapter that converts TENEX Tool to multi-llm-ts Plugin
 * Handles all tool types with unified interface
 */
export class ToolPlugin extends Plugin
⋮----
constructor(
        private readonly tool: Tool,
        private readonly tenexContext: ExecutionContext
)
⋮----
serializeInTools(): boolean
⋮----
isEnabled(): boolean
⋮----
getName(): string
⋮----
getDescription(): string
⋮----
getRunningDescription(tool: string, args: Record<string, unknown>): string
⋮----
// For the continue tool specifically, provide a concise description
⋮----
// For other tools, provide a generic running description
⋮----
getParameters(): MultiLLMPluginParameter[]
⋮----
// Extract parameter info from schema shape
⋮----
// Add enum values if present
⋮----
private mapSchemaTypeToPluginType(
        schemaType: string
): "string" | "number" | "boolean" | "array" | "object"
⋮----
async execute(
        context: PluginExecutionContext,
        parameters: Record<string, unknown>
): Promise<unknown>
⋮----
// Execute the tool using the type-safe executor
⋮----
// Serialize the typed result for transport through LLM layer
⋮----
// Create a human-readable output message
⋮----
// Check if it's a control flow result
⋮----
// Check if it's a termination result
⋮----
// Regular tool output
⋮----
// Extract error message if present
⋮----
// Return both serialized result and human-readable output
⋮----
// Include the full typed result for ReasonActLoop
⋮----
// Log the successful tool execution
⋮----
result, // Pass the original typed result
⋮----
// Create an error result for logging
⋮----
// Log the failed tool execution
⋮----
private formatError(error: ToolError): string
⋮----
// If the field is empty and message is just "Required", make it clearer
</file>

<file path="src/llm/ToolResult.ts">
/**
 * Serializable representation of tool execution results
 * Preserves type information across LLM boundaries
 */
⋮----
import type { ToolExecutionResult } from "@/tools/types";
import type { ToolError } from "@/tools/core";
⋮----
/**
 * Simplified serialized tool result
 */
export interface SerializedToolResult {
    /** Whether the tool execution was successful */
    success: boolean;

    /** Tool execution duration in milliseconds */
    duration: number;

    /** The actual result data */
    data: {
        output?: unknown;
        error?: {
            kind: string;
            message: string;
        };
    };
}
⋮----
/** Whether the tool execution was successful */
⋮----
/** Tool execution duration in milliseconds */
⋮----
/** The actual result data */
⋮----
/**
 * Serialize a tool result for LLM transport
 */
export function serializeToolResult(result: ToolExecutionResult): SerializedToolResult
⋮----
/**
 * Check if an object is a serialized tool result
 */
export function isSerializedToolResult(obj: unknown): obj is SerializedToolResult
⋮----
/**
 * Deserialize a tool result back to typed format
 */
function deserializeToolError(
    error: { kind: string; message: string } | undefined
): ToolError | undefined
⋮----
// Create a proper ToolError based on the kind
⋮----
field: "unknown", // We don't serialize the field, so use a default
⋮----
tool: "unknown", // We don't serialize the tool name separately
⋮----
// If unknown kind, treat as system error
⋮----
export function deserializeToolResult(serialized: SerializedToolResult): ToolExecutionResult
</file>

<file path="src/llm/types.ts">
/**
 * Clean LLM types with single responsibility
 * No agent or orchestration concerns
 */
⋮----
import { NDKAgent } from "@/events";
import { NDKKind, NDKProject, NDKTask } from "@nostr-dev-kit/ndk";
import type {
    LlmCompletionOpts,
    Message as LlmMessage,
    LlmResponse,
    LlmTool,
    LlmToolCall,
} from "multi-llm-ts";
⋮----
// Re-export multi-llm-ts types directly
export type Message = LlmMessage;
export type CompletionResponse = LlmResponse;
export type ToolDefinition = LlmTool;
export type ToolCall = LlmToolCall;
⋮----
// Extended completion options with routing context
export interface CompletionOptions extends LlmCompletionOpts {
    configName?: string;
    agentName?: string;
}
⋮----
// Model information that can be passed along with responses
export interface ModelInfo {
    contextWindow?: number;
    maxCompletionTokens?: number;
}
⋮----
// Import and re-export tool types
import type { Tool, ExecutionContext } from "@/tools/types";
⋮----
// Simplified completion request that uses multi-llm-ts types
export interface CompletionRequest {
    messages: Message[];
    options?: CompletionOptions;
    tools?: Tool[];
    toolContext?: ExecutionContext;
}
⋮----
// Streaming types
export type StreamEvent =
    | { type: "content"; content: string }
    | { type: "tool_start"; tool: string; args: Record<string, unknown> }
    | { type: "tool_complete"; tool: string; result: unknown }
    | { type: "error"; error: string }
    | { type: "done"; response: CompletionResponse };
⋮----
/**
 * Pure LLM service interface - single responsibility
 */
export interface LLMService {
    complete(request: CompletionRequest): Promise<CompletionResponse>;
    stream(request: CompletionRequest): AsyncIterable<StreamEvent>;
}
⋮----
complete(request: CompletionRequest): Promise<CompletionResponse>;
stream(request: CompletionRequest): AsyncIterable<StreamEvent>;
⋮----
/**
 * LLM configuration - decoupled from implementation
 */
export interface LLMConfig {
    provider:
        | "anthropic"
        | "openai"
        | "google"
        | "ollama"
        | "mistral"
        | "groq"
        | "openrouter"
        | "deepseek";
    model: string;
    apiKey?: string;
    baseUrl?: string;
    enableCaching?: boolean;
    temperature?: number;
    maxTokens?: number;
    defaultOptions?: Partial<CompletionOptions>;
}
⋮----
/**
 * LLM Provider types
 */
export type LLMProvider =
    | "openai"
    | "openrouter"
    | "anthropic"
    | "google"
    | "groq"
    | "deepseek"
    | "ollama"
    | "mistral";
⋮----
/**
 * Event kinds used in the TENEX system
 */
⋮----
/**
 * LLM configurations collection
 */
export type LLMConfigs = Record<string, LLMConfig>;
⋮----
/**
 * LLM Preset configuration
 */
export interface LLMPreset {
    provider: LLMProvider;
    model: string;
    enableCaching?: boolean;
    temperature?: number;
    maxTokens?: number;
}
⋮----
/**
 * Provider authentication
 */
export interface ProviderAuth {
    apiKey?: string;
    baseUrl?: string;
    headers?: Record<string, string>;
}
</file>

<file path="src/logging/__tests__/ExecutionLogger.test.ts">
import { describe, it, expect, beforeEach, mock, spyOn } from "bun:test";
import { ExecutionLogger, createExecutionLogger } from "../ExecutionLogger";
import type { TracingContext } from "@/tracing";
import { createTracingLogger } from "@/tracing";
import type { Phase } from "@/conversations/phases";
⋮----
// Mock dependencies
⋮----
// Mock chalk to avoid color codes in tests
⋮----
const mockChalk: any = (str: string)
⋮----
// Mock tracing logger
⋮----
// Reset all mocks
⋮----
// Mock console.log to capture output
⋮----
// Create test context
⋮----
// Create logger instance
⋮----
// Note: routing decision doesn't call tracingLogger in the implementation
⋮----
// Verify truncation happened (60 chars + "...")
⋮----
// Test seconds
⋮----
totalDuration: 45000, // 45 seconds
⋮----
// Reset spy
⋮----
// Test minutes
⋮----
totalDuration: 125000, // 2m 5s
⋮----
expect(outputStr).toContain("{...}"); // object formatting
expect(outputStr).toContain("..."); // parameter truncation
⋮----
// Agent thinking without context
</file>

<file path="src/logging/ExecutionLogger.ts">
import { type LogModule, logger as baseLogger } from "@/utils/logger";
import type { TracingContext, TracingLogger } from "@/tracing";
import { createTracingLogger } from "@/tracing";
import chalk from "chalk";
import type { Phase } from "@/conversations/phases";
import type { Agent } from "@/agents/types";
⋮----
/**
 * Event types for structured logging
 */
export type EventType =
    | "agent_thinking"
    | "agent_decision"
    | "agent_handoff"
    | "phase_transition_trigger"
    | "phase_transition_decision"
    | "phase_transition_executed"
    | "routing_analysis"
    | "routing_decision"
    | "tool_execution_start"
    | "tool_execution_complete"
    | "conversation_start"
    | "conversation_complete"
    | "execution_flow_start"
    | "execution_flow_complete";
⋮----
export interface AgentThinkingEvent {
    type: "agent_thinking";
    agent: string;
    reasoning: string;
    context: {
        userMessage?: string;
        considerations?: string[];
        leaningToward?: string;
        confidence?: number;
    };
}
⋮----
export interface AgentDecisionEvent {
    type: "agent_decision";
    agent: string;
    decisionType: "routing" | "tool_use" | "phase_transition" | "completion";
    decision: string;
    reasoning: string;
    confidence?: number;
    alternatives?: string[];
}
⋮----
export interface AgentHandoffEvent {
    type: "agent_handoff";
    from: string;
    to: string;
    task: string;
    context?: string;
    phase: Phase;
}
⋮----
export interface PhaseTransitionTriggerEvent {
    type: "phase_transition_trigger";
    conversationId: string;
    currentPhase: Phase;
    trigger: string;
    triggerAgent: string;
    signal: string;
}
⋮----
export interface PhaseTransitionDecisionEvent {
    type: "phase_transition_decision";
    conversationId: string;
    from: Phase;
    to: Phase;
    decisionBy: string;
    reason: string;
    confidence?: number;
}
⋮----
export interface PhaseTransitionExecutedEvent {
    type: "phase_transition_executed";
    conversationId: string;
    from: Phase;
    to: Phase;
    handoffTo?: string;
    handoffMessage?: string;
    duration?: number;
}
⋮----
export interface RoutingAnalysisEvent {
    type: "routing_analysis";
    agent: string;
    messageAnalysis: string;
    candidateAgents: string[];
    phaseConsiderations?: string;
}
⋮----
export interface RoutingDecisionEvent {
    type: "routing_decision";
    agent: string;
    targetAgents: string[];
    targetPhase?: Phase;
    reason: string;
    confidence?: number;
}
⋮----
export interface ToolExecutionStartEvent {
    type: "tool_execution_start";
    agent: string;
    tool: string;
    parameters?: Record<string, unknown>;
}
⋮----
export interface ToolExecutionCompleteEvent {
    type: "tool_execution_complete";
    agent: string;
    tool: string;
    status: "success" | "error";
    duration: number;
    result?: string;
    error?: string;
}
⋮----
export interface ConversationStartEvent {
    type: "conversation_start";
    conversationId: string;
    title?: string;
    userMessage: string;
    eventId?: string;
}
⋮----
export interface ConversationCompleteEvent {
    type: "conversation_complete";
    conversationId: string;
    finalPhase: Phase;
    totalDuration: number;
    success: boolean;
}
⋮----
export interface ExecutionFlowStartEvent {
    type: "execution_flow_start";
    conversationId: string;
    narrative: string;
}
⋮----
export interface ExecutionFlowCompleteEvent {
    type: "execution_flow_complete";
    conversationId: string;
    narrative: string;
    success: boolean;
}
⋮----
export type LogEvent =
    | AgentThinkingEvent
    | AgentDecisionEvent
    | AgentHandoffEvent
    | PhaseTransitionTriggerEvent
    | PhaseTransitionDecisionEvent
    | PhaseTransitionExecutedEvent
    | RoutingAnalysisEvent
    | RoutingDecisionEvent
    | ToolExecutionStartEvent
    | ToolExecutionCompleteEvent
    | ConversationStartEvent
    | ConversationCompleteEvent
    | ExecutionFlowStartEvent
    | ExecutionFlowCompleteEvent;
⋮----
/**
 * Unified execution logger for structured event logging
 */
export class ExecutionLogger
⋮----
constructor(
        private context: TracingContext,
        private module: LogModule = "agent"
)
⋮----
/**
     * Update context (e.g., when agent changes)
     */
updateContext(context: TracingContext): void
⋮----
/**
     * Log an event with structured formatting
     */
logEvent(event: LogEvent): void
⋮----
// Agent Events
private logAgentThinking(event: AgentThinkingEvent): void
⋮----
private logAgentDecision(event: AgentDecisionEvent): void
⋮----
private logAgentHandoff(event: AgentHandoffEvent): void
⋮----
// Phase Transition Events
private logPhaseTransitionTrigger(event: PhaseTransitionTriggerEvent): void
⋮----
private logPhaseTransitionDecision(event: PhaseTransitionDecisionEvent): void
⋮----
private logPhaseTransitionExecuted(event: PhaseTransitionExecutedEvent): void
⋮----
// Routing Events
private logRoutingAnalysis(event: RoutingAnalysisEvent): void
⋮----
private logRoutingDecision(event: RoutingDecisionEvent): void
⋮----
// Tool Execution Events
private logToolExecutionStart(event: ToolExecutionStartEvent): void
⋮----
private logToolExecutionComplete(event: ToolExecutionCompleteEvent): void
⋮----
// Conversation Events
private logConversationStart(event: ConversationStartEvent): void
⋮----
private logConversationComplete(event: ConversationCompleteEvent): void
⋮----
private logExecutionFlowComplete(event: ExecutionFlowCompleteEvent): void
⋮----
// Helper methods
private truncate(text: string, maxLength: number): string
⋮----
private shortId(id: string): string
⋮----
private formatParams(params: Record<string, unknown>): string
⋮----
private formatValue(value: unknown): string
⋮----
private formatDuration(ms: number): string
⋮----
// Quick logging methods
agentThinking(agent: string, reasoning: string, context?: AgentThinkingEvent["context"]): void
⋮----
agentDecision(
        agent: string, 
        decisionType: AgentDecisionEvent["decisionType"], 
        decision: string,
        reasoning: string,
        options?: { confidence?: number; alternatives?: string[] }
): void
⋮----
routingDecision(
        agent: string,
        targetAgents: string[],
        reason: string,
        options?: { targetPhase?: Phase; confidence?: number }
): void
⋮----
toolStart(agent: string, tool: string, parameters?: Record<string, unknown>): void
⋮----
toolComplete(
        agent: string, 
        tool: string, 
        status: "success" | "error", 
        duration: number,
        options?: { result?: string; error?: string }
): void
⋮----
/**
 * Create an execution logger instance
 */
export function createExecutionLogger(context: TracingContext, module?: LogModule): ExecutionLogger
</file>

<file path="src/nostr/__tests__/NostrPublisher.test.ts">
import { describe, it, expect, beforeEach, afterEach, mock, spyOn } from "bun:test";
import type { NDKEvent, NDKTag } from "@nostr-dev-kit/ndk";
import { NostrPublisher, type NostrPublisherContext, type ResponseOptions } from "../NostrPublisher";
import type { Agent } from "@/agents/types";
import type { ConversationManager } from "@/conversations/ConversationManager";
import type { Conversation } from "@/conversations/types";
import { TypingIndicatorManager } from "../TypingIndicatorManager";
⋮----
import { logger } from "@/utils/logger";
import { Message } from "multi-llm-ts";
import { EVENT_KINDS } from "@/llm/types";
⋮----
// Mock NDKEvent at module level
const createMockNDKEvent = () =>
⋮----
// Keep track of created events
⋮----
// Mock NDKEvent constructor
⋮----
// Mock TypingIndicatorManager
⋮----
// Reset the current mock event before each test
⋮----
// Setup mock conversation
⋮----
// Setup mock conversation manager
⋮----
// Setup mock agent
⋮----
// Setup mock triggering event
⋮----
// Setup mock project context
⋮----
// Mock getProjectContext
⋮----
// Mock getNDK
⋮----
// Spy on logger methods
⋮----
// Setup context
⋮----
// Create publisher instance
⋮----
// Verify the event was created correctly
⋮----
// Verify conversation was updated
⋮----
// Verify logging - check both metadata and published response logs
⋮----
// Verify LLM metadata tags were added
⋮----
// Verify routing metadata was added
⋮----
// Verify p-tags were added
⋮----
// Verify additional tags were added
⋮----
// Verify event was not published
⋮----
// Verify error was logged
⋮----
// Verify event was not published
⋮----
// Verify message was added to context
⋮----
// Override the reply method to return an event with failing sign
⋮----
// Verify conversation was saved before the failure
⋮----
// Override the reply method to return an event with failing publish
⋮----
// Verify conversation was saved before the failure
⋮----
// Override reply to return an event with failing publish
⋮----
// Mock NDKEvent constructor to return event with failing publish
⋮----
// Check if error was logged
⋮----
// Verify all metadata was added
</file>

<file path="src/nostr/__tests__/TaskPublisher.test.ts">
import { describe, it, expect, beforeEach, afterEach, mock, spyOn } from "bun:test";
import type NDK from "@nostr-dev-kit/ndk";
import { NDKTask } from "@nostr-dev-kit/ndk";
import { TaskPublisher } from "../TaskPublisher";
import type { Agent } from "@/agents/types";
⋮----
import { logger } from "@/utils/logger";
⋮----
// Create a factory for mock tasks
const createMockTask = () =>
⋮----
// Keep track of created tasks
⋮----
// Mock NDKTask at module level
⋮----
// Reset the current mock task before each test
⋮----
// Setup mocks
⋮----
// Mock getProjectContext
⋮----
// Spy on logger
⋮----
// Create TaskPublisher instance
⋮----
// Test by trying to publish progress (which requires currentTask)
⋮----
// Create a new TaskPublisher with a mock that will fail on sign
⋮----
// Create a new TaskPublisher with a mock that will fail on publish
⋮----
// First create a task
⋮----
// Note: completeTask method is not fully implemented in the source
// This test validates the error handling for now
⋮----
// First create a task
⋮----
// Create a task first
⋮----
// Mock reply to throw error on publish
⋮----
// Should not throw, but log the error
⋮----
// The implementation modifies the reply object that was returned
⋮----
// Modify the reply method to return an object with p tags
⋮----
// Get the reply object that was created and modified
⋮----
// p tags should be filtered out
⋮----
// Create first task
⋮----
// Store the reply mock count for task1
⋮----
// Create second task (should replace current task)
⋮----
// Progress should be published to the second task
⋮----
// task1.reply should not have been called again after task2 was created
⋮----
// Check if the logger was called with the expected pattern
</file>

<file path="src/nostr/__tests__/TypingIndicatorManager.integration.test.ts">
import { describe, it, expect, mock } from "bun:test";
import { TypingIndicatorManager } from "../TypingIndicatorManager";
import type { NostrPublisher } from "../NostrPublisher";
import type { NDKEvent } from "@nostr-dev-kit/ndk";
⋮----
// Scenario: Rapid typing indicators that should not flicker
⋮----
// First typing indicator
⋮----
// Quick stop and start again (simulating rapid messages)
⋮----
// Check current state
⋮----
// Verify no stop was published yet
⋮----
// Request final stop to trigger the delayed stop
⋮----
// Since we started at time 0, and last start was at ~200ms,
// we need to wait until 5000ms from the first start
const waitTime = 5000 - (Date.now() - startTime) + 100; // +100ms buffer
⋮----
// Clean up
</file>

<file path="src/nostr/__tests__/TypingIndicatorManager.manual.test.ts">
/**
 * Manual test to demonstrate typing indicator behavior
 * Run with: npm test -- src/nostr/__tests__/TypingIndicatorManager.manual.test.ts
 */
⋮----
import { describe, it } from "bun:test";
import { TypingIndicatorManager } from "../TypingIndicatorManager";
import type { NostrPublisher } from "../NostrPublisher";
import type { NDKEvent } from "@nostr-dev-kit/ndk";
⋮----
// Simulate rapid typing indicators
⋮----
// Wait to see when stop is published
⋮----
}, 10000); // 10 second timeout for this test
</file>

<file path="src/nostr/__tests__/TypingIndicatorManager.test.ts">
import { describe, it, expect, beforeEach, afterEach, mock, spyOn } from "bun:test";
import { TypingIndicatorManager } from "../TypingIndicatorManager";
import type { NostrPublisher } from "../NostrPublisher";
import type { NDKEvent } from "@nostr-dev-kit/ndk";
import { logger } from "@/utils/logger";
⋮----
// Request stop immediately
⋮----
// Stop should not be published immediately
⋮----
// Wait for minimum duration (5 seconds)
⋮----
// Now stop should have been called
⋮----
// Rapid sequence: start -> stop -> start -> stop
⋮----
// Only two start calls should have been made
⋮----
// No stop call should have been made yet
⋮----
// After 5 seconds, stop should be called once
⋮----
// Multiple stop requests
⋮----
// Wait for stop to trigger
⋮----
// Should only publish stop once
⋮----
// Wait 2 seconds
⋮----
// Start typing again (updates message but doesn't reset timer)
⋮----
// Request stop
⋮----
// Should stop after 3 more seconds (5 total from first start)
⋮----
// Note: Bun doesn't have built-in fake timers like Vitest
// This test is kept for reference but skipped
// The actual timing tests above use real timeouts
⋮----
// Create a new manager instance for this test
⋮----
// Wait for stop to execute
⋮----
// Should log error
⋮----
// Manager should reset state despite error
⋮----
// Make publishTypingIndicatorRaw throw for stop
⋮----
// Should log error
⋮----
// Manager should reset state despite error
⋮----
// Execute multiple starts concurrently
⋮----
// Should be typing with state consistency
⋮----
// All start calls should have gone through
⋮----
// Request stop
⋮----
// Wait 2 seconds (less than minimum)
⋮----
// Start again (should cancel the pending stop)
⋮----
// Wait for original stop time to pass
⋮----
// Should still be typing (stop was cancelled)
⋮----
// Stop should not have been called
⋮----
// Cleanup while stop is pending
⋮----
// Wait for stop duration
⋮----
// Stop should not have been called (timer was cleared)
⋮----
// Should not call publisher
⋮----
// Should not call publisher
⋮----
// Start again without message (should keep previous)
⋮----
// Should use the first message for the second call
⋮----
// Should have called 3 times (initial + 2 retries)
⋮----
// Should have taken at least 3 seconds (1s + 2s delays)
⋮----
expect(true).toBe(false); // Should not reach here
⋮----
// State should be reset after failure
⋮----
// Wait exactly 5 seconds
⋮----
// Stop should execute quickly now
⋮----
// Small delay for execution
⋮----
// ForceStop should have executed immediately
</file>

<file path="src/nostr/index.ts">
// Centralized publisher
</file>

<file path="src/nostr/ndkClient.ts">
import { getRelayUrls } from "@/utils/relays";
/**
 * TENEX CLI: NDK Singleton
 * Manages a single NDK instance for the CLI
 */
import NDK from "@nostr-dev-kit/ndk";
⋮----
export async function initNDK(): Promise<void>
⋮----
// Disconnect existing instance
⋮----
export function getNDK(): NDK
⋮----
export async function shutdownNDK(): Promise<void>
⋮----
// Disconnect all relays
</file>

<file path="src/nostr/NostrPublisher.ts">
import type { Agent } from "@/agents/types";
import { getTotalExecutionTimeSeconds } from "@/conversations/executionTime";
import type { Conversation } from "@/conversations/types";
import type { ConversationManager } from "@/conversations/ConversationManager";
import { EVENT_KINDS } from "@/llm/types";
import { getNDK } from "@/nostr";
import { EXECUTION_TAGS } from "@/nostr/tags";
import type { LLMMetadata } from "@/nostr/types";
import { getProjectContext } from "@/services";
import type { ContinueFlow, Complete, EndConversation } from "@/tools/types";
import { logger } from "@/utils/logger";
import { NDKEvent, type NDKTag } from "@nostr-dev-kit/ndk";
import { Message } from "multi-llm-ts";
import { TypingIndicatorManager } from "./TypingIndicatorManager";
⋮----
// Tool execution status interface (from ToolExecutionPublisher)
export interface ToolExecutionStatus {
    tool: string;
    status: "starting" | "running" | "completed" | "failed";
    args?: Record<string, unknown>;
    result?: unknown;
    error?: string;
    duration?: number;
}
⋮----
// Context passed to publisher on creation
export interface NostrPublisherContext {
    conversationId: string;
    agent: Agent;
    triggeringEvent: NDKEvent;
    conversationManager: ConversationManager;
}
⋮----
// Options for publishing responses
export interface ResponseOptions {
    content: string;
    llmMetadata?: LLMMetadata;
    continueMetadata?: ContinueFlow;
    completeMetadata?: Complete | EndConversation;
    additionalTags?: NDKTag[];
    destinationPubkeys?: string[];
}
⋮----
// TENEX logging types
export interface TenexLogData {
    event: string;
    agent: string;
    details: Record<string, unknown>;
    timestamp?: number;
}
⋮----
// Metadata for finalizing stream
export interface FinalizeMetadata {
    llmMetadata?: LLMMetadata;
    continueMetadata?: ContinueFlow;
    completeMetadata?: Complete | EndConversation;
}
⋮----
export class NostrPublisher
⋮----
constructor(public readonly context: NostrPublisherContext)
⋮----
/**
     * Clean up any resources (e.g., pending timers).
     */
cleanup(): void
⋮----
private getConversation(): Conversation
⋮----
/**
     * Publishes an agent's response to Nostr and updates the conversation state.
     *
     * IMPORTANT: This method follows a save-then-publish pattern for transactional integrity:
     * 1. First updates the conversation state in memory
     * 2. Then saves the conversation to persistent storage
     * 3. Only after successful save does it publish to Nostr
     *
     * This ensures that we never have events on the network that aren't reflected
     * in our local state, preventing state inconsistencies.
     */
async publishResponse(options: ResponseOptions): Promise<NDKEvent>
⋮----
// Just use the content provided by the caller
⋮----
// Add metadata tags
⋮----
// Debug logging for metadata
⋮----
// Add p-tags for destination pubkeys if provided
⋮----
// Add any additional tags
⋮----
// With the new simplified system, we don't need to manually add messages to context
// The conversation history (NDKEvents) is the source of truth
// Just save the conversation state BEFORE publishing
⋮----
// Sign and publish only after local state is successfully updated
⋮----
async publishError(message: string): Promise<NDKEvent>
⋮----
async publishTenexLog(logData: TenexLogData): Promise<NDKEvent>
⋮----
// Set timestamp
⋮----
// Create structured content
⋮----
// Add base tags
⋮----
// Add conversation reference
⋮----
// Add TENEX-specific tags
⋮----
async publishTypingIndicator(state: "start" | "stop", message?: string): Promise<NDKEvent | void>
⋮----
// Use the typing indicator manager for start calls
⋮----
// For stop calls, use the manager's stop method which handles timing
⋮----
/**
     * Internal method used by TypingIndicatorManager to publish raw typing events.
     * This bypasses the timing logic and publishes immediately.
     */
async publishTypingIndicatorRaw(state: "start" | "stop", message?: string): Promise<NDKEvent>
⋮----
// Use provided message or default
⋮----
// Add base tags (project, phase)
⋮----
// Add conversation references
⋮----
async publishToolStatus(status: ToolExecutionStatus): Promise<NDKEvent>
⋮----
// Add base tags
⋮----
// Add tool-specific tags
⋮----
// Build human-readable content (keeping existing format)
⋮----
// await event.publish();
⋮----
createStreamPublisher(): StreamPublisher
⋮----
// Private helper methods
public createBaseReply(): NDKEvent
⋮----
// When the triggering event has E tag, replace e tag with E tag value
⋮----
private addBaseTags(event: NDKEvent): void
⋮----
// Always add project tag
⋮----
// Always add current phase tag from the single source of truth
⋮----
// Always add execution time tag using the fresh conversation object
⋮----
private cleanPTags(event: NDKEvent): void
⋮----
// Remove all p-tags added by NDK's reply() method to ensure clean routing
⋮----
private addLLMMetadata(event: NDKEvent, metadata?: LLMMetadata): void
⋮----
private addRoutingMetadata(event: NDKEvent, continueMetadata?: ContinueFlow): void
⋮----
// Add phase information
⋮----
// Only add phase-transition tag if phase is actually changing
⋮----
// Add routing reason
⋮----
// Routing message no longer exists - content is used instead
⋮----
// Add routing context summary if provided
⋮----
// Add agents as a tag for debugging/tracing
⋮----
export class StreamPublisher
⋮----
private pendingContent = ""; // Content waiting to be published
private accumulatedContent = ""; // Total content accumulated so far
⋮----
private static readonly FLUSH_DELAY_MS = 100; // Delay before actually publishing
private static readonly SENTENCE_ENDINGS = /[.!?](?:\s|$)/; // Regex to detect sentence endings
⋮----
constructor(private readonly publisher: NostrPublisher)
⋮----
addContent(content: string): void
⋮----
// Add content to buffers
⋮----
// Check if we should flush based on sentence endings
⋮----
// If no flush is scheduled, schedule one automatically
⋮----
// If we have a sentence ending and there's already a scheduled flush,
// cancel it and flush immediately
⋮----
private shouldFlushAtSentenceEnd(): boolean
⋮----
// Check if the pending content ends with a sentence ending
⋮----
// Only flush at sentence endings if enough time has passed since last flush
// This prevents too frequent flushing for rapid short sentences
⋮----
async flush(): Promise<void>
⋮----
// Skip if no content to flush or already finalized
⋮----
// If there's already a scheduled flush, we need to handle it
⋮----
// If a flush is already scheduled, publish it immediately and schedule the new content.
// This prioritizes latency for the first batch while still batching subsequent content.
⋮----
// Schedule this content to be published after a delay
// This balances between network efficiency (batching) and user experience (low latency)
⋮----
async finalize(metadata: FinalizeMetadata): Promise<NDKEvent | undefined>
⋮----
// Cancel any pending flush timeout
⋮----
// Move any scheduled content back to pending
⋮----
// Use accumulated content for the final reply, not just pending content
⋮----
// StreamPublisher only handles text streaming, not terminal tool publishing
// Terminal tools publish their own events directly
⋮----
isFinalized(): boolean
⋮----
getSequenceNumber(): number
⋮----
// Private helper methods
private cancelScheduledFlush(): void
⋮----
private async publishScheduledContent(): Promise<void>
⋮----
// Capture the state into local variables immediately
⋮----
// Clear scheduled state even if we return early
⋮----
// Clear the shared state *before* the async operation
⋮----
// Create streaming response event (ephemeral kind 21111)
⋮----
streamingEvent.kind = EVENT_KINDS.STREAMING_RESPONSE; // Ephemeral streaming response kind
streamingEvent.content = this.accumulatedContent; // Send complete status, not just the delta
⋮----
// Tag the conversation
// First try lowercase 'e', then uppercase 'E' for legacy support
⋮----
// Add agent identifier
⋮----
// Add streaming metadata
⋮----
// Update last flush time after successful publish
⋮----
// On failure, prepend content to the start of the pending buffer to be retried
⋮----
this.sequence--; // Roll back sequence number on failure
</file>

<file path="src/nostr/tags.ts">
/**
 * Strongly typed Nostr tag definitions for consistency across the codebase
 */
⋮----
// Status tags
⋮----
// LLM-related tags - consistent prefix for all LLM metadata
⋮----
// Session and execution tags
⋮----
// Claude-specific tags
</file>

<file path="src/nostr/TaskPublisher.ts">
import type { Agent } from "@/agents/types";
import { getProjectContext } from "@/services";
import { logger } from "@/utils/logger";
import type NDK from "@nostr-dev-kit/ndk";
import { NDKTask } from "@nostr-dev-kit/ndk";
⋮----
export interface TaskCreationOptions {
    title: string;
    prompt: string;
    branch?: string;
    conversationRootEventId?: string;
}
⋮----
export interface TaskCompletionOptions {
    sessionId?: string;
    totalCost?: number;
    messageCount?: number;
    duration?: number;
    error?: string;
}
⋮----
/**
 * Publishes NDKTask events to Nostr
 * Single Responsibility: Manage the lifecycle of NDKTask events (create and complete)
 */
export class TaskPublisher
⋮----
constructor(
⋮----
async createTask(options: TaskCreationOptions): Promise<NDKTask>
⋮----
// Tag the project
⋮----
// Add branch tag if provided
⋮----
// Link to conversation if provided
⋮----
// Sign with the agent's signer
⋮----
// we want to allow any message that prefaces the creation of this task to be published
// setTimeout(task.publish, 500);
⋮----
// Store the task instance for future operations
⋮----
async completeTask(success: boolean, options: TaskCompletionOptions): Promise<void>
⋮----
async publishTaskProgress(content: string, sessionId?: string): Promise<void>
⋮----
// Create a proper reply using the task event
⋮----
// remove all p tags
⋮----
// Add session ID if available
⋮----
// Add project tag
⋮----
// Sign with the agent's signer
</file>

<file path="src/nostr/types.ts">
import type { NDKEvent } from "@nostr-dev-kit/ndk";
⋮----
export interface LLMMetadata {
    model: string;
    cost: number;
    promptTokens: number;
    completionTokens: number;
    totalTokens: number;
    contextWindow?: number;
    maxCompletionTokens?: number;
    systemPrompt?: string;
    userPrompt?: string;
    rawResponse?: string;
}
⋮----
export interface PublishOptions {
    llmMetadata?: LLMMetadata;
    metadata?: Record<string, string | number | boolean | string[]>;
}
</file>

<file path="src/nostr/TypingIndicatorManager.ts">
import { logger } from "@/utils/logger";
import type { NostrPublisher } from "./NostrPublisher";
import type { NDKEvent } from "@nostr-dev-kit/ndk";
⋮----
/**
 * Manages typing indicator state to prevent flickering and ensure minimum duration.
 * 
 * Key features:
 * - Ensures typing indicators remain visible for at least 5 seconds
 * - Prevents flickering when multiple messages are sent rapidly
 * - Debounces stop events to batch rapid changes
 */
export class TypingIndicatorManager
⋮----
private static readonly MINIMUM_DURATION_MS = 5000; // 5 seconds
private static readonly DEBOUNCE_DELAY_MS = 200; // 200ms debounce for rapid messages
⋮----
private static readonly RETRY_DELAY_MS = 1000; // 1 second between retries
⋮----
constructor(publisher: NostrPublisher)
⋮----
/**
     * Start or update the typing indicator.
     * If already typing, this will update the message and reset the timer.
     */
async start(message?: string): Promise<NDKEvent>
⋮----
// Clear any pending stop timer
⋮----
// If not currently typing, record start time
⋮----
// Update the message if provided
⋮----
// Publish the typing indicator using the raw method to bypass timing logic
// Type assertion needed as publishTypingIndicatorRaw is an internal method
⋮----
publishTypingIndicatorRaw(state: "start" | "stop", message?: string): Promise<NDKEvent>;
⋮----
this.retryCount = 0; // Reset retry count on success
⋮----
// Attempt retry with exponential backoff
⋮----
return this.start(message); // Recursive retry
⋮----
// After max retries, reset state and throw
⋮----
/**
     * Request to stop the typing indicator.
     * This will be delayed to ensure minimum duration is met.
     */
async stop(): Promise<void>
⋮----
return; // Already stopped
⋮----
// Clear any existing stop timer
⋮----
// Calculate how long we've been typing
⋮----
// Schedule the actual stop
⋮----
// Always reset state on error to prevent stuck indicators
⋮----
// Ensure timer is cleared
⋮----
/**
     * Force immediate stop, bypassing minimum duration.
     * Use this only when necessary (e.g., on error or shutdown).
     */
async forceStop(): Promise<void>
⋮----
// Clear any pending stop timer
⋮----
// Always reset state on error
⋮----
// Always reset state when force stopping
⋮----
/**
     * Check if currently typing.
     */
isCurrentlyTyping(): boolean
⋮----
/**
     * Clean up any pending timers.
     */
cleanup(): void
</file>

<file path="src/nostr/utils.ts">
import { getProjectContext, isProjectContextInitialized } from "@/services";
import type { NDKEvent } from "@nostr-dev-kit/ndk";
⋮----
/**
 * Check if an event is from an agent (either project agent or individual agent)
 * @param event - The NDK event to check
 * @returns true if the event is from an agent, false if from a user
 */
export function isEventFromAgent(event: NDKEvent): boolean
⋮----
// Check if it's from the project itself
⋮----
// Check if it's from any of the registered agents
⋮----
/**
 * Check if an event is from a user (not from an agent)
 * @param event - The NDK event to check
 * @returns true if the event is from a user, false if from an agent
 */
export function isEventFromUser(event: NDKEvent): boolean
⋮----
/**
 * Get the agent slug if the event is from an agent
 * @param event - The NDK event to check
 * @returns The agent slug if found, undefined otherwise
 */
export function getAgentSlugFromEvent(event: NDKEvent): string | undefined
⋮----
// Project context not initialized
</file>

<file path="src/prompts/__tests__/FragmentRegistry.test.ts">
import { FragmentRegistry } from "../core/FragmentRegistry";
import type { PromptFragment } from "../core/types";
</file>

<file path="src/prompts/__tests__/PromptBuilder.test.ts">
import { PromptBuilder } from "../core/PromptBuilder";
import { fragmentRegistry } from "../core/FragmentRegistry";
import type { PromptFragment } from "../core/types";
⋮----
// Clear the global registry before each test
⋮----
// Restore registry state
⋮----
interface TestArgs {
                name: string;
                value: number;
            }
⋮----
fragmentRegistry.register({ id: "f2", template: () => "Middle" }); // default 50
⋮----
interface UserArgs {
                user: { name: string; role: string };
                permissions: string[];
            }
⋮----
// Manually add a fragment config without registering the fragment
</file>

<file path="src/prompts/core/FragmentRegistry.ts">
import type { PromptFragment } from "./types";
⋮----
export class FragmentRegistry
⋮----
register<T>(fragment: PromptFragment<T>): void
⋮----
get(id: string): PromptFragment<unknown> | undefined
⋮----
has(id: string): boolean
⋮----
clear(): void
⋮----
getAllIds(): string[]
</file>

<file path="src/prompts/core/index.ts">

</file>

<file path="src/prompts/core/PromptBuilder.ts">
import { fragmentRegistry } from "./FragmentRegistry";
import type { FragmentConfig, PromptFragment } from "./types";
⋮----
export class PromptBuilder
⋮----
add<T>(fragmentId: string, args: T, condition?: (args: T) => boolean): this
⋮----
addFragment<T>(fragment: PromptFragment<T>, args: T, condition?: (args: T) => boolean): this
⋮----
build(): string
⋮----
// Validate arguments if validator is provided
⋮----
clear(): this
⋮----
getFragmentCount(): number
</file>

<file path="src/prompts/core/types.ts">
export interface PromptFragment<T = unknown> {
    id: string;
    priority?: number;
    template: (args: T) => string;
    validateArgs?: (args: unknown) => args is T;
    expectedArgs?: string; // Description of expected arguments for error messages
}
⋮----
expectedArgs?: string; // Description of expected arguments for error messages
⋮----
export interface FragmentConfig {
    fragmentId: string;
    args: unknown;
    condition?: (args: unknown) => boolean;
}
</file>

<file path="src/prompts/fragments/__tests__/agent-execution.test.ts">
import { PromptBuilder } from "@/prompts/core/PromptBuilder";
import { fragmentRegistry } from "@/prompts/core/FragmentRegistry";
import type { Agent } from "@/agents/types";
import type { Phase } from "@/conversations/phases";
import type { Conversation } from "@/conversations/types";
import type { NDKEvent } from "@nostr-dev-kit/ndk";
⋮----
// Import fragments to register them
⋮----
// Conversation history is handled as message array
⋮----
// Full prompt fragment doesn't exist, so we'll just verify components separately
⋮----
// Verify all components are present
</file>

<file path="src/prompts/fragments/__tests__/agent-tools.test.ts">
import { describe, it, expect } from "bun:test";
import type { Agent } from "@/agents/types";
import { agentToolsFragment } from "../agent-tools";
import { learnTool } from "@/tools/implementations/learn";
⋮----
// Check that the result contains the tool name and description
⋮----
// Check that the promptFragment is included
</file>

<file path="src/prompts/fragments/__tests__/agentFragments.test.ts">
import { describe, it, expect, beforeEach } from "bun:test";
import { phaseContextFragment } from "../agentFragments";
import type { Conversation, PhaseTransition } from "@/conversations/types";
import type { Phase } from "@/conversations/phases";
⋮----
// Transition to a different phase
⋮----
mockConversation.phase = "plan"; // Looking for plan, but only have execute
</file>

<file path="src/prompts/fragments/__tests__/available-agents.test.ts">
import { PromptBuilder } from "../../core/PromptBuilder";
import "../available-agents"; // Ensure fragment is registered
import type { Agent } from "@/agents/types";
⋮----
currentAgentPubkey: "pm123", // Orchestrator agent
⋮----
currentAgentPubkey: "dev456", // Specialist agent
</file>

<file path="src/prompts/fragments/__tests__/expertise-boundaries.test.ts">
import { describe, it, expect } from "bun:test";
import { expertiseBoundariesFragment } from "../expertise-boundaries";
</file>

<file path="src/prompts/fragments/__tests__/integration.test.ts">
import { PromptBuilder } from "../../core/PromptBuilder";
⋮----
import type { Agent } from "@/agents/types";
import type { Phase } from "@/conversations/phases";
import { describe, expect, it } from "bun:test";
⋮----
// Should have available agents
⋮----
// Should have orchestrator routing instructions
⋮----
// Should have routing rules
⋮----
// Should NOT have identity section
⋮----
// Should still have instructions and project context
⋮----
name: "Test Project", // This would be set by AgentRegistry
⋮----
// Should have identity section with project name
⋮----
// Should have instructions and project context
</file>

<file path="src/prompts/fragments/__tests__/mcp-tools.test.ts">
import { describe, it, expect, beforeEach, mock } from "bun:test";
import { fragmentRegistry } from "../../core/FragmentRegistry";
import { mcpService } from "@/services/mcp/MCPService";
import type { Tool } from "@/tools/types";
⋮----
// Mock MCP service
⋮----
// The fragment is auto-registered when imported
⋮----
// Check that we have one analytics server section
⋮----
// The tool names contain "analytics" too, so let's be more specific
⋮----
// Should have both tools
⋮----
// Should show parameter requirements correctly
⋮----
// Should have two server sections
⋮----
// server1 should have 2 tools
⋮----
// server2 should have 1 tool
⋮----
// Fragment doesn't add "No parameters required" text when there are no parameters
⋮----
// Should still include the tool even with empty description
⋮----
// Check that servers appear in alphabetical order
⋮----
// The fragment doesn't sort servers alphabetically, it keeps insertion order
</file>

<file path="src/prompts/fragments/__tests__/orchestrator-routing-clarity.test.ts">
import { describe, expect, it } from "bun:test";
import {
    orchestratorRoutingInstructionsFragment,
    orchestratorHandoffGuidanceFragment,
} from "../orchestrator-routing";
⋮----
// Clear requests
⋮----
// Complex clear requests
⋮----
// Ambiguous requests
⋮----
// Should not contain "complexity" or "complex" in the context of task assessment
⋮----
// Should show routing rules
</file>

<file path="src/prompts/fragments/__tests__/orchestrator-routing.test.ts">
import { PromptBuilder } from "../../core/PromptBuilder";
import "../orchestrator-routing"; // Ensure fragments are registered
</file>

<file path="src/prompts/fragments/__tests__/phase-definitions.test.ts">
import { describe, expect, it } from "bun:test";
import { phaseDefinitionsFragment } from "../phase-definitions";
⋮----
// Should contain all phase definitions
⋮----
// Should contain the enhanced descriptions for key phases
⋮----
// Should contain goals
</file>

<file path="src/prompts/fragments/__tests__/tool-registry.test.ts">
import { describe, test, expect } from "bun:test";
import { getTool } from "@/tools/registry";
⋮----
// Test that the tool registry has proper structure
⋮----
// The shape for a ZodObject has type "object" and properties
⋮----
// Test validation
</file>

<file path="src/prompts/fragments/__tests__/voice-mode.test.ts">
import { describe, expect, it } from "bun:test";
import type { NDKEvent } from "@nostr-dev-kit/ndk";
import { isVoiceMode } from "../voice-mode";
</file>

<file path="src/prompts/fragments/agent-common.ts">
// Common utility functions for agent prompts
⋮----
export function buildAgentIdentity(name: string, role?: string): string
⋮----
export function buildAgentPrompt(args: {
    name: string;
    role?: string;
    instructions: string;
    projectName?: string;
}): string
⋮----
// Identity
⋮----
// Instructions
⋮----
// Project context
</file>

<file path="src/prompts/fragments/agent-completion-guidance.ts">
import type { PromptFragment } from "../core/types";
import type { Phase } from "@/conversations/phases";
⋮----
interface CompletionGuidanceArgs {
    phase: Phase;
    isOrchestrator: boolean;
}
⋮----
/**
 * Provides clear guidance on when and how to use the complete() tool vs normal responses
 */
⋮----
// Orchestrator doesn't need completion guidance
⋮----
// Register the fragment
import { fragmentRegistry } from "../core/FragmentRegistry";
</file>

<file path="src/prompts/fragments/agent-reasoning.ts">
import type { PromptFragment } from "../core/types";
import { fragmentRegistry } from "../core/FragmentRegistry";
⋮----
/**
 * Fragment that instructs agents to output their reasoning in structured tags
 * This enables the ExecutionLogger to extract and display agent thinking
 */
⋮----
priority: 90, // High priority to ensure it's included early
⋮----
/**
 * Fragment for orchestrator agents to include routing reasoning
 */
⋮----
/**
 * Fragment for domain experts to include expertise reasoning
 */
⋮----
// Register fragments
</file>

<file path="src/prompts/fragments/agent-tools.ts">
import type { Agent } from "@/agents/types";
import { fragmentRegistry } from "@/prompts/core/FragmentRegistry";
import type { PromptFragment } from "@/prompts/core/types";
⋮----
interface AgentToolsArgs {
    agent: Agent;
}
⋮----
priority: 25, // Before MCP tools
⋮----
// Add header
⋮----
// Process each tool
⋮----
// Add tool name and description
⋮----
// Add custom prompt fragment if available
⋮----
// Register the fragment
</file>

<file path="src/prompts/fragments/agentFragments.ts">
import type { Agent } from "@/agents/types";
import type { Phase } from "@/conversations/phases";
import type { Conversation } from "@/conversations/types";
import { fragmentRegistry } from "../core/FragmentRegistry";
import type { PromptFragment } from "../core/types";
import { buildAgentPrompt } from "./agent-common";
import { agentCompletionGuidanceFragment } from "./agent-completion-guidance";
⋮----
// ========================================================================
// EXECUTION & SYSTEM PROMPT FRAGMENTS
// ========================================================================
⋮----
// Complete agent system prompt for execution
interface AgentSystemPromptArgs {
    agent: Agent;
    phase: Phase;
    projectTitle: string;
}
⋮----
// Orchestrator should not have identity section - it's invisible
⋮----
// Only add instructions for orchestrator, no identity
⋮----
// Add project context
⋮----
// Use shared agent prompt builder for non-orchestrator agents
⋮----
// Add completion guidance for non-orchestrator agents
⋮----
// ========================================================================
// CONVERSATION & INTERACTION FRAGMENTS
// ========================================================================
⋮----
// Conversation history handling instructions
interface ConversationHistoryInstructionsArgs {
    isOrchestrator: boolean;
}
⋮----
// Phase context
interface PhaseContextArgs {
    phase: Phase;
    phaseMetadata?: Record<string, unknown>;
    conversation?: Conversation;
}
⋮----
// Add phase-specific context from conversation transitions
⋮----
// ========================================================================
// HELPER FUNCTIONS
// ========================================================================
⋮----
function getPhaseContext(phase: Phase, conversation?: Conversation): string | null
⋮----
// Get the most recent transition into this phase
⋮----
// Register fragments
</file>

<file path="src/prompts/fragments/available-agents.ts">
import type { Agent } from "@/agents/types";
import { fragmentRegistry } from "../core/FragmentRegistry";
import type { PromptFragment } from "../core/types";
⋮----
// Available agents fragment - shows all agents available in the project
interface AvailableAgentsArgs {
    agents: Agent[];
    currentAgent?: Agent;
    currentAgentPubkey?: string;
}
⋮----
// Filter out current agent if specified
⋮----
// Determine if current agent is an orchestrator
⋮----
// Add role-specific guidance
⋮----
// Register the fragment
</file>

<file path="src/prompts/fragments/domain-expert-guidelines.ts">
import { fragmentRegistry } from "../core/FragmentRegistry";
import type { PromptFragment } from "../core/types";
⋮----
/**
 * Guidelines for domain expert agents when providing recommendations and reviews
 */
⋮----
// Register the fragment
</file>

<file path="src/prompts/fragments/execute-task-prompt.ts">
import { fragmentRegistry } from "../core/FragmentRegistry";
import type { PromptFragment } from "../core/types";
⋮----
// Task execution prompt fragment
interface TaskExecutionArgs {
    taskId: string;
    instruction: string;
}
⋮----
// Register the fragment
</file>

<file path="src/prompts/fragments/expertise-boundaries.ts">
import { fragmentRegistry } from "../core/FragmentRegistry";
import type { PromptFragment } from "../core/types";
⋮----
// Fragment for specialized agents to understand their expertise boundaries
interface ExpertiseBoundariesArgs {
    agentRole: string;
    isOrchestrator: boolean;
}
⋮----
// Only provide boundaries guidance for non-orchestrator agents
⋮----
// Register the fragment
</file>

<file path="src/prompts/fragments/inventory.ts">
import { fragmentRegistry } from "../core/FragmentRegistry";
import type { PromptFragment } from "../core/types";
⋮----
// Helper function to convert git status codes to descriptions
function getStatusDescription(status: string): string
⋮----
interface InventoryGenerationArgs {
    repomixContent: string;
    focusFiles?: Array<{ path: string; status: string }>;
}
⋮----
interface ModuleGuideArgs {
    repomixContent: string;
    moduleName: string;
    modulePath: string;
    complexityReason: string;
}
⋮----
interface ComplexModulesExtractionArgs {
    content: string;
}
⋮----
// Register fragments
</file>

<file path="src/prompts/fragments/mcp-tools.ts">
import { fragmentRegistry } from "@/prompts/core/FragmentRegistry";
import type { PromptFragment } from "@/prompts/core/types";
import { mcpService } from "@/services/mcp/MCPService";
import type { Tool } from "@/tools/types";
⋮----
interface MCPToolsArgs {
    enabled?: boolean;
    tools?: Tool[]; // Optional tools parameter
}
⋮----
tools?: Tool[]; // Optional tools parameter
⋮----
priority: 30, // After core tools but before agent-specific content
⋮----
// Use provided tools or get cached tools synchronously
⋮----
// Group tools by server
⋮----
// Generate markdown content
⋮----
// Add parameter information if available
⋮----
// Silent failure - don't break prompt generation
⋮----
// Register the fragment
</file>

<file path="src/prompts/fragments/orchestrator-routing.ts">
import { fragmentRegistry } from "../core/FragmentRegistry";
import type { PromptFragment } from "../core/types";
⋮----
// Orchestrator Agent routing decision instructions
⋮----
// Orchestrator Agent handoff guidance
⋮----
// Register Orchestrator routing fragments
</file>

<file path="src/prompts/fragments/phase-definitions.ts">
import { PHASE_DEFINITIONS, PHASES } from "@/conversations/phases";
import { fragmentRegistry } from "../core/FragmentRegistry";
import type { PromptFragment } from "../core/types";
⋮----
/**
 * Phase definitions fragment - provides all agents with clear understanding of what each phase means
 * This fragment is accessible to all agents so they understand the phase structure and expectations
 */
⋮----
priority: 15, // Higher priority to ensure it appears early in the prompt
⋮----
// Register the fragment
</file>

<file path="src/prompts/fragments/phase.ts">
import type { Phase } from "@/conversations/phases";
import { PHASE_DEFINITIONS } from "@/conversations/phases";
import { fragmentRegistry } from "../core/FragmentRegistry";
import type { PromptFragment } from "../core/types";
⋮----
// Phase constraints fragment - used by AgentExecutor
interface PhaseConstraintsArgs {
    phase: string;
}
⋮----
function getPhaseConstraints(phase: string): string[]
⋮----
// Register fragments
</file>

<file path="src/prompts/fragments/project-md.ts">
import { fragmentRegistry } from "@/prompts/core/FragmentRegistry";
import type { PromptFragment } from "@/prompts/core/types";
import fs from "node:fs";
import path from "node:path";
⋮----
interface ProjectMdArgs {
    projectPath?: string;
}
⋮----
// Ignore errors
⋮----
// Register the fragment
</file>

<file path="src/prompts/fragments/project.ts">
import type { Phase } from "@/conversations/phases";
import { logger } from "@/utils/logger";
import { fragmentRegistry } from "../core/FragmentRegistry";
import type { PromptFragment } from "../core/types";
⋮----
// Project inventory context fragment
interface InventoryContextArgs {
    phase: Phase;
}
⋮----
// Helper function to count total files recursively
function countTotalFiles(dir: string): number
⋮----
// Helper function to get project files (excluding dot files/dirs)
function getProjectFiles():
⋮----
// Helper function to count files in a directory (non-recursive, only direct children)
function countDirectFiles(dir: string): number
⋮----
// Helper function to build tree structure recursively
function buildTree(dir: string, prefix = "", isLast = true, showFiles = true): string[]
⋮----
// Filter and sort entries
⋮----
// Skip dot files/dirs and node_modules
⋮----
// Sort directories first, then files
⋮----
// Always show directories and recurse
⋮----
// Count only direct files in this directory
⋮----
// Always recurse into subdirectories
⋮----
// Skip dot files/dirs and node_modules
⋮----
// Sort directories first, then files
⋮----
// Count total files to decide whether to show individual files
⋮----
// Build the tree structure
⋮----
// If not showing files and there are files in the root directory, add a count
⋮----
// Helper function to load inventory and context synchronously
function loadProjectContextSync(phase: Phase):
⋮----
// Load inventory content for chat and brainstorm phases
⋮----
// Get list of context files
⋮----
// Context directory may not exist
⋮----
// If content is provided directly, use it; otherwise load from file
⋮----
// Get project files to determine if this is a fresh project
⋮----
// Count total files to determine if we should strongly recommend inventory generation
⋮----
// Add context files listing if available
⋮----
// Register fragments
</file>

<file path="src/prompts/fragments/referenced-article.ts">
import { fragmentRegistry } from "../core/FragmentRegistry";
⋮----
interface ReferencedArticleArgs {
    title: string;
    content: string;
    dTag: string;
}
⋮----
priority: 10, // High priority to appear early in the prompt
</file>

<file path="src/prompts/fragments/retrieved-lessons.ts">
import type { Agent } from "@/agents/types";
import type { Phase } from "@/conversations/phases";
import type { Conversation } from "@/conversations/types";
import type { NDKAgentLesson } from "@/events/NDKAgentLesson";
import { logger } from "@/utils/logger";
import { fragmentRegistry } from "../core/FragmentRegistry";
import type { PromptFragment } from "../core/types";
⋮----
// Retrieved lessons fragment - filters and formats relevant lessons from memory
interface RetrievedLessonsArgs {
    agent: Agent;
    phase: Phase;
    conversation: Conversation;
    agentLessons: Map<string, NDKAgentLesson[]>;
}
⋮----
priority: 24, // Before learn-tool-directive
⋮----
// Get only this agent's lessons
⋮----
return ""; // No lessons learned yet
⋮----
// Log lesson availability
⋮----
// Select top 5 lessons to show
⋮----
// Format lessons for the prompt
⋮----
// Use first sentence as summary to save tokens
⋮----
// Register the fragment
</file>

<file path="src/prompts/fragments/tool-use.ts">
import { fragmentRegistry } from "../core/FragmentRegistry";
import type { PromptFragment } from "../core/types";
⋮----
priority: 300, // Place after basic instructions but before specific tool instructions
⋮----
// Register the fragment
</file>

<file path="src/prompts/fragments/voice-mode.ts">
import { fragmentRegistry } from "../core/FragmentRegistry";
import type { PromptFragment } from "../core/types";
import type { NDKEvent } from "@nostr-dev-kit/ndk";
⋮----
export interface VoiceModeOptions {
    isVoiceMode: boolean;
}
⋮----
priority: 20, // High priority to ensure voice instructions are prominent
⋮----
// Register the fragment
⋮----
/**
 * Helper function to check if an event has voice mode enabled
 */
export function isVoiceMode(event: NDKEvent | undefined): boolean
</file>

<file path="src/prompts/utils/__tests__/systemPromptBuilder.test.ts">
import { describe, expect, it } from "bun:test";
import { buildSystemPrompt } from "../systemPromptBuilder";
import type { Agent } from "@/agents/types";
import { PHASES } from "@/conversations/phases";
// Import all required fragments
⋮----
// Orchestrator should not have the yield-back fragment section
</file>

<file path="src/prompts/utils/llmMetadata.ts">
import { openRouterPricing } from "@/llm/pricing";
import type { CompletionResponse } from "@/llm/types";
import type { LLMMetadata } from "@/nostr/types";
import type { Message } from "multi-llm-ts";
⋮----
interface ResponseWithUsage {
    usage?: {
        promptTokens: number;
        completionTokens: number;
        totalTokens?: number;
    };
    experimental_providerMetadata?: {
        openrouter?: { usage?: { total_cost?: number } };
    };
    model?: string;
}
⋮----
export async function buildLLMMetadata(
    response: CompletionResponse,
    messages: Message[]
): Promise<LLMMetadata | undefined>
⋮----
// Convert CompletionResponse to ResponseWithUsage format
⋮----
export async function calculateCost(response: ResponseWithUsage, model: string): Promise<number>
⋮----
// Check if OpenRouter already calculated the cost
⋮----
// Calculate cost based on model pricing
⋮----
// Fallback: rough estimate based on typical pricing
</file>

<file path="src/prompts/utils/messageBuilder.ts">
import type { Conversation } from "@/conversations/types";
import { getAgentSlugFromEvent, isEventFromUser } from "@/nostr/utils";
import { getProjectContext } from "@/services/ProjectContext";
import type { NDKEvent } from "@nostr-dev-kit/ndk";
import { Message } from "multi-llm-ts";
⋮----
/**
 * Convert NDKEvent array to Message array for LLM consumption
 */
export function buildHistoryMessages(history: NDKEvent[]): Message[]
⋮----
// It's from an agent, find out which one
⋮----
// Default to "Agent" if the agent can't be found for some reason
⋮----
// Add system message for attribution
⋮----
// Keep the original content clean
⋮----
/**
 * Check if we need to add a current user message
 * (when the last event in history isn't the current user request)
 */
export function needsCurrentUserMessage(conversation: Conversation): boolean
⋮----
/**
 * Extract the latest user message from conversation history
 */
export function getLatestUserMessage(conversation: Conversation): string | null
⋮----
// Find the most recent user message
</file>

<file path="src/prompts/utils/systemPromptBuilder.ts">
import type { Agent } from "@/agents/types";
import type { Phase } from "@/conversations/phases";
import type { Conversation } from "@/conversations/types";
import type { NDKAgentLesson } from "@/events/NDKAgentLesson";
import { PromptBuilder } from "@/prompts/core/PromptBuilder";
import type { Tool } from "@/tools/types";
⋮----
import { isVoiceMode } from "@/prompts/fragments/voice-mode";
import type { NDKEvent } from "@nostr-dev-kit/ndk";
⋮----
export interface BuildSystemPromptOptions {
    // Required data
    agent: Agent;
    phase: Phase;
    projectTitle: string;
    projectRepository?: string;

    // Optional runtime data
    availableAgents?: Agent[];
    conversation?: Conversation;
    agentLessons?: Map<string, NDKAgentLesson[]>;
    mcpTools?: Tool[];
    triggeringEvent?: NDKEvent;
}
⋮----
// Required data
⋮----
// Optional runtime data
⋮----
/**
 * Builds the system prompt for an agent using the exact same logic as production.
 * This is the single source of truth for system prompt generation.
 */
export function buildSystemPrompt(options: BuildSystemPromptOptions): string
⋮----
// Build system prompt with all agent and phase context
⋮----
// Add voice mode instructions if this is a voice mode event
// But skip for orchestrator since it doesn't speak to users
⋮----
// Add referenced article context if present
⋮----
// Add project inventory context only for non-orchestrator agents
⋮----
// Add PROJECT.md fragment only for project-manager
⋮----
// Only add MCP tools and reasoning instructions for non-orchestrator agents
⋮----
.add("agent-reasoning", {}) // Add reasoning instructions for non-orchestrator agents
⋮----
// .add("tool-use", {});
⋮----
// Add orchestrator-specific routing instructions for orchestrator agents using reason-act-loop backend
// The routing backend doesn't need these instructions as it uses structured output
⋮----
.add("orchestrator-reasoning", {}); // Add orchestrator-specific reasoning
⋮----
// Add expertise boundaries for non-orchestrator agents
⋮----
// Add domain expert guidelines for all non-orchestrator agents
⋮----
.add("expert-reasoning", {}); // Add expert-specific reasoning
</file>

<file path="src/prompts/index.ts">
// Export core functionality
⋮----
// Import all fragments to ensure they're registered when the module is imported
</file>

<file path="src/services/__tests__/ConfigService.test.ts">
import { describe, it, expect, beforeEach, afterEach } from "bun:test";
import { ConfigService } from "../ConfigService";
import { createTempDir, cleanupTempDir } from "@/test-utils";
⋮----
import { promises as fs } from "node:fs";
⋮----
// Clear singleton state
⋮----
// Clean up
⋮----
// Clear singleton and cache
⋮----
// Create an isolated project dir that doesn't load global config
⋮----
description: 123, // Invalid type
invalidField: "test" // Unknown field
⋮----
// Should throw on invalid type
⋮----
// First load
⋮----
// Modify file directly
⋮----
// Second load should return cached value
⋮----
expect(loaded2.description).toBe("Test"); // Still cached
⋮----
// Clear cache and load again
⋮----
expect(loaded3.description).toBe("Modified"); // Fresh from file
⋮----
// Load both
⋮----
// Modify config file
⋮----
// Clear only config cache
⋮----
// Config should be fresh, agents still cached
⋮----
// Write invalid JSON
⋮----
expect(loaded).toEqual({}); // Default value
⋮----
// Try to load from non-existent directory
⋮----
// Try to save to a file path (not a directory)
⋮----
// This should throw because it can't create .tenex directory
⋮----
// Save using convenience methods
⋮----
// Load using convenience methods
⋮----
// Note: loadConfig merges global and project configs
// Since we can't easily mock the global path, we'll test the merging logic
// by saving to different project directories
⋮----
// Load each individually to verify they saved correctly
</file>

<file path="src/services/config/__tests__/mcp-config.test.ts">
import { describe, it, expect } from "bun:test";
import { z } from "zod";
import { TenexMCPSchema, MCPServerConfigSchema } from "../types";
import type { TenexMCP, MCPServerConfig } from "../types";
⋮----
{ args: ["server.js"] }, // Missing command
{ command: "node" }, // Missing args
{}, // Missing both
⋮----
command: 123, // Should be string
⋮----
args: "server.js", // Should be array
⋮----
env: "API_KEY=secret", // Should be object
⋮----
allowedPaths: "/path1", // Should be array
⋮----
// Missing args
⋮----
// Simulate merge
⋮----
// Project should override global
⋮----
// All env values should remain as strings
</file>

<file path="src/services/config/types.ts">
import { z } from "zod";
⋮----
/**
 * Unified configuration types for TENEX
 * All configuration files use the same schemas for both global and project contexts
 */
⋮----
// =====================================================================================
// MAIN CONFIG SCHEMA (config.json)
// =====================================================================================
⋮----
export interface TenexConfig {
    // Global fields
    whitelistedPubkeys?: string[];

    // Project fields (optional for global config)
    description?: string;
    repoUrl?: string;
    projectNaddr?: string;
    paths?: {
        inventory?: string;
    };
}
⋮----
// Global fields
⋮----
// Project fields (optional for global config)
⋮----
// =====================================================================================
// AGENTS SCHEMA (agents.json)
// =====================================================================================
⋮----
export interface TenexAgents {
    [agentSlug: string]: {
        nsec: string;
        file: string;
        eventId?: string;
        orchestratorAgent?: boolean;
    };
}
⋮----
// =====================================================================================
// LLM SCHEMA (llms.json)
// =====================================================================================
⋮----
export interface TenexLLMs {
    configurations: {
        [namedConfig: string]: {
            provider:
                | "anthropic"
                | "openai"
                | "google"
                | "ollama"
                | "mistral"
                | "groq"
                | "openrouter"
                | "deepseek";
            model: string;
            temperature?: number;
            maxTokens?: number;
            enableCaching?: boolean;
        };
    };
    defaults?: {
        agents?: string;
        [agentSlug: string]: string | undefined;
    };
    credentials: {
        [namedCredential: string]: {
            apiKey?: string;
            baseUrl?: string;
            headers?: Record<string, string>;
        };
    };
}
⋮----
// =====================================================================================
// MCP SCHEMA (mcp.json)
// =====================================================================================
⋮----
export interface MCPServerConfig {
    command: string;
    args: string[];
    env?: Record<string, string>;
    description?: string;
    allowedPaths?: string[];
}
⋮----
export interface TenexMCP {
    servers: Record<string, MCPServerConfig>;
    enabled: boolean;
}
⋮----
// =====================================================================================
// LOADED CONFIGURATION STATE
// =====================================================================================
⋮----
export interface LoadedConfig {
    config: TenexConfig;
    agents: TenexAgents;
    llms: TenexLLMs;
    mcp: TenexMCP;
}
⋮----
// =====================================================================================
// HELPER TYPES
// =====================================================================================
⋮----
export type ConfigFile = "config.json" | "agents.json" | "llms.json" | "mcp.json";
⋮----
export interface ConfigPaths {
    global: string;
    project?: string;
}
</file>

<file path="src/services/mcp/__tests__/MCPService.integration.test.ts">
import { describe, it, expect, beforeEach, afterEach, spyOn } from "bun:test";
import { MCPService } from "../MCPService";
import { spawn } from "child_process";
⋮----
import { configService } from "@/services/ConfigService";
import type { TenexMCP } from "@/services/config/types";
⋮----
// Helper to create a simple MCP server for testing
async function createTestMCPServer(serverPath: string)
⋮----
// Reset singleton
⋮----
// Create test directories
⋮----
// Create test MCP server
⋮----
// Shutdown service
⋮----
// Clean up test files
⋮----
// Mock config loading
⋮----
// Initialize service
⋮----
// Verify server is running
⋮----
// Get available tools
⋮----
// Check tool details
⋮----
// Execute echo tool
⋮----
// Execute add tool
⋮----
// Get the process and kill it externally
⋮----
// Kill the process
⋮----
// Wait a bit for the process to die
⋮----
// Server should no longer be running
⋮----
// Tool execution should fail
⋮----
// Create a second test server
⋮----
// Both servers should be running
⋮----
// Get tools from both servers
⋮----
expect(tools).toHaveLength(4); // 2 tools from each server
⋮----
// Execute tools on both servers
⋮----
// Shutdown should stop both servers
⋮----
// Server should not be running
⋮----
// No tools should be available
⋮----
// Create a server that sends invalid JSON
⋮----
// Server should fail health check
⋮----
// Should not start for disallowed project
⋮----
// Reset and try with allowed path
⋮----
// Create a server that checks env vars
⋮----
// Check custom env var
⋮----
// Check that process env is also available
</file>

<file path="src/services/mcp/__tests__/MCPService.test.ts">
import { describe, it, expect, beforeEach, afterEach, mock, spyOn } from "bun:test";
import { MCPService } from "../MCPService";
import { ChildProcess } from "child_process";
import { Client } from "@modelcontextprotocol/sdk/client/index.js";
import { StdioClientTransport } from "@modelcontextprotocol/sdk/client/stdio.js";
import type { MCPServerConfig, TenexMCP } from "@/services/config/types";
import type { Tool, PluginParameter } from "@/tools/types";
import { configService } from "@/services";
⋮----
// Mock modules
⋮----
// We don't need to mock spawn directly as StdioClientTransport handles it
⋮----
// Reset all mocks
⋮----
// Reset singleton instance
⋮----
// Create mock process
⋮----
// Simulate process exit after a short delay
⋮----
// Create mock transport with process
⋮----
process: mockProcess, // The transport has the process
⋮----
// Create mock client
⋮----
// Reset mocks first
⋮----
// Setup mocks
⋮----
// Store the options for verification
⋮----
// Clean up the service after each test
⋮----
// Reset singleton instance
⋮----
// Reset the isInitialized flag
⋮----
// Mock health check failure
⋮----
// Server should not be started if health check fails
⋮----
// Test with disallowed project path
⋮----
// Both servers should start
⋮----
// Mock health check to never resolve
⋮----
healthCheckPromise = new Promise(() => {}); // Never resolves
⋮----
// Wait for initialization to complete (which includes the health check timeout)
⋮----
// The process should not be added to clients due to health check failure
⋮----
// Kill the process to simulate server crash
⋮----
// Should still attempt to kill process
⋮----
// Mock process that doesn't exit
⋮----
// Wait a bit but don't call exit callback
⋮----
// Force the timeout
⋮----
// This should be allowed but might be a security concern
⋮----
// Should allow since /test/project starts with /test
</file>

<file path="src/services/mcp/mcpInstaller.ts">
import { configService } from "@/services/ConfigService";
import type { MCPServerConfig } from "@/services/config/types";
import { logger } from "@/utils/logger";
import type { NDKMCPTool } from "@/events/NDKMCPTool";
⋮----
/**
 * Installs an MCP server from an NDKMCPTool event into a project's configuration
 */
export async function installMCPServerFromEvent(
    projectPath: string,
    mcpTool: NDKMCPTool
): Promise<void>
⋮----
// Parse command and args
⋮----
// Build server config
⋮----
// Load existing MCP config
⋮----
// Check if server already exists
⋮----
// Add new server
⋮----
// Save config
</file>

<file path="src/services/mcp/MCPService.ts">
import { type ChildProcess } from "node:child_process";
import path from "node:path";
import { configService } from "@/services/ConfigService";
import type { MCPServerConfig, TenexMCP } from "@/services/config/types";
import type { Tool } from "@/tools/types";
import { logger } from "@/utils/logger";
import { Client } from "@modelcontextprotocol/sdk/client/index.js";
import { StdioClientTransport } from "@modelcontextprotocol/sdk/client/stdio.js";
import { z } from "zod";
import { adaptMCPTool } from "./MCPToolAdapter";
⋮----
interface MCPClient {
    client: Client;
    process?: ChildProcess;
    serverName: string;
    config: MCPServerConfig;
}
⋮----
// Define Zod schemas for MCP responses
⋮----
type MCPTool = z.infer<typeof MCPToolSchema>;
⋮----
type MCPContent = z.infer<typeof MCPContentSchema>;
⋮----
interface StdioTransportWithProcess extends StdioClientTransport {
    process?: ChildProcess;
    subprocess?: ChildProcess;
}
⋮----
export class MCPService
⋮----
private constructor()
⋮----
static getInstance(): MCPService
⋮----
async initialize(projectPath?: string): Promise<void>
⋮----
// Don't throw - allow the system to continue without MCP
⋮----
private async startServers(mcpConfig: TenexMCP): Promise<void>
⋮----
// Continue with other servers
⋮----
private async startServer(name: string, config: MCPServerConfig): Promise<void>
⋮----
// SECURITY CHECK: Enforce allowedPaths
⋮----
// Only include defined environment variables
⋮----
// Override with config env
⋮----
// Perform health check
⋮----
// Ignore close errors
⋮----
// Store the client with the transport's subprocess
⋮----
// Refresh tool cache when a new server is started
⋮----
private async refreshToolCache(): Promise<void>
⋮----
// Synchronous method to get cached tools
getCachedTools(): Tool[]
⋮----
private async fetchAvailableTools(): Promise<Tool[]>
⋮----
// Keep the async method for compatibility
async getAvailableTools(): Promise<Tool[]>
⋮----
private convertMCPToolToTenexTool(serverName: string, mcpTool: MCPTool): Tool
⋮----
// Use the adapter to create a type-safe tool with Zod schemas
⋮----
async executeTool(
        serverName: string,
        toolName: string,
        args: Record<string, unknown>
): Promise<unknown>
⋮----
// Extract text content from the response
⋮----
async shutdown(): Promise<void>
⋮----
private async shutdownServer(name: string, mcpClient: MCPClient): Promise<void>
⋮----
// Close the client connection
⋮----
// Kill the process if it exists
⋮----
// Give it some time to shut down gracefully
⋮----
// Force kill if graceful shutdown fails
⋮----
// Check if a server is running
isServerRunning(name: string): boolean
⋮----
// Get list of running servers
getRunningServers(): string[]
</file>

<file path="src/services/mcp/MCPToolAdapter.ts">
import type { Tool } from "@/tools/types";
import { createZodSchema, mcpSchemaToZod } from "@/tools/zod-schema";
import { logger } from "@/utils/logger";
import { z } from "zod";
⋮----
// Import MCPPropertyDefinition type
interface MCPPropertyDefinition {
    type?: "string" | "number" | "integer" | "boolean" | "array" | "object";
    description?: string;
    enum?: string[];
    minLength?: number;
    maxLength?: number;
    minimum?: number;
    maximum?: number;
    items?: MCPPropertyDefinition;
    properties?: Record<string, MCPPropertyDefinition>;
    required?: string[];
    minItems?: number;
    maxItems?: number;
}
⋮----
interface MCPTool {
    name: string;
    description?: string;
    inputSchema?: {
        properties?: Record<string, MCPPropertyDefinition>;
        required?: string[];
    };
}
⋮----
/**
 * Converts an MCP tool definition to our type-safe tool system using Zod
 */
export function adaptMCPTool(
    mcpTool: MCPTool,
    serverName: string,
    executeFn: (args: Record<string, unknown>) => Promise<unknown>
): Tool<Record<string, unknown>, unknown>
⋮----
// Convert MCP input schema to Zod schema
⋮----
// Add description to the schema
⋮----
// Create the parameter schema using our Zod adapter
⋮----
// Create a Tool that wraps the MCP tool
⋮----
/**
 * Type-safe MCP tool with proper inference
 */
export interface TypedMCPTool<TInput extends z.ZodType<unknown>>
    extends Tool<z.infer<TInput>, unknown> {
    readonly inputSchema: TInput;
}
⋮----
/**
 * Create a strongly-typed MCP tool
 */
export function createTypedMCPTool<TInput extends z.ZodType<unknown>>(config: {
    name: string;
    serverName: string;
    description?: string;
    inputSchema: TInput;
execute: (args: z.infer<TInput>)
</file>

<file path="src/services/ConfigService.ts">
import os from "node:os";
import path from "node:path";
import { ensureDirectory, fileExists, readJsonFile, writeJsonFile } from "@/lib/fs";
import type {
    ConfigFile,
    LoadedConfig,
    TenexAgents,
    TenexConfig,
    TenexLLMs,
    TenexMCP,
} from "@/services/config/types";
import {
    TenexAgentsSchema,
    TenexConfigSchema,
    TenexLLMsSchema,
    TenexMCPSchema,
} from "@/services/config/types";
import { logger } from "@/utils/logger";
import type { z } from "zod";
⋮----
/**
 * Centralized configuration service for TENEX
 * Handles loading and saving of all configuration files
 * Pure file operations with validation - no business logic
 */
export class ConfigService
⋮----
private readonly cacheTTL = 5000; // 5 seconds
⋮----
private constructor()
⋮----
static getInstance(): ConfigService
⋮----
// =====================================================================================
// PATH UTILITIES
// =====================================================================================
⋮----
getGlobalPath(): string
⋮----
getProjectPath(projectPath: string): string
⋮----
private getConfigFilePath(basePath: string, configFile: ConfigFile): string
⋮----
// =====================================================================================
// COMPLETE CONFIGURATION LOADING
// =====================================================================================
⋮----
async loadConfig(projectPath?: string): Promise<LoadedConfig>
⋮----
// Load global config
⋮----
// Load project config if provided
⋮----
// Merge configs (project overrides global)
⋮----
// Merge arrays properly
⋮----
// Load agents (merge global and project)
⋮----
// Load LLMs (merge global and project)
⋮----
// Load MCP (merge global and project)
⋮----
// =====================================================================================
// INDIVIDUAL FILE LOADING
// =====================================================================================
⋮----
async loadTenexConfig(basePath: string): Promise<TenexConfig>
⋮----
async loadTenexAgents(basePath: string): Promise<TenexAgents>
⋮----
async loadTenexLLMs(basePath: string): Promise<TenexLLMs>
⋮----
async loadTenexMCP(basePath: string): Promise<TenexMCP>
⋮----
// Ensure servers is always defined
⋮----
// =====================================================================================
// INDIVIDUAL FILE SAVING
// =====================================================================================
⋮----
async saveTenexConfig(basePath: string, config: TenexConfig): Promise<void>
⋮----
async saveTenexAgents(basePath: string, agents: TenexAgents): Promise<void>
⋮----
async saveTenexLLMs(basePath: string, llms: TenexLLMs): Promise<void>
⋮----
async saveTenexMCP(basePath: string, mcp: TenexMCP): Promise<void>
⋮----
// =====================================================================================
// BUSINESS LOGIC METHODS
// =====================================================================================
⋮----
/**
     * Get whitelisted pubkeys with CLI override support
     * If CLI option is provided, it ONLY uses those pubkeys (doesn't merge with config)
     * Otherwise, returns pubkeys from the configuration
     */
getWhitelistedPubkeys(cliOption?: string, config?: TenexConfig): string[]
⋮----
// If CLI option is provided, ONLY use those pubkeys (don't merge with config)
⋮----
// Otherwise, use config pubkeys
⋮----
// =====================================================================================
// CONVENIENCE METHODS
// =====================================================================================
⋮----
async saveGlobalConfig(config: TenexConfig): Promise<void>
⋮----
async saveProjectConfig(projectPath: string, config: TenexConfig): Promise<void>
⋮----
async saveGlobalAgents(agents: TenexAgents): Promise<void>
⋮----
async loadProjectAgents(projectPath: string): Promise<TenexAgents>
⋮----
async saveProjectAgents(projectPath: string, agents: TenexAgents): Promise<void>
⋮----
async saveGlobalLLMs(llms: TenexLLMs): Promise<void>
⋮----
async saveProjectLLMs(projectPath: string, llms: TenexLLMs): Promise<void>
⋮----
async saveGlobalMCP(mcp: TenexMCP): Promise<void>
⋮----
async saveProjectMCP(projectPath: string, mcp: TenexMCP): Promise<void>
⋮----
// =====================================================================================
// FILE EXISTENCE CHECKS
// =====================================================================================
⋮----
async configExists(basePath: string, configFile: ConfigFile): Promise<boolean>
⋮----
async globalConfigExists(configFile: ConfigFile): Promise<boolean>
⋮----
async projectConfigExists(projectPath: string, configFile: ConfigFile): Promise<boolean>
⋮----
// =====================================================================================
// PRIVATE IMPLEMENTATION
// =====================================================================================
⋮----
private async loadConfigFile<T>(
        filePath: string,
        schema: z.ZodSchema<T>,
        defaultValue: T
): Promise<T>
⋮----
// Check cache first
⋮----
private async saveConfigFile<T>(
        filePath: string,
        data: T,
        schema: z.ZodSchema<T>
): Promise<void>
⋮----
// Ensure directory exists
⋮----
// Validate before saving
⋮----
// Save to file
⋮----
// Update cache
⋮----
private getFromCache<T>(filePath: string): T | null
⋮----
private addToCache<T>(filePath: string, data: T): void
⋮----
clearCache(filePath?: string): void
⋮----
// Export singleton instance
</file>

<file path="src/services/index.ts">
/**
 * Centralized services for TENEX
 */
</file>

<file path="src/services/ProjectContext.ts">
import type { Agent } from "@/agents/types";
import type { NDKAgentLesson } from "@/events/NDKAgentLesson";
import { logger } from "@/utils/logger";
import type { Hexpubkey, NDKProject } from "@nostr-dev-kit/ndk";
import type { NDKPrivateKeySigner } from "@nostr-dev-kit/ndk";
⋮----
/**
 * ProjectContext provides system-wide access to loaded project and agents
 * Initialized during "tenex project run" by ProjectManager
 */
export class ProjectContext
⋮----
/**
     * Event that represents this project, note that this is SIGNED
     * by the USER, so this.project.pubkey is NOT the project's pubkey but the
     * USER OWNER'S pubkey.
     *
     * - projectCtx.pubkey = The project agent's pubkey (the bot/system)
     * - projectCtx.project.pubkey = The user's pubkey (who created the project)
     */
⋮----
/**
     * Signer the project agent uses (hardwired to orchestrator agent's signer)
     */
⋮----
/**
     * Pubkey of the project agent
     */
⋮----
/**
     * The orchestrator agent for this project
     */
⋮----
/**
     * Lessons learned by agents in this project
     * Key: agent pubkey, Value: array of lessons (limited to most recent 50 per agent)
     */
⋮----
constructor(project: NDKProject, agents: Map<string, Agent>)
⋮----
// Debug logging
⋮----
// Find the orchestrator agent dynamically
⋮----
// Hardwire to orchestrator agent's signer and pubkey
⋮----
// =====================================================================================
// AGENT ACCESS HELPERS
// =====================================================================================
⋮----
getAgent(slug: string): Agent | undefined
⋮----
getAgentByPubkey(pubkey: Hexpubkey): Agent | undefined
⋮----
// Find the agent dynamically
⋮----
getProjectAgent(): Agent
⋮----
getAgentSlugs(): string[]
⋮----
hasAgent(slug: string): boolean
⋮----
// =====================================================================================
// LESSON MANAGEMENT
// =====================================================================================
⋮----
/**
     * Add a lesson for an agent, maintaining the 50-lesson limit per agent
     */
addLesson(agentPubkey: string, lesson: NDKAgentLesson): void
⋮----
// Add the new lesson at the beginning (most recent first)
⋮----
// Keep only the most recent 50 lessons
⋮----
/**
     * Get lessons for a specific agent
     */
getLessonsForAgent(agentPubkey: string): NDKAgentLesson[]
⋮----
/**
     * Get all lessons across all agents
     */
getAllLessons(): NDKAgentLesson[]
⋮----
/**
     * Safely update project data without creating a new instance.
     * This ensures all parts of the system work with consistent state.
     */
updateProjectData(newProject: NDKProject, newAgents: Map<string, Agent>): void
⋮----
// Update orchestrator reference if it exists in new agents
⋮----
// Module-level variable for global access
⋮----
/**
 * Initialize the project context. Should be called once during project startup.
 */
export function setProjectContext(project: NDKProject, agents: Map<string, Agent>): void
⋮----
/**
 * Get the initialized project context
 * @throws Error if not initialized
 */
export function getProjectContext(): ProjectContext
⋮----
/**
 * Check if project context is initialized
 */
export function isProjectContextInitialized(): boolean
</file>

<file path="src/test-utils/mock-llm/scenarios/concurrency-workflow.ts">
import type { LLMToolCall, MockScenario } from "../types";
⋮----
// Routing decisions for orchestrator
⋮----
// Orchestrator Phase 1: Initial task understanding for User A
⋮----
// Orchestrator Phase 1: Initial task understanding for User B
⋮----
// Orchestrator Phase 2: Planning for User A
⋮----
// Orchestrator Phase 2: Planning for User B
⋮----
// Orchestrator Phase 3: Implementation handoff for User A
⋮----
// Orchestrator Phase 3: Implementation handoff for User B
⋮----
// Executor implementation for User A
⋮----
// Executor implementation for User B
⋮----
// Orchestrator Phase 4: Verification for User A
⋮----
// Orchestrator Phase 4: Verification for User B
⋮----
// Orchestrator Phase 5: Completion for User A
⋮----
// Orchestrator Phase 5: Completion for User B
⋮----
// Orchestrator Phase 1: Initial task understanding for User C
⋮----
// Orchestrator Phase 2: Planning for User C
⋮----
// Orchestrator Phase 3: Implementation handoff for User C
⋮----
// Executor implementation for User C
⋮----
// Orchestrator Phase 4: Verification for User C
⋮----
// Orchestrator Phase 5: Completion for User C
</file>

<file path="src/test-utils/mock-llm/scenarios/error-handling.ts">
import type { MockLLMScenario } from "../types";
⋮----
/**
 * Error handling scenarios for testing edge cases and failures
 */
⋮----
// Tool execution failure
⋮----
// Recovery after tool failure
⋮----
// Network timeout simulation
⋮----
streamDelay: 5000, // 5 second delay
⋮----
// Invalid tool arguments
⋮----
arguments: "{ invalid json }" // Intentionally malformed
⋮----
// LLM service error
⋮----
// Empty response handling
⋮----
content: "", // Empty content
toolCalls: [] // No tool calls
⋮----
// Multiple tool calls with mixed success
⋮----
// Infinite loop prevention
⋮----
// Phase transition failure
⋮----
suggestedPhase: "INVALID_PHASE", // Invalid phase
⋮----
// Concurrent execution conflict
⋮----
// Memory/context overflow simulation
⋮----
content: "A".repeat(10000), // Very large response
</file>

<file path="src/test-utils/mock-llm/scenarios/index.ts">
import { orchestratorWorkflowScenario } from "./orchestrator-workflow";
import { errorHandlingScenario } from "./error-handling";
import { statePersistenceScenario } from "./state-persistence";
import { routingDecisions } from "./routing-decisions";
import { performanceTestingScenario } from "./performance-testing";
import { concurrencyWorkflowScenarios } from "./concurrency-workflow";
import { inventoryGenerationScenario } from "./inventory-generation";
import { networkResilienceScenario } from "./network-resilience";
import type { MockLLMScenario, MockScenario } from "../types";
⋮----
/**
 * All available mock scenarios for testing
 */
⋮----
/**
 * Concurrency testing scenario
 */
⋮----
// Add concurrency scenario to all scenarios
⋮----
/**
 * Get a specific scenario by name
 */
export function getScenario(name: string): MockLLMScenario | undefined
⋮----
/**
 * Create a custom scenario for specific test cases
 */
export function createScenario(
    name: string,
    description: string,
    responses: MockLLMScenario['responses']
): MockLLMScenario
</file>

<file path="src/test-utils/mock-llm/scenarios/inventory-generation.ts">
import type { MockLLMScenario } from "../types";
⋮----
/**
 * Inventory generation workflow scenario
 * Tests the complete flow of generating project inventory including:
 * - Initial request in CHAT phase
 * - Transitioning to EXECUTE phase
 * - Using generate_inventory tool
 * - Handling task progress updates
 * - Completing the workflow
 */
⋮----
// Initial chat phase - user asks to generate inventory
⋮----
// EXECUTE phase - Executor generates inventory
⋮----
// Handle inventory generation completion
⋮----
// Orchestrator handles completion
⋮----
// VERIFICATION phase - confirm inventory was created
⋮----
// Verification complete
⋮----
// Alternative flow: Inventory already exists
⋮----
// Error handling: Inventory generation fails
</file>

<file path="src/test-utils/mock-llm/scenarios/network-resilience.ts">
import type { MockLLMScenario } from "../types";
⋮----
/**
 * Scenario for testing Nostr network resilience
 * Provides simple, deterministic responses for network failure testing
 */
⋮----
// Orchestrator responses for initial requests
⋮----
// Planner responses
⋮----
// Executor responses
⋮----
// Verification phase
⋮----
// Generic fallback for any agent
</file>

<file path="src/test-utils/mock-llm/scenarios/orchestrator-workflow.ts">
import type { MockLLMScenario } from "../types";
⋮----
/**
 * Complete orchestrator workflow scenario covering all phases:
 * CHAT -> PLAN -> EXECUTE -> VERIFICATION
 */
⋮----
// Initial chat phase - user asks to create a feature
⋮----
// Chat phase - gathering requirements
⋮----
// Orchestrator transitions to PLAN phase
⋮----
// PLAN phase - Planner creates implementation plan
⋮----
// Planner completes planning
⋮----
// Orchestrator transitions to EXECUTE phase
⋮----
// EXECUTE phase - Executor implements the feature
⋮----
// Executor creates multiple files
⋮----
// Executor runs tests
⋮----
// Executor completes implementation
⋮----
// Orchestrator transitions to VERIFICATION
⋮----
// VERIFICATION phase - final checks
⋮----
// Final orchestrator completion
</file>

<file path="src/test-utils/mock-llm/scenarios/performance-testing.ts">
import type { MockLLMScenario } from "../types";
⋮----
/**
 * Performance testing scenarios that simulate slow responses, timeouts, and system stress
 */
⋮----
// Scenario: Slow LLM response during orchestration
⋮----
streamDelay: 5000, // 5 second delay
⋮----
// Scenario: Very slow planning phase
⋮----
streamDelay: 8000, // 8 second delay
⋮----
// Scenario: Timeout simulation (exceeds typical timeout)
⋮----
streamDelay: 35000, // 35 second delay (should trigger timeout)
⋮----
// Scenario: Slow tool execution
⋮----
streamDelay: 3000, // 3 second delay for tool response
⋮----
// Scenario: Rapid sequential requests (stress test)
⋮----
streamDelay: 50, // Very short delay
⋮----
// Scenario: Memory-intensive response
⋮----
// Generate a large response to test memory handling
content: "Large response data: " + "x".repeat(50000), // 50KB of data
⋮----
// Scenario: Recovery after timeout
⋮----
streamDelay: 100, // Quick response after timeout
⋮----
// Performance test initial response
</file>

<file path="src/test-utils/mock-llm/scenarios/routing-decisions.ts">
import type { MockLLMScenario } from "../types";
⋮----
/**
 * Mock responses for orchestrator routing decisions
 */
⋮----
// Error recovery routing
⋮----
// Infinite loop routing
⋮----
// Timeout test routing
⋮----
// Multi-agent error routing
⋮----
// Default routing response for any orchestrator message with JSON instruction
</file>

<file path="src/test-utils/mock-llm/scenarios/state-persistence.ts">
import type { MockLLMScenario } from "../types";
⋮----
/**
 * Scenario for testing conversation state persistence and recovery
 * Simulates partial workflow execution and recovery after restart
 */
⋮----
// Initial orchestrator response - transition to PLAN phase
⋮----
// Plan phase response - transition to BUILD
⋮----
// Test agent BUILD phase response
⋮----
// Analyze project structure scenario
⋮----
// Recovery scenario - continue analysis
⋮----
// Concurrent task scenarios
</file>

<file path="src/test-utils/mock-llm/example-e2e.test.ts">
import { describe, it, expect, beforeEach, mock } from "bun:test";
import { createMockLLMService } from "./index";
import type { LLMService } from "@/llm/types";
⋮----
// Example of how to use the mock LLM service in E2E tests
⋮----
// Create mock with orchestrator workflow scenario
⋮----
debug: true // Enable debug logging
⋮----
// Mock the LLM router to return our mock service
⋮----
// This would be your actual E2E test code
// For example, starting the daemon and sending events
⋮----
// Simulate user message
⋮----
// The mock will automatically respond based on the scenario
⋮----
// Verify orchestrator response
⋮----
// Get request history for debugging
⋮----
// Add error scenario
⋮----
// Create custom scenario
⋮----
// Simulate multiple continues
</file>

<file path="src/test-utils/mock-llm/index.ts">
import { MockLLMService } from "./MockLLMService";
import type { MockLLMConfig, MockLLMScenario } from "./types";
import { allScenarios } from "./scenarios";
⋮----
/**
 * Create a mock LLM service with predefined scenarios
 */
export function createMockLLMService(
    scenarios?: string[] | MockLLMScenario[],
    config?: Partial<MockLLMConfig>
): MockLLMService
⋮----
// Load by name
⋮----
// Direct scenario object
⋮----
/**
 * Create a simple mock that always returns the same response
 */
export function createSimpleMock(
    content: string,
    toolCalls?: any[]
): MockLLMService
⋮----
/**
 * Create a mock that simulates errors
 */
export function createErrorMock(error: Error): MockLLMService
</file>

<file path="src/test-utils/mock-llm/MockLLMService.ts">
import type { 
    LLMService, 
    Message,
    CompletionRequest,
    CompletionResponse,
    StreamEvent,
    ToolCall
} from "@/llm/types";
import type { MockLLMConfig, MockLLMResponse } from "./types";
import { logger } from "@/utils/logger";
⋮----
export class MockLLMService implements LLMService
⋮----
constructor(config: MockLLMConfig =
⋮----
// Load responses from scenarios
⋮----
// Sort by priority
⋮----
async complete(request: CompletionRequest): Promise<CompletionResponse>
⋮----
// Simulate delay if specified
⋮----
// Convert to CompletionResponse format that matches multi-llm-ts v4
⋮----
async *stream(request: CompletionRequest): AsyncIterable<StreamEvent>
⋮----
// Simulate streaming content
⋮----
// Send tool calls
⋮----
// Send completion event
⋮----
private findMatchingResponse(messages: Message[], model: string): MockLLMResponse['response']
⋮----
// Extract agent name and phase from system prompt
⋮----
// Find matching response
⋮----
// Check all trigger conditions
⋮----
// All conditions matched
⋮----
// Return default response
⋮----
private extractAgentName(systemPrompt: string): string
⋮----
private extractPhase(systemPrompt: string): string
⋮----
private recordRequest(
        messages: Message[], 
        model: string, 
        response: MockLLMResponse['response']
): void
⋮----
// Helper methods for testing
⋮----
addResponse(response: MockLLMResponse): void
⋮----
getRequestHistory()
⋮----
clearHistory(): void
</file>

<file path="src/test-utils/mock-llm/performance.test.ts">
import { describe, it, expect, beforeEach } from "bun:test";
import { MockLLMService } from "./MockLLMService";
import type { MockLLMScenario } from "./types";
⋮----
/**
 * Unit tests for MockLLMService performance features
 * Tests that streamDelay works correctly for simulating slow responses
 */
⋮----
// Create a simple scenario with delays
⋮----
streamDelay: 1000, // 1 second delay
⋮----
streamDelay: 3000, // 3 second delay
⋮----
streamDelay: 0, // No delay
⋮----
// Should take at least 1000ms
⋮----
// Should take at least 3000ms
⋮----
// Should be nearly instant (less than 100ms)
⋮----
// Stream the response
⋮----
// Streaming should also respect the delay
⋮----
// Make a delayed request
⋮----
// Start three requests concurrently
⋮----
// All should complete, with the slowest determining total time
</file>

<file path="src/test-utils/mock-llm/types.ts">
import type { Message, ToolCall } from "@/llm/types";
⋮----
export interface MockLLMResponse {
    /** The messages that should trigger this response */
    trigger: {
        /** Match system prompt content */
        systemPrompt?: string | RegExp;
        /** Match user message content */
        userMessage?: string | RegExp;
        /** Match specific tool calls in the conversation */
        previousToolCalls?: string[];
        /** Match agent name */
        agentName?: string;
        /** Match conversation phase */
        phase?: string;
    };
    /** The response to return when triggered */
    response: {
        /** Text content of the response */
        content?: string;
        /** Tool calls to make */
        toolCalls?: ToolCall[];
        /** Simulate streaming delay in ms */
        streamDelay?: number;
        /** Simulate an error */
        error?: Error;
    };
    /** Priority for matching (higher = checked first) */
    priority?: number;
}
⋮----
/** The messages that should trigger this response */
⋮----
/** Match system prompt content */
⋮----
/** Match user message content */
⋮----
/** Match specific tool calls in the conversation */
⋮----
/** Match agent name */
⋮----
/** Match conversation phase */
⋮----
/** The response to return when triggered */
⋮----
/** Text content of the response */
⋮----
/** Tool calls to make */
⋮----
/** Simulate streaming delay in ms */
⋮----
/** Simulate an error */
⋮----
/** Priority for matching (higher = checked first) */
⋮----
export interface MockLLMScenario {
    name: string;
    description: string;
    responses: MockLLMResponse[];
}
⋮----
export interface MockLLMConfig {
    /** Default response if no triggers match */
    defaultResponse?: MockLLMResponse['response'];
    /** Log all requests for debugging */
    debug?: boolean;
    /** Scenarios to load */
    scenarios?: MockLLMScenario[];
}
⋮----
/** Default response if no triggers match */
⋮----
/** Log all requests for debugging */
⋮----
/** Scenarios to load */
</file>

<file path="src/test-utils/index.ts">
/**
 * Test utilities for TENEX backend
 * 
 * This module provides comprehensive testing utilities including:
 * - Mock LLM service for deterministic E2E testing
 * - Mock factories for common objects
 * - Test environment helpers
 * - Assertion utilities
 */
⋮----
import { mock } from "bun:test";
⋮----
import { tmpdir } from "node:os";
⋮----
/**
 * Create a temporary directory for testing
 */
export async function createTempDir(prefix = "tenex-test-"): Promise<string>
⋮----
/**
 * Clean up a temporary directory
 */
export async function cleanupTempDir(dirPath: string): Promise<void>
⋮----
// Ignore errors during cleanup
⋮----
/**
 * Reset all mocks and singletons
 */
export function resetAllMocks(): void
⋮----
// Reset singletons
⋮----
// Module might not be loaded
⋮----
/**
 * Wait for a condition to be true
 */
export async function waitFor(
    condition: () => boolean | Promise<boolean>,
    timeout = 5000,
    interval = 100
): Promise<void>
⋮----
/**
 * Mock file system operations
 */
export function mockFileSystem(files: Map<string, string>)
⋮----
/**
 * Capture console output during tests
 */
export class ConsoleCapture
⋮----
start(): void
⋮----
stop(): void
⋮----
getLogs(): string[]
⋮----
getErrors(): string[]
⋮----
clear(): void
⋮----
/**
 * Custom assertions
 */
⋮----
/**
     * Assert that an async function throws an error
     */
async toThrowAsync(
        fn: () => Promise<any>,
        expectedError?: string | RegExp | Error
): Promise<void>
⋮----
/**
     * Assert that an array contains an object matching partial properties
     */
toContainObjectMatching<T>(
        array: T[],
        partial: Partial<T>
): void
</file>

<file path="src/test-utils/mock-factories.ts">
import type { NDKEvent } from "@nostr-dev-kit/ndk";
import type { Agent, ExecutionContext } from "@/agents/types";
import type { Conversation, Phase } from "@/conversations/types";
import type { ToolCall } from "@/llm/types";
import { EVENT_KINDS } from "@/llm/types";
⋮----
/**
 * Factory functions for creating mock objects in tests
 */
⋮----
/**
 * MockNostrEvent class that implements the serialize method required by FileSystemAdapter
 */
export class MockNostrEvent implements NDKEvent
⋮----
constructor(overrides?: Partial<NDKEvent>)
⋮----
serialize(includeSignature?: boolean, includeId?: boolean): string
⋮----
static deserialize(ndk: any, serialized: string): MockNostrEvent
⋮----
export function createMockNDKEvent(overrides?: Partial<NDKEvent>): NDKEvent
⋮----
export function createMockAgent(overrides?: Partial<Agent>): Agent
⋮----
export function createMockConversation(overrides?: Partial<Conversation>): Conversation
⋮----
export function createMockExecutionContext(overrides?: Partial<ExecutionContext>): ExecutionContext
⋮----
export function createMockToolCall(overrides?: Partial<ToolCall>): ToolCall
⋮----
export function createMockPhaseTransition(
    from: Phase,
    to: Phase,
    reason?: string
)
⋮----
/**
 * Create a mock file system structure for testing
 */
export function createMockFileSystem(): Map<string, string>
⋮----
// Add common project files
⋮----
/**
 * Create a builder for complex mock objects
 */
export class MockBuilder<T>
⋮----
with<K extends keyof T>(key: K, value: T[K]): this
⋮----
build(defaults: T): T
⋮----
// Usage example:
// const agent = new MockBuilder<Agent>()
//     .with('name', 'TestAgent')
//     .with('allowedTools', ['test-tool'])
//     .build(createMockAgent());
</file>

<file path="src/test-utils/test-persistence-adapter.ts">
import type { Conversation } from "@/conversations/types";
import type { 
    ConversationPersistenceAdapter, 
    ConversationMetadata,
    ConversationSearchCriteria 
} from "@/conversations/persistence/types";
⋮----
/**
 * In-memory persistence adapter for testing
 * Avoids file system operations and serialization issues in tests
 */
export class TestPersistenceAdapter implements ConversationPersistenceAdapter
⋮----
async initialize(): Promise<void>
⋮----
// No initialization needed for in-memory storage
⋮----
async save(conversation: Conversation): Promise<void>
⋮----
// Store conversation directly without serialization
⋮----
// Deep clone to prevent mutations
⋮----
async load(id: string): Promise<Conversation | null>
⋮----
// Return a deep clone to prevent mutations
⋮----
async delete(id: string): Promise<void>
⋮----
async list(): Promise<ConversationMetadata[]>
⋮----
async search(criteria: ConversationSearchCriteria): Promise<ConversationMetadata[]>
⋮----
// Check archived status
⋮----
// Check other criteria
⋮----
async archive(conversationId: string): Promise<void>
⋮----
async restore(conversationId: string): Promise<void>
⋮----
private createMetadata(conv: Conversation): ConversationMetadata
⋮----
// Test-specific methods
clear(): void
⋮----
getAll(): Map<string, Conversation>
</file>

<file path="src/tools/__tests__/zod-schema.test.ts">
import { describe, it, expect } from "bun:test";
import { z } from "zod";
import { createZodSchema, mcpSchemaToZod, ToolSchemas } from "../zod-schema";
import { adaptMCPTool } from "@/services/mcp/MCPToolAdapter";
import { analyze } from "../implementations/analyze";
import type { ExecutionContext } from "../types";
⋮----
// Test valid input
⋮----
// Test invalid input
⋮----
// Should accept without optional field
⋮----
// Should accept with optional field
⋮----
// Should fail without required field
⋮----
const mockExecute = async (args: any) => (
⋮----
// Test parameter validation
⋮----
// Valid input
⋮----
// Invalid input - empty string
⋮----
// Invalid input - missing prompt
⋮----
const validPubkey = "a".repeat(64); // 64 hex chars
</file>

<file path="src/tools/implementations/__tests__/analyze.test.ts">
import { describe, it, expect, beforeEach, mock } from "bun:test";
import { analyze } from "../analyze";
import type { ExecutionContext } from "@/tools/types";
import { logger } from "@/utils/logger";
import { generateRepomixOutput } from "@/utils/repomix";
import { loadLLMRouter } from "@/llm";
import { Message } from "multi-llm-ts";
⋮----
// Mock dependencies
⋮----
// Mock repomix output
⋮----
// Mock LLM router
⋮----
// Mock NostrPublisher
⋮----
// Execute the tool
⋮----
// Verify typing indicator was published
⋮----
// Verify the result
⋮----
// Mock repomix output
⋮----
// Mock LLM router
⋮----
// Mock NostrPublisher to throw error
⋮----
// Execute the tool - should not throw despite publisher error
⋮----
// Verify result is still returned
</file>

<file path="src/tools/implementations/__tests__/complete.test.ts">
import { describe, it, expect } from "bun:test";
import { completeTool } from "../complete";
import { z } from "zod";
⋮----
response: 123, // number instead of string
summary: true, // boolean instead of string
⋮----
// Access the schema shape to verify descriptions
⋮----
// Verify required fields
</file>

<file path="src/tools/implementations/__tests__/continue.test.ts">
import { describe, expect, it, mock } from "bun:test";
import type { Agent } from "@/agents/types";
import type { Conversation } from "@/conversations/types";
import { continueTool } from "../continue";
import type { ExecutionContext } from "@/tools/types";
import type { ToolExecutionResult } from "@/tools/executor";
import { createToolExecutor } from "@/tools/executor";
⋮----
// Mock dependencies
⋮----
// Base context
⋮----
// Execution context
⋮----
// Required fields
⋮----
// Helper function to execute tool and get result
async function executeControl(input: any): Promise<ToolExecutionResult>
</file>

<file path="src/tools/implementations/__tests__/createMilestoneTask.test.ts">
import { describe, it, expect, beforeEach, mock } from "bun:test";
import { createMilestoneTaskTool } from "../createMilestoneTask";
import type { ToolContext } from "@/tools/types";
⋮----
// Mock logger to avoid console output during tests
⋮----
// Mock NDKTask
⋮----
// Mock NDK
⋮----
// Mock project context
⋮----
// Simulate what the tag method does
⋮----
// Store reference for test assertions
⋮----
get()
set(value: string | undefined)
⋮----
set(value: string[][])
⋮----
// Reset mocks
⋮----
// Create tool context
⋮----
// Verify NDKTask was created correctly
⋮----
// Verify tags were added
⋮----
// Verify task was signed and published
⋮----
// Verify assignee tags were added
⋮----
// Verify only valid assignees got p tags
⋮----
// Mock getNDK to return null
⋮----
// Mock sign to throw an error
⋮----
// Verify no conversation reference tag was added
</file>

<file path="src/tools/implementations/__tests__/endConversation.test.ts">
import { describe, it, expect, beforeEach, mock, spyOn } from "bun:test";
import { endConversationTool } from "../endConversation";
import type { ToolContext } from "../../types";
import { logger } from "@/utils/logger";
⋮----
// Spy on logger methods
⋮----
// Clear all spies
⋮----
// Create mock publisher
⋮----
// Create base context
⋮----
// Empty string should still be valid
⋮----
// Modify context to be non-orchestrator
⋮----
// Make publisher throw an error
⋮----
// Check initial log
⋮----
// Check completion log
⋮----
const longResponse = "A".repeat(10000); // 10k character response
</file>

<file path="src/tools/implementations/__tests__/generateInventory.test.ts">
import { describe, it, expect, beforeEach, afterEach, mock } from "bun:test";
import { generateInventoryTool } from "../generateInventory";
import { createTempDir, cleanupTempDir } from "@/test-utils";
import type { ToolContext } from "@/tools/types";
import type { Agent } from "@/agents/types";
⋮----
// Mock logger to avoid console output during tests
⋮----
// Mock Nostr-related modules
⋮----
// Mock inventory utilities
⋮----
// Mock child_process exec function
⋮----
// Since promisify is called at module load time, we need to ensure
// our mock is in place before the module loads
⋮----
// Return a promisified version of our mock
⋮----
// Reset all mocks
⋮----
// Reset mock implementations
⋮----
// Reset exec mock to return standard output
⋮----
// Verify inventory generation was called
⋮----
// Check if called with correct arguments
⋮----
// Should still call generateInventory but without focus files
⋮----
// Should call generateInventory without focus files
⋮----
// Should call generateInventory without agent
⋮----
// Ensure mockGenerateInventory resolves successfully for this test
⋮----
// Test various git status formats
⋮----
// Verify focus files are parsed correctly
⋮----
// Ensure mockGenerateInventory resolves successfully
⋮----
// Verify task was NOT created (because getNDK returns a truthy value but not the actual NDK instance)
// The actual implementation checks for NDK instance which our mock doesn't provide
// This is expected behavior based on the mock setup
⋮----
// The tool accepts z.any() schema, so any input should work
⋮----
// The parameters field is a ParameterSchema object with validate method
</file>

<file path="src/tools/implementations/__tests__/learn.test.ts">
import { describe, it, expect, beforeEach, mock } from "bun:test";
import { learnTool } from "../learn";
import type { ExecutionContext } from "../../types";
import type { Agent } from "@/agents/types";
import type { Conversation } from "@/conversations/types";
import { NDKAgentLesson } from "@/events/NDKAgentLesson";
import type NDK from "@nostr-dev-kit/ndk";
import type { NDKSigner } from "@nostr-dev-kit/ndk";
⋮----
// Mock dependencies
⋮----
import { logger } from "@/utils/logger";
import { getNDK } from "@/nostr";
import { getProjectContext } from "@/services/ProjectContext";
import { getTotalExecutionTimeSeconds } from "@/conversations/executionTime";
⋮----
// Reset all mocks
⋮----
// Setup mock agent
⋮----
// Setup mock conversation
⋮----
// Setup mock context
⋮----
// Setup NDK mock
⋮----
// Setup project context mock
⋮----
// Setup NDKAgentLesson mock
</file>

<file path="src/tools/implementations/__tests__/readPath.test.ts">
import { describe, it, expect, beforeEach, afterEach, mock } from "bun:test";
import { readPathTool } from "../readPath";
import { createTempDir, cleanupTempDir } from "@/test-utils";
import { writeFileSync, mkdirSync } from "node:fs";
⋮----
import type { Tool, ToolContext } from "@/tools/types";
⋮----
// Mock conversation manager
⋮----
// Reset mocks before each test
⋮----
// Reset mock implementation
⋮----
// Create test context
⋮----
// Create test files and directories
⋮----
// Mock conversation with existing tracked file
⋮----
// Make file unreadable (this might not work on all systems)
⋮----
// Skip test if chmod doesn't work
⋮----
// Restore permissions
⋮----
// Should return directory listing instead of error
⋮----
// Create circular symlink (platform dependent)
⋮----
// Skip if symlinks not supported
⋮----
const largeContent = "x".repeat(1024 * 1024); // 1MB
⋮----
// Context without conversation manager
</file>

<file path="src/tools/implementations/__tests__/shell-simple.test.ts">
import { describe, it, expect } from "bun:test";
import { shellTool } from "../shell";
import { createMockExecutionContext } from "@/test-utils";
⋮----
// Missing required command
</file>

<file path="src/tools/implementations/__tests__/shell.test.ts">
import { describe, it, expect, beforeEach, mock } from "bun:test";
import { shellTool } from "../shell";
import type { ExecutionContext } from "@/tools/types";
import { createMockExecutionContext } from "@/test-utils";
⋮----
// Mock child_process
⋮----
// Simulate command output
⋮----
// Simulate process exit
⋮----
// Mock a long-running command
⋮----
// Don't call close handler - simulate hanging process
⋮----
timeout: 100 // 100ms timeout
⋮----
// Wait for timeout
⋮----
// Verify process was killed
⋮----
// Test with command that would produce special characters
⋮----
// Output should be clean
</file>

<file path="src/tools/implementations/__tests__/writeContextFile.test.ts">
import { describe, it, expect, beforeEach, afterEach, mock, spyOn } from "bun:test";
import { writeContextFileTool } from "../writeContextFile";
import { createTempDir, cleanupTempDir } from "@/test-utils";
import { writeFileSync, readFileSync, existsSync, mkdirSync, chmodSync } from "node:fs";
⋮----
import type { ToolContext } from "@/tools/types";
⋮----
// Mock logger to avoid console output during tests
⋮----
// Mock Nostr-related modules
⋮----
// Mock NDKArticle
⋮----
// Store a reference to this instance in mockNDKArticle
⋮----
// Override property definitions to capture values
⋮----
get()
set(value: string | undefined)
⋮----
set(value: number | undefined)
⋮----
// Reset mocks
⋮----
// Reset publish and sign implementations
⋮----
// Create mock conversation manager
⋮----
// Create test context
⋮----
// Verify file was written to correct location
⋮----
// Create context directory and existing file
⋮----
// Update mock to include file in readFiles
⋮----
// Verify file was updated
⋮----
// Create context directory and existing file
⋮----
// Mock has empty readFiles by default
⋮----
// Verify file was not modified
⋮----
// Verify file was created
⋮----
// Create existing file
⋮----
// Mock conversation with no metadata
⋮----
// Verify directory was created
⋮----
// Create a file with the same name as the directory we want to create
⋮----
// Create context directory
⋮----
// Make directory read-only (no write permission)
⋮----
// Restore permissions
⋮----
// Verify NDKArticle properties were set
⋮----
// Verify article methods were called
⋮----
// Make publish throw an error
⋮----
// Tool should still succeed
⋮----
// File should have been written
⋮----
// Make sign throw an error
⋮----
// Tool should still succeed (error is logged but not thrown)
⋮----
// File should have been written
⋮----
// Create existing file
⋮----
// Mock with empty readFiles (default)
⋮----
// Should still work for new files
⋮----
// Verify file was created
⋮----
// Verify file was created with the long name
</file>

<file path="src/tools/implementations/analyze.ts">
import { loadLLMRouter } from "@/llm";
import { logger } from "@/utils/logger";
import { generateRepomixOutput } from "@/utils/repomix";
import { Message } from "multi-llm-ts";
import { z } from "zod";
import type { Tool } from "../types";
import { createZodSchema } from "../types";
⋮----
interface AnalyzeInput {
    prompt: string;
    targetDirectory?: string;
}
⋮----
interface AnalyzeOutput {
    analysis: string;
    repoSize: number;
}
⋮----
// Publish custom typing indicator
⋮----
// Prepare the prompt for the LLM
⋮----
// Call the LLM with the analyze-specific configuration
⋮----
// Stop typing indicator
⋮----
// Stop typing indicator on error
</file>

<file path="src/tools/implementations/complete.ts">
import type { Tool, Termination } from "../types";
import { success, createZodSchema } from "../types";
import { z } from "zod";
import { handleAgentCompletion } from "@/agents/execution/completionHandler";
⋮----
/**
 * Complete tool - signals task completion and returns control to orchestrator
 * 
 * IMPORTANT: This tool ALWAYS routes to the orchestrator, regardless of who invoked you.
 * Use this when:
 * - You've completed your assigned task
 * - You need the orchestrator to decide next steps or phase transitions
 * - You've gathered enough information to move forward (e.g., requirements are clear)
 * 
 * DO NOT use this for conversational responses - just respond normally for those.
 * YOUR JOB IS NOT DONE UNTIL YOU EXPLICITLY USE THIS TOOL
 */
⋮----
// Use the shared completion handler
⋮----
// Return success with the completion
</file>

<file path="src/tools/implementations/continue.ts">
import { ALL_PHASES, type Phase } from "@/conversations/phases";
import { type ProjectContext, getProjectContext } from "@/services/ProjectContext";
import type { Tool, NonEmptyArray } from "../types";
import { success, failure, createZodSchema } from "../types";
import { z } from "zod";
⋮----
/**
 * Continue tool - orchestrator-only control flow tool
 * Routes conversation to next phase/agent
 */
interface ContinueInput {
    phase?: string;
    agents: string[];
    reason: string;
}
⋮----
// Runtime check for orchestrator
⋮----
// Validate agents array is not empty
⋮----
// Get project context
⋮----
// Validate agents and collect valid pubkeys
⋮----
// Try exact match first
⋮----
// If not found, try case-insensitive search
⋮----
// We know validPubkeys is non-empty due to check above
⋮----
// Use current phase if not specified (phase is already lowercase from schema transform)
⋮----
// Update phase IMMEDIATELY if transitioning
⋮----
// Return properly typed control flow
</file>

<file path="src/tools/implementations/createMilestoneTask.ts">
import { getNDK } from "@/nostr";
import { getProjectContext } from "@/services/ProjectContext";
import { logger } from "@/utils/logger";
import { NDKTask } from "@nostr-dev-kit/ndk";
import { z } from "zod";
import type { Tool } from "../types";
import { createZodSchema } from "../types";
⋮----
interface CreateMilestoneTaskInput {
    title: string;
    description: string;
    assignees?: string[];
}
⋮----
interface CreateMilestoneTaskOutput {
    message: string;
    eventId: string;
    title: string;
    descriptionLength: number;
    assignees: string[] | undefined;
}
⋮----
// Check if agent signer is available
⋮----
// Get NDK instance
⋮----
// Get project context
⋮----
// Create the task event
⋮----
// Tag the project event
⋮----
// Add status tag
⋮----
// Add milestone tag to distinguish from claude_code tasks
⋮----
// Add phase tag
⋮----
// If we're in a conversation, reference it as parent
⋮----
// Add assignee tags if provided
⋮----
// Sign and publish the event
</file>

<file path="src/tools/implementations/endConversation.ts">
import { logger } from "@/utils/logger";
import type { Tool, Termination } from "../types";
import { success, failure, createZodSchema } from "../types";
import { z } from "zod";
⋮----
/**
 * End conversation tool - orchestrator-only tool to conclude conversations
 * Returns final response to the user
 */
⋮----
// Runtime check for orchestrator
⋮----
// Publish the final event directly
⋮----
// Log the completion
⋮----
// Return properly typed termination
⋮----
summary: summary || response, // Use summary if provided, otherwise use response
success: true, // Can add logic to determine success based on context
</file>

<file path="src/tools/implementations/generateInventory.ts">
import { exec } from "node:child_process";
import { promisify } from "node:util";
import { generateInventory, inventoryExists } from "@/utils/inventory";
import { logger } from "@/utils/logger";
import { z } from "zod";
import type { Tool } from "../types";
import { createZodSchema } from "../types";
⋮----
type GenerateInventoryInput = z.infer<typeof generateInventorySchema>;
⋮----
interface GenerateInventoryOutput {
    message: string;
    inventoryExists: boolean;
    regenerated: boolean;
}
⋮----
// Check if inventory already exists
⋮----
// Check for recently modified files using git status
⋮----
files: focusFiles.slice(0, 10), // Log first 10 files
⋮----
// Continue without focus files
⋮----
// Prepare options if agent context is available
⋮----
// Generate the inventory
</file>

<file path="src/tools/implementations/learn.ts">
import { NDKAgentLesson } from "@/events/NDKAgentLesson";
import { getNDK } from "@/nostr";
import { getProjectContext } from "@/services/ProjectContext";
import { logger } from "@/utils/logger";
import { z } from "zod";
import type { Tool } from "../types";
import { createZodSchema } from "../types";
⋮----
interface LearnInput {
    title: string;
    lesson: string;
}
⋮----
interface LearnOutput {
    message: string;
    eventId: string;
    title: string;
}
⋮----
// Create the lesson event
⋮----
// Add reference to the agent event if available
⋮----
// Add project tag for scoping
⋮----
// Sign and publish the event
</file>

<file path="src/tools/implementations/readPath.ts">
import { readFile } from "node:fs/promises";
import { stat, readdir } from "node:fs/promises";
import type { Tool } from "../types";
import { createZodSchema } from "../types";
import { resolveAndValidatePath } from "../utils";
import { z } from "zod";
⋮----
type ReadPathInput = z.infer<typeof readPathSchema>;
type ReadPathOutput = string;
⋮----
/**
 * Read path tool - effect tool that reads files or directories from filesystem
 * Performs I/O side effects
 */
⋮----
// Resolve path and ensure it's within project
⋮----
// Check if path is a directory first
⋮----
// Get directory contents
⋮----
// Track file read in conversation metadata if path starts with context/
⋮----
// Only add if not already tracked
⋮----
// If it's an EISDIR error that we somehow missed, provide helpful guidance
⋮----
// If we can't read the directory, fall back to the original error
</file>

<file path="src/tools/implementations/shell.ts">
import type { Tool } from "../types";
import { success, failure, createZodSchema } from "../types";
import { z } from "zod";
import { exec } from "child_process";
import { promisify } from "util";
import { logger } from "@/utils/logger";
⋮----
/**
 * Shell tool - allows agents to execute shell commands
 * Restricted to project-manager agent for safety
 */
⋮----
// Safety check - only project-manager can use this tool
⋮----
// Ensure safe environment
</file>

<file path="src/tools/implementations/writeContextFile.ts">
import { mkdir, writeFile, access } from "node:fs/promises";
⋮----
import { z } from "zod";
import type { Tool } from "../types";
import { createZodSchema } from "../types";
import { NDKArticle } from "@nostr-dev-kit/ndk";
import { getNDK } from "@/nostr";
import { logger } from "@/utils/logger";
import { getProjectContext } from "@/services";
⋮----
interface WriteContextFileInput {
    filename: string;
    content: string;
    title: string;
}
⋮----
interface WriteContextFileOutput {
    message: string;
}
⋮----
// TODO: Implement agent role check when available in context
// Only project-manager should use this tool
⋮----
// Extract just the filename from any path
// If given ../../context/TEST.md or TEST.md, just use TEST.md
⋮----
// Only allow markdown files
⋮----
// Construct the full path
⋮----
// Check if this file was recently read from persisted conversation metadata
⋮----
// Check if file exists
⋮----
// File doesn't exist, allow creation
⋮----
// If file exists and wasn't recently read, deny access
⋮----
// Ensure context directory exists
⋮----
// Write the file
⋮----
// Publish NDKArticle for this context file update
⋮----
// Use the filename without .md extension as the dTag
⋮----
// Set article properties
⋮----
// Tag the article with the project
⋮----
// Sign with the agent's signer
⋮----
// Log error but don't fail the tool execution
</file>

<file path="src/tools/core.ts">
/**
 * First-principles type system for TENEX tools
 *
 * Core philosophy:
 * - Type safety through algebraic data types
 * - Direct async/await for simplicity
 * - Explicit error handling with Result types
 */
⋮----
import type { Phase } from "@/conversations/phases";
import type { Agent } from "@/agents/types";
import type { Conversation } from "@/conversations/types";
import type { ConversationManager } from "@/conversations/ConversationManager";
import type { NostrPublisher } from "@/nostr/NostrPublisher";
import { NDKEvent } from "@nostr-dev-kit/ndk";
⋮----
// ============================================================================
// Core Result Type
// ============================================================================
⋮----
// Import metadata type from executor
import type { ToolExecutionMetadata } from "./executor";
⋮----
// Result type for fallible operations with optional metadata
export type Result<E, A> =
    | { readonly ok: true; readonly value: A; readonly metadata?: ToolExecutionMetadata }
    | { readonly ok: false; readonly error: E };
⋮----
// ============================================================================
// Simple Tool Interface
// ============================================================================
⋮----
// Import unified ExecutionContext
import type { ExecutionContext } from "@/agents/execution/types";
⋮----
// ============================================================================
// Tool Type Definition
// ============================================================================
⋮----
/**
 * Simple, unified tool interface
 */
export interface Tool<Input = unknown, Output = unknown> {
    readonly name: string;
    readonly description: string;
    readonly parameters: ParameterSchema<Input>;
    readonly promptFragment?: string;
    readonly execute: (
        input: Validated<Input>,
        context: ExecutionContext
    ) => Promise<Result<ToolError, Output>>;
}
⋮----
// ============================================================================
// Control Flow and Termination Types
// ============================================================================
⋮----
export interface ContinueFlow {
    readonly type: "continue";
    readonly routing: RoutingDecision;
}
⋮----
export interface RoutingDecision {
    readonly phase?: Phase;
    readonly agents: NonEmptyArray<string>; // Agent pubkeys
    readonly reason: string;
    readonly context?: Readonly<Record<string, unknown>>;
}
⋮----
readonly agents: NonEmptyArray<string>; // Agent pubkeys
⋮----
/**
 * Termination types that end execution
 */
export type Termination = Complete | EndConversation;
⋮----
export interface Complete {
    readonly type: "complete";
    readonly completion: CompletionSummary;
}
⋮----
export interface EndConversation {
    readonly type: "end_conversation";
    readonly result: ConversationResult;
}
⋮----
export interface CompletionSummary {
    readonly response: string;
    readonly summary: string;
    readonly nextAgent: string;
}
⋮----
export interface ConversationResult {
    readonly response: string;
    readonly summary: string;
    readonly success: boolean;
    readonly artifacts?: ReadonlyArray<string>;
}
⋮----
// ============================================================================
// Parameter Schema and Validation
// ============================================================================
⋮----
export interface ParameterSchema<T> {
    readonly shape: SchemaShape;
    readonly validate: (input: unknown) => Result<ValidationError, Validated<T>>;
}
⋮----
export type SchemaShape =
    | { type: "string"; description: string; enum?: ReadonlyArray<string>; required?: boolean }
    | { type: "number"; description: string; min?: number; max?: number; required?: boolean }
    | { type: "boolean"; description: string; required?: boolean }
    | { type: "array"; description: string; items: SchemaShape; required?: boolean }
    | {
          type: "object";
          description: string;
          properties: Readonly<Record<string, SchemaShape>>;
          required?: ReadonlyArray<string>;
      };
⋮----
// Branded type for validated input
export interface Validated<T> {
    readonly _brand: "validated";
    readonly value: T;
}
⋮----
// ============================================================================
// Error Types
// ============================================================================
⋮----
export type ToolError = ValidationError | ExecutionError | SystemError;
⋮----
export interface ValidationError {
    readonly kind: "validation";
    readonly field: string;
    readonly message: string;
}
⋮----
export interface ExecutionError {
    readonly kind: "execution";
    readonly tool: string;
    readonly message: string;
    readonly cause?: unknown;
}
⋮----
export interface SystemError {
    readonly kind: "system";
    readonly message: string;
    readonly stack?: string;
}
⋮----
// ============================================================================
// Helper Types
// ============================================================================
⋮----
// Non-empty array type
export interface NonEmptyArray<T> extends ReadonlyArray<T> {
    readonly 0: T;
}
⋮----
// Helper type guards
export const isNonEmptyArray = <T>(array: ReadonlyArray<T>): array is NonEmptyArray<T>
⋮----
// ============================================================================
// Result Constructors
// ============================================================================
⋮----
export const success = <A>(value: A, metadata?: ToolExecutionMetadata): Result<never, A> => (
⋮----
export const failure = <E>(error: E): Result<E, never> => (
</file>

<file path="src/tools/executor.ts">
/**
 * Simplified tool executor
 */
⋮----
import type { Tool, ToolError } from "./core";
import type { ExecutionContext } from "./types";
import { logger } from "@/utils/logger";
⋮----
/**
 * Metadata that tools can provide for better UI/logging
 */
export interface ToolExecutionMetadata {
    /** Human-readable message describing what the tool is doing */
    displayMessage?: string;
    /** The actual arguments that were executed (for tools that skip tool_start) */
    executedArgs?: Record<string, unknown>;
    /** Any other metadata the tool wants to provide */
    [key: string]: unknown;
}
⋮----
/** Human-readable message describing what the tool is doing */
⋮----
/** The actual arguments that were executed (for tools that skip tool_start) */
⋮----
/** Any other metadata the tool wants to provide */
⋮----
/**
 * Simple, unified tool execution result
 */
export interface ToolExecutionResult<T = unknown> {
    success: boolean;
    output?: T;
    error?: ToolError;
    duration: number;
    /** Optional metadata for UI/logging purposes */
    metadata?: ToolExecutionMetadata;
}
⋮----
/** Optional metadata for UI/logging purposes */
⋮----
/**
 * Simple tool executor
 */
export class ToolExecutor
⋮----
constructor(private readonly context: ExecutionContext)
⋮----
/**
     * Execute a tool with the given input
     */
async execute<I, O>(tool: Tool<I, O>, input: unknown): Promise<ToolExecutionResult<O>>
⋮----
// Skip validation for generate_inventory tool
⋮----
// Pass through any input for generate_inventory
⋮----
// Validate input for other tools
⋮----
// Execute the tool
⋮----
metadata: result.metadata, // Pass through metadata if present
⋮----
/**
 * Create a tool executor for a given context
 */
export function createToolExecutor(context: ExecutionContext): ToolExecutor
</file>

<file path="src/tools/registry.ts">
import { analyze } from "./implementations/analyze";
import { continueTool } from "./implementations/continue";
import { createMilestoneTaskTool } from "./implementations/createMilestoneTask";
import { endConversationTool } from "./implementations/endConversation";
import { generateInventoryTool } from "./implementations/generateInventory";
import { learnTool } from "./implementations/learn";
import { readPathTool } from "./implementations/readPath";
import { writeContextFileTool } from "./implementations/writeContextFile";
import { completeTool } from "./implementations/complete";
import { shellTool } from "./implementations/shell";
import type { Tool } from "./types";
⋮----
// Registry of all available tools
⋮----
export function getTool(name: string): Tool | undefined
⋮----
export function getTools(names: string[]): Tool[]
⋮----
export function getAllTools(): Tool[]
</file>

<file path="src/tools/toolLogger.ts">
import { promises as fs } from "node:fs";
import { join } from "node:path";
import type { ExecutionContext, ToolExecutionResult, ToolError } from "./types";
⋮----
export interface ToolCallLogEntry {
    timestamp: string;
    timestampMs: number;
    requestId: string;

    // Context
    agentName: string;
    phase: string;
    conversationId: string;

    // Tool information
    toolName: string;
    args: Record<string, unknown>;
    argsLength: number;

    // Result
    status: "success" | "error";
    output?: string;
    outputLength?: number;
    error?: string;
    metadata?: Record<string, unknown>;

    // Performance
    performance: {
        startTime: number;
        endTime: number;
        durationMs: number;
    };

    // Trace information
    trace: {
        callStack?: string[];
        parentRequestId?: string;
        batchId?: string;
        batchIndex?: number;
        batchSize?: number;
    };
}
⋮----
// Context
⋮----
// Tool information
⋮----
// Result
⋮----
// Performance
⋮----
// Trace information
⋮----
export class ToolCallLogger
⋮----
constructor(projectPath: string)
⋮----
private async ensureLogDirectory(): Promise<void>
⋮----
// Ignore if directory already exists
⋮----
private getLogFileName(): string
⋮----
const date = new Date().toISOString().split("T")[0]; // YYYY-MM-DD
⋮----
private getLogFilePath(): string
⋮----
private generateRequestId(toolName: string, agentName: string): string
⋮----
async logToolCall(
        toolName: string,
        args: Record<string, unknown>,
        context: ExecutionContext,
        result: ToolExecutionResult,
        performance: { startTime: number; endTime: number },
        trace?: {
            callStack?: string[];
            parentRequestId?: string;
            batchId?: string;
            batchIndex?: number;
            batchSize?: number;
        }
): Promise<void>
⋮----
// Context
⋮----
// Tool information
⋮----
// Result
⋮----
// Performance
⋮----
// Trace information
⋮----
// Write to JSONL file
⋮----
// Don't let logging errors break the main flow
⋮----
private extractOutput(result: ToolExecutionResult): string | undefined
⋮----
// Check if it's a control flow result
⋮----
// Check if it's a termination result
⋮----
// Regular tool output
⋮----
private extractError(result: ToolExecutionResult): string | undefined
⋮----
private extractMetadata(result: ToolExecutionResult): Record<string, unknown> | undefined
⋮----
// Return metadata for special result types
⋮----
private formatError(error: ToolError): string
⋮----
// Singleton instance
⋮----
export function initializeToolLogger(projectPath: string): ToolCallLogger
⋮----
export function getToolLogger(): ToolCallLogger | null
</file>

<file path="src/tools/types.ts">
/**
 * Simplified tool system for TENEX
 */
⋮----
import type { Phase } from "@/conversations/phases";
import type { Agent } from "@/agents/types";
import type { Conversation } from "@/conversations/types";
import type { ConversationManager } from "@/conversations/ConversationManager";
import type { NDKEvent } from "@nostr-dev-kit/ndk";
import type { NostrPublisher } from "@/nostr/NostrPublisher";
⋮----
// Re-export core types
⋮----
// Re-export unified ExecutionContext from agents
</file>

<file path="src/tools/utils.ts">
import { isAbsolute, relative, resolve } from "node:path";
⋮----
/**
 * Resolves and validates a file path to ensure it stays within the project boundaries.
 *
 * @param filePath - The file path to validate (can be absolute or relative)
 * @param projectPath - The root project path
 * @returns The resolved absolute path if valid
 * @throws Error if the path would escape the project directory
 */
export function resolveAndValidatePath(filePath: string, projectPath: string): string
</file>

<file path="src/tools/zod-schema.ts">
/**
 * Zod-based schema system for TENEX tools
 */
⋮----
import { z } from "zod";
import type { ParameterSchema, SchemaShape, Validated, ValidationError, Result } from "./core";
⋮----
// MCP schema type definitions
interface MCPPropertyDefinition {
    type?: "string" | "number" | "integer" | "boolean" | "array" | "object";
    description?: string;
    enum?: string[];
    minLength?: number;
    maxLength?: number;
    minimum?: number;
    maximum?: number;
    items?: MCPPropertyDefinition;
    properties?: Record<string, MCPPropertyDefinition>;
    required?: string[];
    minItems?: number;
    maxItems?: number;
}
⋮----
interface MCPSchema {
    properties?: Record<string, MCPPropertyDefinition>;
    required?: string[];
}
⋮----
/**
 * Convert a Zod schema to our SchemaShape format
 */
function zodToSchemaShape(schema: z.ZodType<unknown>, isOptional = false): SchemaShape
⋮----
// Handle undefined/null/void schemas
⋮----
// Handle union schemas (like z.undefined().or(z.null()))
⋮----
// If it's a union of undefined/null/void, treat it as no parameters
⋮----
// Otherwise, use the first option
⋮----
// For enum values, we need to check if it's a ZodEnum
⋮----
// Get min/max from the schema definition more safely
⋮----
// Fallback for unknown types
⋮----
/**
 * Create a ParameterSchema from a Zod schema
 */
export function createZodSchema<T>(schema: z.ZodType<T>): ParameterSchema<T>
⋮----
// Extract the first error for simplicity
⋮----
/**
 * Helper function to create schemas with descriptions
 */
export function withDescription<T extends z.ZodType<unknown>>(schema: T, description: string): T
⋮----
/**
 * Common schema patterns for tools
 */
⋮----
/**
     * File path schema with validation
     */
⋮----
/**
     * Command schema with safety checks
     */
⋮----
/**
     * Phase schema
     */
⋮----
/**
     * Agent pubkey schema
     */
⋮----
/**
     * Non-empty string
     */
⋮----
/**
     * Optional field helper
     */
⋮----
/**
     * Array with at least one element
     */
⋮----
/**
 * Convert MCP tool input schema to Zod schema
 */
export function mcpSchemaToZod(mcpSchema: MCPSchema): z.ZodType<unknown>
⋮----
/**
 * Convert a single MCP property definition to Zod schema
 */
function mcpPropertyToZod(propDef: MCPPropertyDefinition): z.ZodType<unknown>
⋮----
/**
 * Type inference helper for Zod schemas
 */
export type InferZodSchema<T extends z.ZodType<unknown>> = z.infer<T>;
</file>

<file path="src/tracing/__tests__/TracingContext.test.ts">
import { describe, expect, it, vi } from "vitest";
import {
    createTracingContext,
    createAgentExecutionContext,
    createToolExecutionContext,
    createPhaseExecutionContext,
    generateExecutionId,
    formatTracingContext,
    type TracingContext
} from "../TracingContext";
⋮----
// Empty strings are falsy, so they won't be included in optional fields
⋮----
// Start with base context
⋮----
// Add agent context
⋮----
// Add phase context
⋮----
// Add tool context
⋮----
// Original should be unchanged
⋮----
// They should share the same base properties
</file>

<file path="src/tracing/__tests__/TracingLogger.test.ts">
import { describe, expect, it, beforeEach } from "bun:test";
import { TracingLogger } from "../TracingLogger";
import type { TracingContext } from "../TracingContext";
</file>

<file path="src/tracing/index.ts">

</file>

<file path="src/tracing/TracingContext.ts">
import { randomBytes } from "node:crypto";
⋮----
/**
 * Minimal context for debugging execution flow through the TENEX system.
 */
export interface TracingContext {
    conversationId: string; // ID of the conversation (from Nostr event)
    executionId: string; // Unique ID for this specific execution/request
    currentAgent?: string; // Current agent name for debugging
    currentPhase?: string; // Current phase for debugging
    currentTool?: string; // Current tool being executed
}
⋮----
conversationId: string; // ID of the conversation (from Nostr event)
executionId: string; // Unique ID for this specific execution/request
currentAgent?: string; // Current agent name for debugging
currentPhase?: string; // Current phase for debugging
currentTool?: string; // Current tool being executed
⋮----
/**
 * Generate a unique execution ID
 */
export function generateExecutionId(prefix = "exec"): string
⋮----
/**
 * Create a new tracing context for a conversation
 */
export function createTracingContext(conversationId: string): TracingContext
⋮----
/**
 * Create an agent execution context
 */
export function createAgentExecutionContext(
    parent: TracingContext,
    agentName: string
): TracingContext
⋮----
/**
 * Create a tool execution context
 */
export function createToolExecutionContext(
    parent: TracingContext,
    toolName: string
): TracingContext
⋮----
/**
 * Create a phase execution context
 */
export function createPhaseExecutionContext(parent: TracingContext, phase: string): TracingContext
⋮----
/**
 * Format tracing context for logging
 */
export function formatTracingContext(context: TracingContext): Record<string, unknown>
</file>

<file path="src/tracing/TracingLogger.ts">
import { type LogModule, logger as baseLogger, parseModuleVerbosity } from "@/utils/logger";
import type { TracingContext } from "./TracingContext";
import { formatTracingContext } from "./TracingContext";
⋮----
/**
 * Enhanced logger that automatically includes tracing context in all log entries
 */
export class TracingLogger
⋮----
constructor(
        private context: TracingContext,
        private module?: LogModule
)
⋮----
// Check if tracing is enabled based on module verbosity
⋮----
// Only enable tracing for verbose or debug levels
⋮----
/**
     * Create a scoped logger for a specific module
     */
forModule(module: LogModule): TracingLogger
⋮----
/**
     * Update the tracing context
     */
withContext(context: TracingContext): TracingLogger
⋮----
/**
     * Format message with tracing context
     */
private formatMessage(
        _message: string,
        additionalContext?: Record<string, unknown>
): [Record<string, unknown>]
⋮----
info(message: string, additionalContext?: Record<string, unknown>): void
⋮----
success(message: string, additionalContext?: Record<string, unknown>): void
⋮----
// Log context separately for success messages
⋮----
warning(message: string, additionalContext?: Record<string, unknown>): void
⋮----
error(message: string, error?: unknown, additionalContext?: Record<string, unknown>): void
⋮----
// Log error with full context
⋮----
debug(message: string, additionalContext?: Record<string, unknown>): void
⋮----
/**
     * Log the start of an operation
     */
startOperation(operation: string, additionalContext?: Record<string, unknown>): void
⋮----
/**
     * Log the completion of an operation
     */
completeOperation(operation: string, additionalContext?: Record<string, unknown>): void
⋮----
/**
     * Log a failed operation
     */
failOperation(
        operation: string,
        error: unknown,
        additionalContext?: Record<string, unknown>
): void
⋮----
/**
     * Log an event publication
     */
logEventPublished(
        eventId: string,
        eventType: string,
        additionalContext?: Record<string, unknown>
): void
⋮----
/**
     * Log LLM interaction
     */
logLLMRequest(model: string, additionalContext?: Record<string, unknown>): void
⋮----
/**
     * Log LLM response
     */
logLLMResponse(model: string, additionalContext?: Record<string, unknown>): void
⋮----
/**
 * Create a tracing logger instance
 */
export function createTracingLogger(context: TracingContext, module?: LogModule): TracingLogger
</file>

<file path="src/utils/__tests__/lessonMetrics.test.ts">
import { describe, it, expect, beforeEach, mock } from "bun:test";
import {
    calculateLessonMetrics,
    logLessonMetrics,
    logLessonUsage,
    logLessonCreationPattern,
} from "../lessonMetrics";
import type { ProjectContext } from "@/services/ProjectContext";
import type { NDKAgentLesson } from "@/events/NDKAgentLesson";
import { logger } from "@/utils/logger";
⋮----
// Mock the logger
⋮----
// Helper to create mock lessons
function createMockLesson(params: {
    id: string;
    pubkey: string;
    title: string;
    lesson: string;
    phase?: string;
    keywords?: string[];
    createdAt?: number;
}): NDKAgentLesson
⋮----
content: params.lesson, // Some lessons may use content field
⋮----
// Helper to create mock ProjectContext
function createMockProjectContext(
    lessons: NDKAgentLesson[],
    agents: { name: string; pubkey: string }[]
): ProjectContext
⋮----
createMockLesson({ id: "5", pubkey: "a1", title: "L5", lesson: "Test" }), // No phase
⋮----
expect(metrics.mostCommonKeywords[2].count).toBe(1); // react, promises, git, rebase all have count 1
⋮----
// Create lessons with 15 unique keywords
⋮----
createMockLesson({ id: "1", pubkey: "a1", title: "L1", lesson: "Short" }), // 5 chars
⋮----
}), // 19 chars
createMockLesson({ id: "3", pubkey: "a1", title: "L3", lesson: "Medium length" }), // 13 chars
⋮----
// (5 + 19 + 13) / 3 = 37 / 3 = 12.33... rounds to 12
⋮----
content: "Content instead of lesson", // 25 chars
⋮----
const now = Date.now() / 1000; // Current time in seconds
⋮----
}), // 1 day ago
⋮----
}), // 1 hour ago
⋮----
}), // 2 hours ago
⋮----
}), // Now
⋮----
createMockLesson({ id: "1", pubkey: "a1", title: "L1", lesson: "Test" }), // No timestamp
⋮----
}), // Zero timestamp
⋮----
{ id: "2", pubkey: "malformed1", tags: [] } as any, // Has pubkey but minimal data
{ id: "3", pubkey: "malformed2", tags: [] } as any, // Has pubkey but minimal data
⋮----
// Should not throw
⋮----
expect(metrics.totalLessons).toBe(4); // All lessons counted, even malformed ones
expect(metrics.lessonsByAgent.get("Unknown")).toBe(2); // Malformed lessons counted as Unknown (no matching agent)
⋮----
// Create lessons with various malformed structures
⋮----
{ id: "2", tags: null } as any, // null tags
{ id: "3", tags: undefined } as any, // undefined tags
{ id: "4" } as any, // no tags property at all
⋮----
// We need to update the implementation to handle this gracefully
// For now, let's create a wrapper that filters out truly malformed lessons
⋮----
expect(metrics.totalLessons).toBe(1); // Only the valid lesson
</file>

<file path="src/utils/git/createExecutionBranch.ts">
import { execSync } from "node:child_process";
import { logger } from "@/utils/logger";
⋮----
export interface GitBranchResult {
    branchName: string;
    created: boolean;
}
⋮----
/**
 * Create a git branch for execution
 */
export function createExecutionBranch(
    baseName: string,
    projectPath: string = process.cwd()
): GitBranchResult
⋮----
// Check if we're in a git repository
⋮----
// Generate branch name
⋮----
// Create and checkout new branch
</file>

<file path="src/utils/git/gitignore.ts">
import fs from "node:fs/promises";
import path from "node:path";
import { logger } from "@/utils/logger";
⋮----
/**
 * Ensures .tenex is in the project's .gitignore file
 */
export async function ensureTenexInGitignore(projectPath: string): Promise<void>
⋮----
// Check if .gitignore exists
⋮----
// .gitignore doesn't exist, we'll create it
⋮----
// Check if .tenex is already in .gitignore
⋮----
// Add .tenex to .gitignore
</file>

<file path="src/utils/git/index.ts">

</file>

<file path="src/utils/git/initializeGitRepo.ts">
import { exec } from "node:child_process";
import fs from "node:fs/promises";
import path from "node:path";
import { promisify } from "node:util";
import { logger } from "@/utils/logger";
⋮----
/**
 * Check if a directory is a git repository (has its own .git directory)
 */
export async function isGitRepository(projectPath: string): Promise<boolean>
⋮----
/**
 * Initialize a git repository in the given directory
 */
export async function initializeGitRepository(projectPath: string): Promise<void>
</file>

<file path="src/utils/agentFetcher.ts">
import type NDK from "@nostr-dev-kit/ndk";
import { logger } from "./logger";
⋮----
/**
 * Fetches an agent definition from a Nostr event
 * @param eventId - The ID of the event containing the agent definition
 * @param ndk - The NDK instance to use for fetching
 * @returns The agent definition or null if not found
 */
export async function fetchAgentDefinition(
    eventId: string,
    ndk: NDK
): Promise<
⋮----
kinds: [4199], // NDKAgent kind
</file>

<file path="src/utils/conversationFetcher.ts">
import { getAgentSlugFromEvent } from "@/nostr/utils";
import { type ProjectContext, getProjectContext } from "@/services";
import type NDK from "@nostr-dev-kit/ndk";
import type { NDKEvent } from "@nostr-dev-kit/ndk";
import chalk from "chalk";
⋮----
interface ConversationEvent {
    event: NDKEvent;
    author: string;
    isHuman: boolean;
    timestamp: Date;
    content: string;
    depth: number;
}
⋮----
interface ConversationTree {
    root: ConversationEvent;
    replies: Map<string, ConversationEvent[]>;
}
⋮----
export async function fetchConversation(
    bech32Event: string,
    ndk: NDK,
    _projectPath: string
): Promise<string>
⋮----
// Fetch the event directly using the nevent string
⋮----
// Get project context to identify human user
⋮----
// Get the root event Id (it's either E-tagged or then we already have it).
⋮----
// Fetch profiles for all participants
⋮----
// Build conversation tree
⋮----
// Format as markdown
⋮----
async function fetchAllEventsInConversation(ndk: NDK, rootEventId: string): Promise<NDKEvent[]>
⋮----
// Sort by created_at
⋮----
async function fetchParticipantProfiles(
    events: NDKEvent[],
    ndk: NDK,
    projectCtx: ProjectContext
): Promise<Map<string, string>>
⋮----
// Collect unique pubkeys
⋮----
// Fetch profiles
⋮----
// Check if it's an agent first
⋮----
// Fetch from nostr
⋮----
function buildConversationTree(
    events: NDKEvent[],
    participants: Map<string, string>,
    humanPubkey: string
): ConversationTree
⋮----
// First pass: create ConversationEvent objects
⋮----
// Find parent
⋮----
// This is the root
⋮----
// Second pass: build reply structure
⋮----
// Direct reply to root
⋮----
// Calculate depths
function setDepth(event: ConversationEvent, depth: number): void
⋮----
// Get the first event from the map if no root event found
⋮----
function formatConversationMarkdown(tree: ConversationTree, humanPubkey: string): string
⋮----
function formatEvent(event: ConversationEvent, indent = ""): void
⋮----
// Format content with proper indentation
⋮----
lines.push(""); // Empty line between messages
⋮----
// Format replies
</file>

<file path="src/utils/error-formatter.ts">
/**
 * Format various error types into human-readable strings
 */
export function formatToolError(error: unknown): string
⋮----
// Try to extract meaningful properties from the error object
⋮----
// Common error properties
⋮----
// If we found specific properties, use them
⋮----
// Otherwise, try to stringify the object
</file>

<file path="src/utils/errors.ts">
export function formatError(error: unknown): string
</file>

<file path="src/utils/formatting.ts">
import chalk from "chalk";
⋮----
/**
 * Format markdown text with chalk styling
 */
export function formatMarkdown(text: string): string
⋮----
/**
 * Colorize JSON string with chalk styling
 */
export function colorizeJSON(json: string): string
</file>

<file path="src/utils/inventory.ts">
import { loadLLMRouter } from "@/llm";
import { TaskPublisher, getNDK } from "@/nostr";
import { PromptBuilder } from "@/prompts/core/PromptBuilder";
import { configService, getProjectContext, isProjectContextInitialized } from "@/services";
import { logger } from "@/utils/logger";
import type { NDKTask } from "@nostr-dev-kit/ndk";
import { Message } from "multi-llm-ts";
import { generateRepomixOutput } from "./repomix.js";
import "@/prompts"; // This ensures all fragments are registered
import type { Agent } from "@/agents/types";
⋮----
interface ComplexModule {
    name: string;
    path: string;
    reason: string;
    suggestedFilename: string;
}
⋮----
interface ComplexModulesResponse {
    complexModules: ComplexModule[];
}
⋮----
interface InventoryResult {
    content: string;
    complexModules: ComplexModule[];
}
⋮----
interface InventoryGenerationOptions {
    conversationRootEventId?: string;
    agent?: Agent;
    focusFiles?: Array<{ path: string; status: string }>;
}
⋮----
/**
 * Generate comprehensive inventory using repomix + LLM
 */
export async function generateInventory(
    projectPath: string,
    options?: InventoryGenerationOptions
): Promise<void>
⋮----
// Ensure context directory exists
⋮----
// Create NDK task if context is available
⋮----
// Initial status update
⋮----
// Step 1: Generate repomix content once for efficiency
⋮----
// Step 2: Generate main inventory with complex module identification
⋮----
// Step 3: Save main inventory
⋮----
// TODO: Replace with proper NostrPublisher approach
// if (task && taskPublisher) {
//     await taskPublisher.updateTask(task, {
//         status: "in-progress",
//         progress: 50,
//         message: "Main inventory generated, analyzing complex modules...",
//     });
// }
⋮----
// Step 4: Generate individual module guides for complex modules (max 10)
⋮----
if (!module) continue; // Skip if module is undefined
⋮----
// TypeScript now knows module is defined
⋮----
// TODO: Replace with proper NostrPublisher approach
// if (task && taskPublisher) {
//     const progress = 50 + Math.floor(((i + 1) / modulesToProcess.length) * 40);
//     await taskPublisher.updateTask(task, {
//         status: "in-progress",
//         progress,
//         message: `Generated guide for ${definedModule.name}`,
//     });
// }
⋮----
// Final completion update
⋮----
// TODO: Replace with proper NostrPublisher approach
// await taskPublisher.updateTask(task, {
//     status: "completed",
//     progress: 100,
//     message: `Inventory generation completed with ${modulesToProcess.length} complex module guides`,
// });
⋮----
/**
 * Generate main inventory and identify complex modules
 */
async function generateMainInventory(
    projectPath: string,
    repomixContent: string,
    focusFiles?: Array<{ path: string; status: string }>
): Promise<InventoryResult>
⋮----
// Use PromptBuilder to construct the prompt from fragments
⋮----
// Debug: Log the router configuration
⋮----
// Extract complex modules from JSON at the end
⋮----
// Strip the complexModules JSON block from the content before saving
⋮----
/**
 * Generate detailed guide for a specific complex module
 */
async function generateModuleGuide(
    projectPath: string,
    module: ComplexModule,
    repomixContent: string
): Promise<void>
⋮----
// Use PromptBuilder to construct the prompt from fragments
⋮----
// Save module guide
⋮----
/**
 * Strip the complexModules JSON block from the inventory content
 */
function stripComplexModulesJson(content: string): string
⋮----
// Pattern to match the entire section about complex modules JSON format
// This includes the instruction text and the JSON block
⋮----
// Remove the entire complex modules JSON section
⋮----
// Also handle case where JSON block might appear without the instruction text
⋮----
/**
 * Type guard to validate complex modules response structure
 */
function isComplexModulesResponse(data: unknown): data is ComplexModulesResponse
⋮----
/**
 * Extract complex modules from LLM response with fallback mechanism
 */
async function extractComplexModules(
    content: string,
    projectPath?: string
): Promise<ComplexModule[]>
⋮----
// Look for JSON block at the end
⋮----
// Type guard to validate the structure
⋮----
/**
 * Fallback mechanism for JSON extraction using a cleanup LLM call
 */
async function fallbackExtractComplexModules(
    content: string,
    projectPath?: string
): Promise<ComplexModule[]>
⋮----
// Use PromptBuilder to construct the prompt from fragments
⋮----
// Type guard to validate the structure
⋮----
/**
 * Update inventory for specific files (placeholder for future implementation)
 */
export async function updateInventory(projectPath: string, files: string[]): Promise<void>
⋮----
// For now, just regenerate the full inventory
// Future optimization: implement partial updates
⋮----
/**
 * Check if inventory exists
 */
export async function inventoryExists(projectPath: string): Promise<boolean>
⋮----
/**
 * Load inventory content for system prompts
 */
export async function loadInventoryContent(projectPath: string): Promise<string | null>
⋮----
/**
 * Get the inventory file path
 */
async function getInventoryPath(projectPath: string): Promise<string>
⋮----
/**
 * Load project configuration
 */
async function loadProjectConfig(
    projectPath: string
): Promise<
⋮----
// Get config from ProjectContext if available
⋮----
// Fallback: try to load config directly
</file>

<file path="src/utils/lessonMetrics.ts">
import type { NDKAgentLesson } from "@/events/NDKAgentLesson";
import type { ProjectContext } from "@/services/ProjectContext";
import { logger } from "@/utils/logger";
⋮----
export interface LessonMetrics {
    totalLessons: number;
    lessonsByAgent: Map<string, number>;
    lessonsByPhase: Map<string, number>;
    mostCommonKeywords: Array<{ keyword: string; count: number }>;
    averageLessonLength: number;
    oldestLesson?: Date;
    newestLesson?: Date;
}
⋮----
/**
 * Calculate comprehensive metrics for lessons in a project
 */
export function calculateLessonMetrics(projectCtx: ProjectContext): LessonMetrics
⋮----
// Count by agent
⋮----
// Count by phase
⋮----
// Count keywords
⋮----
// Track lesson length
⋮----
// Track timestamps
⋮----
// Sort keywords by frequency
⋮----
/**
 * Track lesson usage in prompts
 */
export function logLessonUsage(
    agentName: string,
    agentPubkey: string,
    lessonsShown: Array<{
        title: string;
        eventId: string;
        fromAgent: string;
        keywords: string[];
    }>
): void
⋮----
/**
 * Track lesson creation patterns
 */
export function logLessonCreationPattern(
    lesson: NDKAgentLesson,
    agentName: string,
    context: {
        phase: string;
        conversationId: string;
        totalLessonsForAgent: number;
        totalLessonsInProject: number;
    }
): void
</file>

<file path="src/utils/logger.ts">
import chalk from "chalk";
⋮----
/**
 * TENEX Logging System
 *
 * Environment Variables:
 * - LOG_LEVEL: Sets default verbosity (silent|normal|verbose|debug)
 * - TENEX_LOG: Enable specific modules with optional verbosity
 *   Format: module1:level,module2:level
 *   Example: TENEX_LOG=agent:debug,llm:verbose,tools
 *   If no level specified, defaults to debug
 *
 * Available modules:
 * - agent: Agent execution and behavior
 * - conversation: Conversation management
 * - llm: LLM interactions
 * - nostr: Nostr protocol operations
 * - tools: Tool execution
 * - general: General/miscellaneous logging
 *
 * Tracing (execution flow debugging) is only enabled for modules
 * with verbose or debug verbosity levels.
 */
⋮----
export type VerbosityLevel = "silent" | "normal" | "verbose" | "debug";
⋮----
export type LogModule = "agent" | "conversation" | "llm" | "nostr" | "tools" | "general";
⋮----
export interface ModuleVerbosityConfig {
    default: VerbosityLevel;
    modules?: {
        [moduleName: string]: VerbosityLevel;
    };
}
⋮----
export interface LoggerConfig {
    useEmoji?: boolean;
    useLabels?: boolean;
    debugEnabled?: boolean;
    moduleVerbosity?: ModuleVerbosityConfig;
}
⋮----
export function parseModuleVerbosity(): ModuleVerbosityConfig
⋮----
// Only try to parse environment variables in Node.js environment
⋮----
// Set default level from environment
⋮----
// Parse TENEX_LOG environment variable
// Format: TENEX_LOG=module1:level,module2:level
// Example: TENEX_LOG=agent:debug,llm:verbose,tools:debug
// If no level specified, defaults to debug
// Example: TENEX_LOG=agent,llm,tools (all set to debug)
⋮----
export function configureLogger(config: Partial<LoggerConfig>): void
⋮----
// Agent color assignment for consistent coloring
⋮----
function getAgentColor(agentName: string): typeof chalk.red
⋮----
function shouldLog(
    level: string,
    module?: LogModule,
    verbosityRequired: VerbosityLevel = "normal"
): boolean
⋮----
// Always show errors and warnings
⋮----
// Debug logs respect the debug flag
⋮----
// Get module-specific verbosity
⋮----
function formatModulePrefix(module?: LogModule): string
⋮----
export function logError(message: string, error?: unknown, module?: LogModule): void
⋮----
export function logInfo(
    message: string,
    module?: LogModule,
    verbosity: VerbosityLevel = "normal",
    ...args: unknown[]
): void
⋮----
export function logSuccess(
    message: string,
    module?: LogModule,
    verbosity: VerbosityLevel = "normal"
): void
⋮----
export function logWarning(
    message: string,
    module?: LogModule,
    verbosity: VerbosityLevel = "normal",
    ...args: unknown[]
): void
⋮----
export function logDebug(
    message: string,
    module?: LogModule,
    verbosity: VerbosityLevel = "debug",
    ...args: unknown[]
): void
⋮----
// Agent Logger class for contextual logging
export class AgentLogger
⋮----
constructor(agentName: string, projectName?: string)
⋮----
setModule(module: LogModule): void
⋮----
private formatMessage(
        emoji: string,
        message: string,
        colorFn: typeof chalk.red,
        verbosity: VerbosityLevel
): string
⋮----
info(message: string, verbosity: VerbosityLevel = "normal", ...args: unknown[]): void
⋮----
success(message: string, verbosity: VerbosityLevel = "normal", ...args: unknown[]): void
⋮----
warning(message: string, verbosity: VerbosityLevel = "normal", ...args: unknown[]): void
⋮----
error(message: string, error?: unknown): void
⋮----
// Errors always show
⋮----
debug(message: string, verbosity: VerbosityLevel = "debug", ...args: unknown[]): void
⋮----
// Factory function for creating agent loggers
export function createAgentLogger(agentName: string, projectName?: string): AgentLogger
⋮----
// Scoped logger for easier module-specific logging
export class ScopedLogger
⋮----
constructor(private module: LogModule)
⋮----
success(message: string, verbosity: VerbosityLevel = "normal"): void
⋮----
// Conversation flow logging functions
function truncateText(text: string, maxLength = 100): string
⋮----
function formatConversationHeader(conversationId?: string, title?: string): string
⋮----
// Human-readable conversation flow logging
export function logConversationStart(
    userMessage: string,
    conversationId?: string,
    title?: string,
    eventId?: string
): void
⋮----
export function logLLMInteraction(
    type: string,
    config: {
        model?: string;
        systemPrompt?: string;
        userPrompt?: string;
        response?: string;
        reasoning?: string;
    },
    conversationId?: string,
    title?: string
): void
⋮----
const maxLength = 500; // Configurable via env var in future
⋮----
export function logPhaseTransition(
    from: string,
    to: string,
    reason?: string,
    conversationId?: string,
    title?: string
): void
⋮----
export function logUserMessage(
    message: string,
    conversationId?: string,
    title?: string,
    eventId?: string
): void
⋮----
export function logAgentResponse(
    agentName: string,
    message: string,
    conversationId?: string,
    title?: string,
    eventId?: string
): void
⋮----
export function logConversationError(
    error: string,
    context?: Record<string, unknown>,
    conversationId?: string,
    title?: string
): void
⋮----
// Export a logger object for compatibility
⋮----
// Conversation flow logging
</file>

<file path="src/utils/process.ts">
import { logger } from "@/utils/logger";
⋮----
export type ShutdownHandler = (signal: string) => Promise<void>;
⋮----
export function setupGracefulShutdown(shutdownHandler: ShutdownHandler): void
⋮----
const shutdown = async (signal: string): Promise<void> =>
⋮----
// Handle various termination signals
⋮----
// Handle uncaught errors
</file>

<file path="src/utils/projectInitialization.ts">
import { ProjectManager } from "@/daemon/ProjectManager";
import { getNDK, initNDK } from "@/nostr/ndkClient";
import { isProjectContextInitialized } from "@/services";
import { logger } from "@/utils/logger";
⋮----
/**
 * Initialize project context if not already initialized
 * This includes NDK setup and ProjectManager initialization
 *
 * Used by commands that need full project context:
 * - tenex project run
 * - tenex debug chat
 * - tenex debug system-prompt
 * - tenex inventory generate
 */
export async function ensureProjectInitialized(projectPath: string): Promise<void>
⋮----
// Step 1: Initialize NDK connection
⋮----
// Step 2: Initialize ProjectContext using ProjectManager
</file>

<file path="src/utils/relays.ts">
/**
 * Default Nostr relay URLs for TENEX
 */
⋮----
/**
 * Get relay URLs for NDK connection
 */
export function getRelayUrls(): string[]
</file>

<file path="src/utils/repomix.ts">
import { randomUUID } from "node:crypto";
import { unlinkSync, readFileSync, existsSync } from "node:fs";
import { tmpdir } from "node:os";
import { join, resolve, relative } from "node:path";
import { runDefaultAction, type CliOptions } from "repomix";
import { logger } from "@/utils/logger";
⋮----
export interface RepomixResult {
    content: string;
    size: number;
    lines: number;
    cleanup: () => void;
}
⋮----
/**
 * Generate repository content using repomix
 * @param projectPath - The root path of the project
 * @param targetDirectory - Optional directory to analyze relative to projectPath
 */
export async function generateRepomixOutput(
    projectPath: string,
    targetDirectory?: string
): Promise<RepomixResult>
⋮----
// Resolve the target path
⋮----
// Validate that the target directory exists
⋮----
// Ensure the target is within the project path
⋮----
// Configure repomix options for XML output
⋮----
// Use the programmatic API to generate repomix output
⋮----
// Read the generated file
⋮----
// Clean up on error
⋮----
// Ignore cleanup errors
</file>

<file path="src/utils/setup.ts">
import { LLMConfigEditor } from "@/llm/LLMConfigEditor";
import { configService } from "@/services";
import type { TenexConfig } from "@/services/config/types";
import { logger } from "@/utils/logger";
import chalk from "chalk";
import inquirer from "inquirer";
⋮----
export async function runInteractiveSetup(): Promise<TenexConfig>
⋮----
// Load current configuration to check what's missing
⋮----
// Step 1: Get whitelisted pubkeys if needed
⋮----
// Step 2: Save basic configuration
⋮----
// Step 3: Set up LLM configurations if needed
⋮----
const llmEditor = new LLMConfigEditor("", true); // Global config
⋮----
async function promptForPubkeys(): Promise<string[]>
</file>

<file path="src/utils/string.ts">
export function toKebabCase(str: string): string
</file>

<file path="src/browser.ts">
// Browser-compatible exports for TENEX CLI package
// Only includes exports that work in browser environments
</file>

<file path="src/index.ts">
// Main exports for TENEX CLI package
</file>

<file path="src/tenex.ts">
import { logger } from "@/utils/logger";
// CLI entry point for TENEX
import { Command } from "commander";
import { agentCommand } from "./commands/agent/index";
import { daemonCommand } from "./commands/daemon";
import { runDebugSystemPrompt } from "./commands/debug/index";
import { inventoryCommand } from "./commands/inventory/index";
import { mcpCommand } from "./commands/mcp/index";
import { projectCommand } from "./commands/project/index";
import { setupCommand } from "./commands/setup/index";
import { initNDK } from "./nostr/ndkClient";
⋮----
// Add main commands
⋮----
// Add debug command
⋮----
// Initialize NDK before parsing commands
export async function main(): Promise<void>
⋮----
// Only run if called directly (not imported)
</file>

<file path="tests/e2e/agent-error-recovery.test.ts">
import { describe, it, expect, beforeEach, afterEach } from "bun:test";
import { 
    setupE2ETest, 
    cleanupE2ETest, 
    createConversation, 
    executeAgent,
    getConversationState,
    e2eAssertions,
    type E2ETestContext 
} from "./test-harness";
import { createMockLLMService } from "@/test-utils/mock-llm";
⋮----
// Error recovery scenarios
⋮----
// Initial chat phase
⋮----
// Plan phase with tool error
⋮----
// Recovery from tool error
⋮----
// Execute phase with shell error
⋮----
command: "false", // This command always exits with error
⋮----
// Recovery from shell error
⋮----
// Infinite loop detection scenario
⋮----
// Planner keeps suggesting PLAN phase (infinite loop)
⋮----
suggestedPhase: "PLAN" // Loop back to PLAN
⋮----
// Agent timeout scenario
⋮----
// Simulate timeout by providing error
⋮----
// Multi-agent error responses
⋮----
// Orchestrator error
⋮----
name: "invalidTool", // This tool doesn't exist
⋮----
// Recovery in orchestrator
⋮----
// Setup E2E test environment with routing decisions always included
⋮----
// Add error recovery scenarios to existing mock
⋮----
// Create conversation
⋮----
// Execute Orchestrator to start the workflow
⋮----
// Get conversation state
⋮----
// Verify workflow progressed despite errors
⋮----
// Execute Planner with tool error
⋮----
// Verify error recovery happened
⋮----
// Should have recovery responses triggered
⋮----
// Verify proper tool call sequence with recovery
⋮----
"continue",           // Initial routing
"generateInventory",  // Failed tool
"continue"           // Recovery action
⋮----
// Add infinite loop scenarios to existing mock
⋮----
// Create conversation
⋮----
// Execute workflow which should trigger loop detection
⋮----
// Verify proper handling of repeated calls
⋮----
// Should detect repetition and stop
⋮----
expect(continueCalls.length).toBeLessThan(10); // Should stop before too many
⋮----
// Add timeout scenarios to existing mock
⋮----
// Create conversation
⋮----
// Execute with simulated delay
⋮----
// Should have executed without throwing
⋮----
// Verify timeout response was triggered
⋮----
// Create complex scenario with multiple error types
⋮----
// Add verification failure scenario
⋮----
// Add complex error scenarios to existing mock
⋮----
// Create conversation
⋮----
// Execute multiple agents to trigger various errors
⋮----
// Should have multiple phase transitions
⋮----
// Verify error recovery across phases
⋮----
// Add multi-agent error scenarios to existing mock
⋮----
// Create conversation
⋮----
// Execute and verify error handling
⋮----
// Verify error handling across different agents
⋮----
// Should have handled errors from multiple agent types
</file>

<file path="tests/e2e/complete-tool-integration.test.ts">
import { describe, it, expect, beforeEach, afterEach, mock } from "bun:test";
import path from "path";
import { createTempDir, createMockLLMService } from "@/test-utils";
import { ConversationManager } from "@/conversations/ConversationManager";
import { AgentRegistry } from "@/agents/AgentRegistry";
import { AgentExecutor } from "@/agents/execution/AgentExecutor";
import { ConfigService } from "@/services/ConfigService";
import { EVENT_KINDS } from "@/llm/types";
import type { MockResponse } from "@/test-utils/mock-llm/types";
import type { ExecutionContext } from "@/agents/execution/types";
import { NostrPublisher } from "@/nostr/NostrPublisher";
import { EventMonitor } from "@/daemon/EventMonitor";
⋮----
// Mock file system
⋮----
// Create a custom scenario for complete tool testing
⋮----
// Create initial event
⋮----
// Create conversation
⋮----
// Create execution context for orchestrator
⋮----
// Execute orchestrator - should delegate to planner
⋮----
// Verify orchestrator delegated to planner
⋮----
// Verify phase transition
⋮----
// Create execution context for planner
⋮----
// Execute planner - should use complete tool
⋮----
// Verify planner used complete tool
⋮----
// Verify the complete tool was called
⋮----
// Execute orchestrator again - should receive completion
⋮----
// Verify orchestrator received and processed the completion
⋮----
// Create a scenario where agent uses complete without summary
⋮----
// Configure for this test
⋮----
// Create event for execution phase
⋮----
// Create conversation in EXECUTE phase
⋮----
conversation.phase = "EXECUTE"; // Manually set phase for this test
⋮----
// Execute executor agent
⋮----
// Verify complete was called without summary
</file>

<file path="tests/e2e/concurrency-multiple-conversations.test.ts">
import { describe, it, expect, beforeEach, afterEach } from "bun:test";
import {
    setupE2ETest,
    cleanupE2ETest,
    createConversation,
    executeAgent,
    getConversationState,
    type E2ETestContext
} from "./test-harness";
import { logger } from "@/utils/logger";
⋮----
// Step 1: Create two conversations with different requests
⋮----
// Verify both conversations are created
⋮----
// Step 2: Execute both conversations concurrently
⋮----
// Step 3: Continue executing both workflows in parallel
const workflowA = async () =>
⋮----
const workflowB = async () =>
⋮----
// Step 4: Verify both conversations reached expected phases
⋮----
// Verify phase transitions for User A
⋮----
// Verify phase transitions for User B
⋮----
// Step 5: Verify conversations didn't interfere with each other
⋮----
// Verify correct content isolation
⋮----
}, 30000); // Extended timeout for concurrent operations
⋮----
// Create three conversations to test resource contention
⋮----
// Execute all conversations concurrently
const executeConversation = async (conversationId: string, userLabel: string) =>
⋮----
// All should reach verification phase
⋮----
// Verify no cross-contamination between conversations
⋮----
// Should contain references to current user
⋮----
// Should not contain references to other users
⋮----
}, 45000); // Extended timeout for multiple concurrent operations
⋮----
// Create two conversations that will transition at similar times
⋮----
// Execute with minimal delays to simulate race conditions
const executeWithTiming = async (conversationId: string, label: string, delay: number) =>
⋮----
// Add small random delays to increase race condition likelihood
⋮----
// Execute both with minimal timing differences
⋮----
// Both should reach verification despite race conditions
⋮----
// Verify phase transition integrity
⋮----
// Each expected transition should occur exactly once
⋮----
// No duplicate or out-of-order transitions
</file>

<file path="tests/e2e/inventory-generation-simple.test.ts">
import { describe, it, expect, beforeEach, afterEach, mock } from "bun:test";
⋮----
import { createTempDir, cleanupTempDir } from "@/test-utils";
⋮----
// Mock the inventory generation utilities
⋮----
// Import the tool after mocking dependencies
import { generateInventoryTool } from "@/tools/implementations/generateInventory";
import type { ToolContext } from "@/tools/types";
⋮----
// Reset mock
⋮----
// Execute the tool
⋮----
// Verify successful execution
⋮----
// Verify inventory generation was called
⋮----
// Verify inventory file was created
⋮----
// Verify content
⋮----
// Pre-create an inventory
⋮----
// Execute the tool
⋮----
// Verify regeneration
⋮----
// Verify new content
</file>

<file path="tests/e2e/mcp-service-error-handling.test.ts">
import { describe, it, expect, beforeEach, afterEach, mock } from "bun:test";
import path from "node:path";
import { MCPService } from "@/services/mcp/MCPService";
import { configService } from "@/services/ConfigService";
import { createTempDir, cleanupTempDir } from "@/test-utils";
import type { TenexConfig } from "@/services/config/types";
⋮----
// Mock child_process to simulate various MCP server behaviors
⋮----
// Never yield anything to simulate timeout
await new Promise(() => {}); // Hang forever
⋮----
// Normal initialization
⋮----
// Tools list response
⋮----
// Track written data for verification
⋮----
: new Promise(() => {}) // Never resolves for normal operation
⋮----
// Mock logger to capture error logs
⋮----
// Reset logger calls
⋮----
// Create temp directory
⋮----
// Clear config cache
⋮----
// Get MCP service instance and reset its state
⋮----
// Force reinitialize by setting private property
⋮----
// Kill all mock processes
⋮----
// Clean up
⋮----
// Create config with server that will fail to start
⋮----
// Initialize should not throw
⋮----
// Should log the error
⋮----
// Service should still be marked as initialized (but with no tools)
⋮----
// Should handle the invalid JSON gracefully
⋮----
// Wait for the crash to be triggered
⋮----
// Should have logged the process error
⋮----
// Wait for the exit to be triggered
⋮----
// Should handle the exit gracefully
⋮----
// Good server should have provided its tools
⋮----
// Should have logged errors for the failing servers
⋮----
// Should log that MCP is disabled
⋮----
// No servers should have been started
⋮----
// Should return no tools
⋮----
// This would require more complex mocking of the tool execution flow
// For now, we've covered the main error paths in server initialization
</file>

<file path="tests/e2e/nostr-network-resilience.test.ts">
import { describe, it, expect, beforeEach, afterEach, mock } from "bun:test";
import path from "node:path";
import {
    setupE2ETest,
    cleanupE2ETest,
    createConversation,
    executeAgent,
    getConversationState,
    type E2ETestContext
} from "./test-harness";
import { logger } from "@/utils/logger";
import type { NDKEvent } from "@nostr-dev-kit/ndk";
⋮----
// Track network calls for verification
⋮----
// Mock state for controlling network behavior
⋮----
const MAX_FAILURES = 2; // For intermittent failures
⋮----
// Mock NDKEvent to simulate network conditions
⋮----
// Simulate timeout delay
⋮----
// Successful publish
⋮----
// Simulate network delay for realistic behavior
⋮----
// Set a fake ID to simulate successful publish
⋮----
// Set basic event properties
⋮----
// Mock NDK module
⋮----
constructor(ndk?: any, event?: any)
⋮----
tag(tag: string[]): void
⋮----
constructor(options?: any)
⋮----
async connect(): Promise<void>
⋮----
// Simulate connection delay
⋮----
// Helper functions to control failure modes
const setFailureMode = (mode: typeof failureMode) =>
⋮----
const resetFailureMode = () =>
⋮----
const clearNetworkCalls = () =>
⋮----
// Reset test state
⋮----
// Create conversation
⋮----
// Set network to fail all publish attempts
⋮----
// Execute agent - should handle network failures gracefully
⋮----
// Verify conversation state was still updated despite network failures
⋮----
// Verify network failures were attempted
⋮----
// Verify agent response was saved locally even though publish failed
⋮----
// Set network to fail intermittently (first 2 attempts fail, then succeed)
⋮----
// Execute multiple agent interactions
⋮----
// Continue to planning phase
⋮----
// Verify recovery from failures
⋮----
expect(failures.length).toBe(2); // First 2 attempts failed
expect(successes.length).toBeGreaterThan(0); // Subsequent attempts succeeded
⋮----
// Verify conversation progressed despite intermittent failures
⋮----
// Set network to timeout
⋮----
// Execute agent with timeout scenario
⋮----
// Should not hang indefinitely despite timeouts
expect(duration).toBeLessThan(10000); // Should complete within 10 seconds
⋮----
// Verify timeout errors were encountered
⋮----
// Verify conversation state is still consistent
⋮----
// Clear network state
⋮----
// Execute initial interaction successfully
⋮----
// Now set network to fail
⋮----
// Continue execution with network failures
⋮----
// Reset network to working
⋮----
// Continue execution
⋮----
// Verify message ordering is maintained
⋮----
// Messages should be in chronological order
⋮----
// Verify no duplicate messages
⋮----
// Track typing indicator events specifically
⋮----
// Execute with normal network
⋮----
// Check for typing indicator events
⋮----
// Now test with failing typing indicators
⋮----
// Main flow should continue despite typing indicator failures
⋮----
// Create multiple conversations
⋮----
// Set intermittent failures
⋮----
// Execute agents concurrently
⋮----
// Verify both conversations progressed despite network issues
⋮----
// Verify network calls show recovery pattern
⋮----
// Verify no cross-contamination between conversations
</file>

<file path="tests/e2e/orchestrator-simple.test.ts">
import { describe, it, expect, beforeEach, afterEach } from "bun:test";
import { setupE2ETest, cleanupE2ETest, executeAgent, createConversation, getConversationState, type E2ETestContext } from "./test-harness";
⋮----
// Create conversation
⋮----
// Execute orchestrator
⋮----
// Get conversation state
⋮----
// Verify routing happened
⋮----
// Check mock LLM history
⋮----
// Verify the routing decision was made correctly
⋮----
// Parse the routing decision
⋮----
// Test infinite loop scenario
⋮----
// Test timeout scenario
⋮----
// Verify both routing decisions were made
⋮----
// Verify routing decisions
</file>

<file path="tests/e2e/orchestrator-workflow.test.ts">
import { describe, it, expect, beforeEach, afterEach } from "bun:test";
import {
    setupE2ETest,
    cleanupE2ETest,
    createConversation,
    executeAgent,
    getConversationState,
    waitForPhase,
    e2eAssertions,
    type E2ETestContext
} from "./test-harness";
import { logger } from "@/utils/logger";
⋮----
// Step 1: Create conversation with initial request
⋮----
// Step 2: Execute Orchestrator to handle initial request
⋮----
// Verify Orchestrator decided to continue in CHAT phase
⋮----
// Step 3: Execute assigned agent (Executor in CHAT phase)
⋮----
// Step 4: Orchestrator should transition to PLAN phase
⋮----
// Step 5: Execute Planner
⋮----
// Step 6: Transition to EXECUTE phase
⋮----
// Step 7: Execute implementation
⋮----
// Step 8: Run tests/verification
⋮----
// Step 9: Transition to VERIFICATION
⋮----
// Step 10: Complete verification
⋮----
// Final assertions
⋮----
// Verify complete tool sequence
⋮----
// Check metrics
⋮----
}, 30000); // 30 second timeout for full workflow
⋮----
// Create conversation starting in PLAN phase
⋮----
// Manually set to PLAN phase for this test
⋮----
// Execute Planner with a plan that needs review
⋮----
// Orchestrator should evaluate the plan
⋮----
// Verify orchestrator made a routing decision
</file>

<file path="tests/e2e/performance-execution-timeout.test.ts">
import { describe, it, expect, beforeEach, afterEach } from "bun:test";
import { 
    setupE2ETest, 
    cleanupE2ETest, 
    createConversation,
    executeAgent,
    getConversationState,
    type E2ETestContext 
} from "./test-harness";
import type { MockLLMService } from "@/test-utils";
⋮----
/**
 * E2E Tests for Execution Timeout Handling
 * 
 * Tests the system's ability to handle timeouts at the execution layer:
 * - Agent execution timeouts
 * - Recovery from timed-out operations
 * - Proper error propagation and state management
 * - Prevention of zombie executions
 */
⋮----
// Setup with performance testing scenario
⋮----
// Create a conversation with timeout test request
⋮----
// Add a response with extreme delay to simulate timeout
⋮----
streamDelay: 60000, // 60 seconds - simulates a timeout scenario
⋮----
// Execute agent - in real scenario this would timeout
// For now we test that the delay is configured
⋮----
// In real implementation, timeout would throw
⋮----
// Verify the mock was configured with long delay
⋮----
// The response should have the extreme delay configured
⋮----
// Create multiple conversations
⋮----
// Configure different response delays
⋮----
// Execute all conversations concurrently
⋮----
// Verify execution times match configured delays
⋮----
// Verify conversation states progressed
⋮----
// Track memory before execution
⋮----
// Configure large response with delay
⋮----
content: "x".repeat(100000) // 100KB response
⋮----
// Check memory after execution
⋮----
// Memory increase should be reasonable
expect(memIncrease).toBeLessThan(10 * 1024 * 1024); // Less than 10MB
⋮----
// Verify large response was handled
⋮----
// First add a very slow response
⋮----
streamDelay: 30000, // 30 second delay
⋮----
// Also add a fast recovery response with higher priority
⋮----
// Should use the fast response due to higher priority
⋮----
// Verify recovery succeeded
⋮----
// Configure responses with various delays
⋮----
// Execute multiple requests
⋮----
// Verify timing matches configured delay
⋮----
// Check request history
⋮----
// Move to EXECUTE phase to trigger the 35-second timeout scenario
⋮----
// Execute with the timeout test trigger
⋮----
// The scenario has a 35-second delay configured
⋮----
// Verify the timeout scenario was triggered
</file>

<file path="tests/e2e/performance-simple.test.ts">
import { describe, it, expect } from "bun:test";
import { createMockLLMService } from "@/test-utils/mock-llm";
⋮----
/**
 * Simple E2E test to verify performance testing capabilities
 * This demonstrates that the E2E framework supports performance testing with delays
 */
⋮----
// Create mock LLM with performance testing scenario
⋮----
// Add a simple delayed response
⋮----
streamDelay: 2000, // 2 second delay
⋮----
// Make a request
⋮----
// Verify delay was applied
⋮----
// Add response with very long delay
⋮----
streamDelay: 10000, // 10 second delay
⋮----
// In a real scenario, this would be handled by the execution layer
// For now, we just verify the delay is configured
⋮----
// Add responses with different delays
⋮----
// Execute requests concurrently
⋮----
// All should complete within the slowest request time
⋮----
// This test documents how to use performance testing scenarios
⋮----
// The performance-testing scenario includes:
// - Slow orchestrator responses (5s delay)
// - Very slow planning phase (8s delay)
// - Timeout simulation (35s delay)
// - Memory-intensive responses
// - Recovery after timeout scenarios
⋮----
// Verify scenario was loaded
⋮----
// Find a slow response scenario
</file>

<file path="tests/e2e/performance-timeout.test.ts">
import { describe, it, expect, beforeEach, afterEach } from "bun:test";
import { 
    setupE2ETest, 
    cleanupE2ETest, 
    createConversation,
    executeAgent,
    getConversationState,
    type E2ETestContext 
} from "./test-harness";
import type { MockLLMService } from "@/test-utils";
⋮----
/**
 * E2E Tests for Performance and Timeout Handling
 * 
 * Tests system behavior under performance stress:
 * - Slow LLM responses
 * - Timeout handling
 * - Recovery after timeouts
 * - Large response handling
 */
⋮----
// Setup with performance testing scenario
⋮----
// Create a conversation with performance test request
⋮----
// Execute with slow response scenario
⋮----
// Verify execution took at least 5 seconds (due to streamDelay)
⋮----
// Verify conversation progressed despite delay
⋮----
// Check request history
⋮----
// Create conversation for planning phase test
⋮----
// Execute orchestrator to move to PLAN phase
⋮----
// Now execute planning phase with delay
⋮----
// Update conversation to trigger planning phase scenario
⋮----
// Verify planning took at least 8 seconds
⋮----
// Verify phase progression
⋮----
// Move to EXECUTE phase
⋮----
// Track memory before execution
⋮----
// Track memory after execution
⋮----
// Verify memory increase is reasonable (less than 10MB)
⋮----
// Verify large response was handled
⋮----
// Verify quick recovery
⋮----
// Verify conversation moved to verification
⋮----
// Execute multiple rapid requests concurrently
⋮----
// All should complete without errors
⋮----
// Verify all requests were processed
⋮----
// Verify execution time tracking
⋮----
// Check if execution metrics were stored
⋮----
// Verify conversation has timestamp metadata
⋮----
// Move to EXECUTE phase to trigger timeout scenario
⋮----
// Execute with a shorter timeout than the mock response delay
⋮----
// Note: Real timeout handling would happen at the LLM service level
// This test verifies the mock scenario is configured correctly
⋮----
// Since our mock doesn't actually enforce timeouts,
// we verify the delay was configured correctly
⋮----
// The streamDelay should be 35000ms as configured
</file>

<file path="tests/e2e/simple-flow.test.ts">
import { describe, it, expect, beforeEach, afterEach } from "bun:test";
import { setupE2ETest, cleanupE2ETest, type E2ETestContext } from "./test-harness";
import { createMockLLMService } from "@/test-utils";
import type { MockLLMResponse } from "@/test-utils/mock-llm/types";
⋮----
// Add a custom response
⋮----
// Create a custom mock LLM with a default response
⋮----
// Execute through the complete method
⋮----
// Add a response with tool calls
⋮----
// Load orchestrator workflow scenario
⋮----
// Test orchestrator response
⋮----
// Verify tool call arguments
⋮----
// Make multiple requests
⋮----
// Check history
</file>

<file path="tests/e2e/state-persistence.test.ts">
import { describe, it, expect, beforeEach, afterEach } from "bun:test";
import {
    setupE2ETest,
    cleanupE2ETest,
    createConversation,
    executeAgent,
    getConversationState,
    waitForPhase,
    e2eAssertions,
    type E2ETestContext
} from "./test-harness";
import { ConversationManager } from "@/conversations/ConversationManager";
import { AgentRegistry } from "@/agents/AgentRegistry";
import { FileSystemAdapter } from "@/conversations/persistence/FileSystemAdapter";
import path from "path";
⋮----
// Step 1: Create conversation and execute initial workflow
⋮----
// Execute orchestrator to start the workflow
⋮----
// Wait for phase transition to PLAN
⋮----
// Continue to BUILD phase
⋮----
// Wait for BUILD phase
⋮----
// Execute Test Agent in BUILD phase
⋮----
// Step 2: Verify state before "crash"
⋮----
// Verify phase transitions
⋮----
// Step 3: Simulate system restart by creating new instances
⋮----
// Step 4: Load conversation from persistence
⋮----
// Verify recovered agent contexts
⋮----
// Step 5: Continue workflow after recovery
⋮----
// Continue with the recovered conversation
⋮----
// Verify completion
⋮----
// Verify tool call sequence includes completion
⋮----
// Create multiple conversations
⋮----
// Execute initial phases for all conversations
⋮----
// Wait for all to reach PLAN phase
⋮----
// Verify all conversations are persisted
⋮----
// Should have one file per conversation
⋮----
// Verify each conversation file exists and contains valid data
⋮----
// Simulate restart and recover all conversations
⋮----
// Load and verify all conversations
⋮----
// Create conversation
⋮----
// Start execution
⋮----
// Wait a moment to accumulate execution time
⋮----
// Get state before restart
⋮----
// Simulate restart
⋮----
// Load conversation
⋮----
// Verify metrics are preserved
⋮----
// Continue execution
⋮----
// Verify metrics continue to accumulate
⋮----
// Create conversation
⋮----
// Execute initial phase
⋮----
// Make persistence directory read-only to simulate write error
⋮----
// Note: This test is simplified because filesystem permissions are complex
// In a real scenario, we would test various failure modes:
// - Disk full
// - Permission denied
// - Corrupted JSON files
// - Network failures for remote persistence
⋮----
// Instead, we'll test recovery from corrupted data
⋮----
// Corrupt the persisted file
⋮----
// Attempt to load with new manager
⋮----
// Should handle corrupted file gracefully
⋮----
// The current implementation might return null or throw
// This test verifies the system doesn't crash completely
⋮----
// Expected behavior - unable to recover from corrupted data
⋮----
// If recovery succeeded, verify it's in a valid state
⋮----
// Create conversation
⋮----
// Execute multiple interactions
⋮----
// Small delay to ensure distinct timestamps
⋮----
// Get state before restart
⋮----
// Simulate restart
⋮----
// Load conversation
⋮----
// Verify history is preserved in correct order
⋮----
// Check that history entries match
⋮----
// Tool calls should also match if present
</file>

<file path="tests/e2e/state-recovery.test.ts">
import { describe, it, expect, beforeEach, afterEach, mock } from "bun:test";
import { ConversationManager } from "@/conversations/ConversationManager";
import { AgentExecutor } from "@/agents/execution/AgentExecutor";
import { TestPersistenceAdapter } from "@/test-utils/test-persistence-adapter";
import { createMockLLMService } from "@/test-utils/mock-llm";
import { createMockNDKEvent, createMockAgent } from "@/test-utils/mock-factories";
import { EVENT_KINDS } from "@/llm/types";
import { logger } from "@/utils/logger";
import fs from "fs/promises";
import path from "path";
import os from "os";
⋮----
/**
 * E2E Tests for State Recovery and Persistence
 * 
 * These tests validate that TENEX can properly persist conversation state
 * and recover from interruptions, ensuring system reliability.
 */
⋮----
// Create temporary test directory
⋮----
// Setup test persistence adapter
⋮----
// Create mock LLM service with state persistence scenarios
⋮----
// Mock Nostr publisher to prevent network calls
⋮----
cleanup()
async publishResponse()
async publishError()
async publishTypingIndicator()
async stopTypingIndicator()
⋮----
// Mock AgentPublisher to prevent publishing during tests
⋮----
constructor()
initialize()
publishAgent()
⋮----
// Mock project context to avoid complex initialization
⋮----
// Mock LLM router
⋮----
getService()
validateModel()
⋮----
// Clean up test directory
⋮----
// Clear test persistence
⋮----
// Create conversation manager with test persistence
⋮----
// Create initial event
⋮----
// Create conversation
⋮----
// Verify initial state is persisted
⋮----
// Skip agent execution test for now - focus on persistence
// This test verifies that conversation state is properly saved
⋮----
// Manually update conversation state to simulate agent execution
⋮----
// Save the updated state
⋮----
// Verify state after first execution
⋮----
// Create first conversation manager instance
⋮----
// Create and execute initial conversation
⋮----
// Add some agent context
⋮----
// Update phase
⋮----
// Update the conversation object in memory to set execution time
⋮----
// Update title
⋮----
// Save the updated conversation
⋮----
// Simulate system restart - create new conversation manager
⋮----
// Load conversation from persistence
⋮----
// Verify all state is recovered correctly
⋮----
// Create conversation
⋮----
// Simulate partial agent execution
⋮----
// Mark conversation as in PLAN phase but incomplete
⋮----
// Add partial history
⋮----
// Save incomplete state
⋮----
// Simulate recovery - create new manager and load conversation
⋮----
expect(recovered?.history).toHaveLength(2); // Initial + partial response
⋮----
// Verify we can work with the recovered state
⋮----
// We should be able to add more context to the recovered conversation
⋮----
// Create conversation
⋮----
// First ensure we can do a simple update
⋮----
// Verify the simple update worked
⋮----
// Now test concurrent updates
⋮----
// Execute updates concurrently
⋮----
// Verify final state
⋮----
// We should have both agent contexts
⋮----
// Original history entry (initial user message) + at least one concurrent update
⋮----
// Create multiple conversations
⋮----
// Archive one conversation
⋮----
// List active conversations
⋮----
// Search for archived conversations
⋮----
// Restore archived conversation
⋮----
// Verify restoration
</file>

<file path="tests/e2e/test-harness.ts">
import { mock } from "bun:test";
import path from "node:path";
import { 
    createTempDir, 
    cleanupTempDir, 
    createMockLLMService,
    createMockNDKEvent,
    type MockLLMService
} from "@/test-utils";
import { TestPersistenceAdapter } from "@/test-utils/test-persistence-adapter";
import { ConversationManager } from "@/conversations/ConversationManager";
import { AgentRegistry } from "@/agents/AgentRegistry";
import { AgentExecutor } from "@/agents/execution/AgentExecutor";
import { RoutingBackend } from "@/agents/execution/RoutingBackend";
import type { ExecutionContext } from "@/agents/execution/types";
import { ConfigService } from "@/services/ConfigService";
import { EVENT_KINDS } from "@/llm/types";
import { logger } from "@/utils/logger";
⋮----
export interface E2ETestContext {
    projectPath: string;
    tempDir: string;
    mockLLM: MockLLMService;
    conversationManager: ConversationManager;
    agentRegistry: AgentRegistry;
    configService: typeof ConfigService;
    services: {
        configService: typeof ConfigService;
        projectContext: any;
    };
    projectConfig: any;
}
⋮----
/**
 * Setup E2E test environment
 */
export async function setupE2ETest(scenarios: string[] = []): Promise<E2ETestContext>
⋮----
// Create temp directory
⋮----
// Mock file system
⋮----
// Initialize mock LLM
⋮----
// Mock LLM router
⋮----
constructor()
getService()
validateModel()
⋮----
// Mock Nostr publisher
⋮----
async publishResponse()
async publishError()
async publishTypingIndicator()
async stopTypingIndicator()
⋮----
// Mock AgentPublisher to prevent publishing during tests
⋮----
async publishProfile()
async publishEvents()
async publishAgentCreation()
⋮----
// Mock logging
⋮----
logToolCall()
logToolResult()
logStream()
logComplete()
logError()
logEvent()
routingDecision()
agentThinking()
⋮----
// Mock tracing
⋮----
getRequest()
getConversation()
getAgent()
addConversation()
addAgent()
removeAgent()
⋮----
// Create test agent
⋮----
// Mock project context to avoid complex initialization
⋮----
// Initialize services with test persistence adapter
⋮----
// Ensure built-in agents are loaded
⋮----
// Get orchestrator for testing
⋮----
/**
 * Cleanup E2E test environment
 */
export async function cleanupE2ETest(context: E2ETestContext | undefined): Promise<void>
⋮----
/**
 * Create and execute an agent
 */
export async function executeAgent(
    context: E2ETestContext,
    agentName: string,
    conversationId: string,
    userMessage: string,
    options: {
onStreamContent?: (content: string)
⋮----
// Try both the provided name and lowercase version
⋮----
// Create a mock NDK event for the triggering event
⋮----
// Create mock publisher
⋮----
// Create AgentExecutor first for use in ExecutionContext
⋮----
agentExecutor: undefined, // Will be set below
⋮----
// Create AgentExecutor with the context
⋮----
// Set the agentExecutor in the context
⋮----
// Use appropriate backend based on agent
⋮----
/**
 * Create a conversation from a user message
 */
export async function createConversation(
    context: E2ETestContext,
    title: string,
    content: string,
    tags: string[][] = []
): Promise<string>
⋮----
/**
 * Helper to get conversation state
 */
export async function getConversationState(
    context: E2ETestContext,
    conversationId: string
)
⋮----
/**
 * Wait for a specific phase
 */
export async function waitForPhase(
    context: E2ETestContext,
    conversationId: string,
    expectedPhase: string,
    timeout = 5000
): Promise<void>
⋮----
/**
 * Extract tool calls from mock LLM history
 */
export function getToolCallsFromHistory(mockLLM: MockLLMService): string[]
⋮----
/**
 * Custom assertions for E2E tests
 */
⋮----
toHavePhaseTransition(
        transitions: any[],
        from: string,
        to: string
): void
⋮----
toHaveToolCallSequence(
        mockLLM: MockLLMService,
        expectedSequence: string[]
): void
⋮----
// Check if expected sequence appears in order (not necessarily consecutive)
⋮----
return; // Found complete sequence
</file>

<file path=".gitignore">
# Dependencies
node_modules/
.pnp
.pnp.js

# Build outputs
dist/
build/
out/
.next/
.nuxt/
.cache/

# Testing
coverage/
.nyc_output/
test-results/
playwright-report/
playwright/.cache/

# Environment variables
.env
.env.local
.env.development.local
.env.test.local
.env.production.local

# IDE
.vscode/
.idea/
*.swp
*.swo
*~
.DS_Store

# Logs
logs/
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*
lerna-debug.log*
.pnpm-debug.log*

# Runtime data
pids
*.pid
*.seed
*.pid.lock

# Temporary files
tmp/
temp/
.tmp/

# OS generated files
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# Project specific
.tenex/
llms.json
agents.json
.wallet.json
*.bak
*.tmp

# Test artifacts
test-*.json
test-*.txt
__snapshots__/.repomix-output.txt
</file>

<file path=".npmignore">
# Source files
src/
bin/tenex.ts

# Configuration files
tsconfig.json
tsconfig.build.json
.eslintrc*
.prettierrc*
biome.json

# Development files
scripts/
tests/
**/*.test.ts
**/*.spec.ts
**/__tests__/

# Build artifacts
*.tsbuildinfo

# Documentation
*.md
!README.md
!LICENSE

# Git
.git/
.gitignore

# Dependencies
node_modules/

# IDE
.vscode/
.idea/

# OS files
.DS_Store
Thumbs.db

# Logs
*.log
npm-debug.log*

# Coverage
coverage/
.nyc_output/

# Local config
.env
.env.local
.env.*.local

# Temporary files
tmp/
temp/
</file>

<file path="biome.json">
{
    "$schema": "https://biomejs.dev/schemas/1.9.4/schema.json",
    "files": {
        "ignore": [
            ".tenex/**",
            "node_modules/**",
            "dist/**",
            "build/**",
            "*.min.js",
            "*.bundle.js",
            "**/*.test.js",
            "**/*.test.ts",
            "**/*.test.jsx",
            "**/*.test.tsx",
            "**/*.spec.js",
            "**/*.spec.ts",
            "**/*.spec.jsx",
            "**/*.spec.tsx",
            "**/tests/**",
            "**/test/**",
            "**/__tests__/**",
            "**/__test__/**"
        ]
    },
    "formatter": {
        "enabled": true,
        "formatWithErrors": false,
        "indentStyle": "space",
        "indentWidth": 2,
        "lineWidth": 100
    },
    "linter": {
        "enabled": true,
        "rules": {
            "recommended": true
        }
    },
    "organizeImports": {
        "enabled": true
    },
    "javascript": {
        "formatter": {
            "quoteStyle": "double",
            "trailingCommas": "es5",
            "semicolons": "always"
        }
    }
}
</file>

<file path="bunfig.toml">
# Bun configuration file

[test]
# Enable coverage reporting
coverage = true
coverageDirectory = "./coverage"
coverageReporter = ["text", "lcov"]

# Test file patterns
# Include all test files
root = "./src"

# Timeout for tests (in milliseconds)
timeout = 30000

# Run tests in watch mode by default in development
# watchMode = true

[install]
# Lockfile configuration
lockfile = { save = true }

# Auto-install peer dependencies
peer = true

[run]
# Enable source maps
smol = true
</file>

<file path="E2E_FRAMEWORK.md">
# TENEX E2E Testing Framework

## Overview

This document describes the End-to-End (E2E) testing framework for TENEX, which enables comprehensive testing of agent workflows without relying on actual LLM services. The framework uses deterministic mock responses to simulate complete agent interactions.

## Architecture

### Core Components

1. **Mock LLM Service** - Simulates LLM responses based on predefined scenarios
2. **Test Harness** - Sets up the test environment and manages test lifecycle
3. **Scenario Definitions** - Predefined agent conversation flows
4. **Assertion Utilities** - Custom assertions for agent-specific validations

### Key Design Decisions

1. **Deterministic Responses**: All LLM responses are predefined based on triggers (agent name, phase, message content, previous tools)
2. **Real Component Integration**: We use real ConversationManager, AgentExecutor, and tool implementations - only the LLM is mocked
3. **Scenario-Based Testing**: Complex workflows are defined as reusable scenarios
4. **Phase-Aware Testing**: Tests validate proper phase transitions throughout the workflow

## Test Structure

### 1. Setup Phase
```typescript
// Initialize test environment
const testDir = await createTempDir();
const projectPath = path.join(testDir, "test-project");

// Create mock LLM with scenarios
const mockLLM = createMockLLMService(['orchestrator-workflow']);

// Initialize real components
const conversationManager = new ConversationManager(projectPath);
const agentRegistry = new AgentRegistry(projectPath);
```

### 2. Execution Phase
```typescript
// Create conversation
const conversation = await conversationManager.createConversation(event);

// Execute agent
const executor = new AgentExecutor(executionContext);
await executor.execute();
```

### 3. Validation Phase
```typescript
// Verify phase transitions
expect(conversation.phase).toBe("VERIFICATION");
expect(conversation.phaseTransitions).toHaveLength(3);

// Verify tool calls
const history = mockLLM.getRequestHistory();
expect(history).toContainToolCall("continue");
```

## Scenario Development

### Anatomy of a Scenario

Each scenario consists of:
1. **Triggers** - Conditions that must match for the response to be used
2. **Responses** - The content and tool calls to return
3. **Priority** - Higher priority responses are checked first

Example:
```typescript
{
    trigger: {
        agentName: "Orchestrator",
        phase: "CHAT",
        userMessage: /create.*authentication/i
    },
    response: {
        content: "I'll help you create an authentication system.",
        toolCalls: [{
            id: "1",
            type: "function",
            function: {
                name: "continue",
                arguments: JSON.stringify({
                    summary: "Creating auth system",
                    suggestedPhase: "PLAN"
                })
            }
        }]
    },
    priority: 10
}
```

## Test Patterns

### 1. Full Workflow Test
Tests complete agent workflow from initial request to completion.

### 2. Error Recovery Test
Validates system behavior when tools fail or return errors.

### 3. Phase Transition Test
Ensures correct phase transitions and state management.

### 4. Multi-Agent Collaboration Test
Tests handoffs between different agents.

## Running Tests

```bash
# Run all E2E tests
bun test tests/e2e

# Run specific test
bun test ./tests/e2e/orchestrator-workflow.test.ts

# Run with debug output
DEBUG=true bun test ./tests/e2e/orchestrator-workflow.test.ts

# Run with extended timeout
bun test ./tests/e2e/orchestrator-workflow.test.ts --timeout 30000
```

## Best Practices

1. **Isolation**: Each test should create its own temporary directory
2. **Cleanup**: Always cleanup temp directories and reset singletons
3. **Determinism**: Avoid randomness in tests - use fixed IDs and timestamps
4. **Debugging**: Use `mockLLM.getRequestHistory()` to debug test failures
5. **Scenarios**: Reuse scenarios across tests for consistency

## Extending the Framework

### Adding New Scenarios

1. Create a new scenario file in `src/test-utils/mock-llm/scenarios/`
2. Define triggers and responses for your workflow
3. Export the scenario and add to `allScenarios`

### Adding Custom Assertions

1. Add assertion functions to `src/test-utils/assertions.ts`
2. Focus on agent-specific validations (phase transitions, tool sequences, etc.)

## Troubleshooting

### Common Issues

1. **"No matching response found"** - Check trigger conditions match exactly
2. **Phase mismatch** - Ensure conversation phase is updated correctly
3. **Tool validation errors** - Verify tool arguments are valid JSON

### Debug Mode

Enable debug logging:
```typescript
const mockLLM = createMockLLMService(['scenario'], {
    debug: true // Logs all trigger matching attempts
});
```

## Lessons Learned

### Module Mocking Challenges

1. **Import Order Matters** - Mocks must be defined before importing modules that use them
2. **Cached Modules** - Bun caches imported modules, making it difficult to mock dependencies after import
3. **Deep Dependencies** - Some modules have deep dependency chains that require extensive mocking

### Successful Patterns

1. **Mock LLM Service** - Works well for deterministic testing of agent behaviors
2. **Scenario-Based Testing** - Predefined response sequences effectively test complex workflows
3. **Tool Call Verification** - Tracking tool calls provides good validation of agent decisions

### Recommendations

1. **Start Simple** - Begin with unit tests for individual components before full E2E
2. **Mock at Boundaries** - Mock external services (LLM, Nostr, filesystem) not internal logic
3. **Use Real Components** - When possible, use real implementations with mocked I/O

## Future Enhancements

1. **Performance Testing** - Add scenarios with delays to test timeout handling
2. **Concurrency Testing** - Test multiple simultaneous conversations
3. **State Persistence** - Test conversation recovery after restart
4. **Real Nostr Integration** - Optional tests with local Nostr relay
5. **Test Harness Improvements** - Better module isolation and mock management
</file>

<file path="eslint.config.js">
export default typescript.config(
</file>

<file path="find_orphaned_files.sh">
#!/bin/bash

echo "Finding orphaned TypeScript/JavaScript files in src/"
echo "===================================================="
echo

# Function to check if a file is imported
check_file_imports() {
    local file=$1
    local filename=$(basename "$file")
    local dirname=$(dirname "$file")
    local basename_no_ext=${filename%.*}
    
    # For index.ts files, check if the directory is imported
    if [[ "$filename" == "index.ts" ]] || [[ "$filename" == "index.js" ]]; then
        # Get the parent directory name
        local parent_dir=$(basename "$dirname")
        # Check if the directory itself is imported
        local dir_imports=$(rg -c "from ['\"](\.\./|@/|src/).*$parent_dir['\"]" src/ 2>/dev/null | wc -l)
        if [ $dir_imports -gt 0 ]; then
            return 1  # Directory is imported, so index.ts is used
        fi
    fi
    
    # Skip checking for imports in the file itself
    local exclude_pattern="^$file:"
    
    # Various import patterns to check
    local patterns=(
        # Relative imports from parent directories
        "\.\./[^'\"]*$basename_no_ext['\"]"
        "\.\./[^'\"]*$basename_no_ext\.js['\"]"
        # Relative imports from same directory
        "\./$basename_no_ext['\"]"
        "\./$basename_no_ext\.js['\"]"
        # Absolute imports using @/
        "@/${file#src/}"
        "@/[^'\"]*$basename_no_ext['\"]"
        # Module path imports
        "${file#src/}"
        "${file#src/}\.js"
    )
    
    local total_imports=0
    
    for pattern in "${patterns[@]}"; do
        local count=$(rg -c "$pattern" src/ 2>/dev/null | grep -v "$exclude_pattern" | wc -l)
        total_imports=$((total_imports + count))
    done
    
    # Also check for barrel exports (index.ts files that might export this file)
    if [ -f "$dirname/index.ts" ] && [[ "$file" != "$dirname/index.ts" ]]; then
        local index_exports=$(rg -c "from ['\"]\./$basename_no_ext" "$dirname/index.ts" 2>/dev/null || echo 0)
        total_imports=$((total_imports + index_exports))
    fi
    
    return $total_imports
}

# Get all source files (excluding tests)
files=$(find src -type f \( -name "*.ts" -o -name "*.tsx" -o -name "*.js" -o -name "*.jsx" \) | grep -v "__tests__" | grep -v "\.test\." | grep -v "\.spec\." | sort)

orphaned_files=()

for file in $files; do
    # Skip entry points and common config files
    if [[ "$file" == "src/tenex.ts" ]] || 
       [[ "$file" == "src/index.ts" ]] ||
       [[ "$file" == "src/types.ts" ]] ||
       [[ "$file" == *"/types.ts" ]] ||
       [[ "$file" == *"/constants.ts" ]] ||
       [[ "$file" == *"vite.config.ts" ]] ||
       [[ "$file" == *"jest.config.ts" ]]; then
        continue
    fi
    
    # Skip declaration files
    if [[ "$file" == *".d.ts" ]]; then
        continue
    fi
    
    check_file_imports "$file"
    import_count=$?
    
    if [ $import_count -eq 0 ]; then
        orphaned_files+=("$file")
    fi
done

# Display results
if [ ${#orphaned_files[@]} -eq 0 ]; then
    echo "No orphaned files found!"
else
    echo "Found ${#orphaned_files[@]} potentially orphaned files:"
    echo
    
    # Group files by type
    index_files=()
    regular_files=()
    
    for file in "${orphaned_files[@]}"; do
        if [[ "$(basename "$file")" == "index.ts" ]] || [[ "$(basename "$file")" == "index.js" ]]; then
            index_files+=("$file")
        else
            regular_files+=("$file")
        fi
    done
    
    # Display regular files first
    if [ ${#regular_files[@]} -gt 0 ]; then
        echo "Regular files:"
        echo "--------------"
        for file in "${regular_files[@]}"; do
            echo "📄 $file"
            
            # Show file info
            echo "   Size: $(wc -c < "$file" | xargs) bytes"
            echo "   Lines: $(wc -l < "$file" | xargs)"
            
            # Show exports
            exports=$(rg "^export" "$file" 2>/dev/null | head -3)
            if [ -n "$exports" ]; then
                echo "   Exports:"
                echo "$exports" | sed 's/^/     - /'
            fi
            
            # Show any TODO or DEPRECATED comments
            todos=$(rg "(TODO|DEPRECATED|FIXME)" "$file" 2>/dev/null | head -2)
            if [ -n "$todos" ]; then
                echo "   Notes:"
                echo "$todos" | sed 's/^/     - /'
            fi
            
            echo
        done
    fi
    
    # Display index files separately
    if [ ${#index_files[@]} -gt 0 ]; then
        echo "Index files (potentially unused module entry points):"
        echo "----------------------------------------------------"
        for file in "${index_files[@]}"; do
            echo "📁 $file"
            echo "   Directory: $(dirname "$file")"
            echo "   Exports: $(rg -c "^export" "$file" 2>/dev/null || echo 0) items"
            echo
        done
    fi
fi

# Additional check for files that might be CLI tools or scripts
echo
echo "Additional analysis:"
echo "-------------------"
if [ ${#orphaned_files[@]} -gt 0 ]; then
    standalone_scripts=()
    
    for file in "${orphaned_files[@]}"; do
        # Check if file has a shebang or main execution block
        if head -1 "$file" | grep -q "^#!" || rg -q "if \(__name__ ==|if \(import\.meta\.main\)" "$file"; then
            standalone_scripts+=("$file")
        fi
    done
    
    if [ ${#standalone_scripts[@]} -gt 0 ]; then
        echo "Potential standalone scripts:"
        for file in "${standalone_scripts[@]}"; do
            echo "🔧 $file"
        done
    else
        echo "No standalone scripts detected."
    fi
else
    echo "No files to analyze."
fi
</file>

<file path="package.json">
{
    "name": "@tenex/cli",
    "version": "0.1.0",
    "description": "TENEX Command Line Interface",
    "main": "./dist/index.js",
    "types": "./dist/index.d.ts",
    "exports": {
        ".": {
            "import": "./dist/index.js",
            "types": "./dist/index.d.ts"
        },
        "./events": {
            "import": "./dist/browser.js",
            "types": "./dist/browser.d.ts"
        }
    },
    "bin": {
        "tenex": "src/tenex.ts"
    },
    "files": [
        "dist",
        "src"
    ],
    "scripts": {
        "start": "bun run ./src/tenex.ts",
        "build": "bun scripts/build-bundled.js",
        "prepublishOnly": "bun run build",
        "test": "bun test",
        "test:watch": "bun test --watch",
        "test:coverage": "bun test --coverage",
        "test:unit": "bun test src/**/__tests__/*.test.ts",
        "test:integration": "bun test src/**/__tests__/*.integration.test.ts",
        "test:e2e": "bun test tests/e2e/*.test.ts",
        "typecheck": "tsc --noEmit",
        "lint": "eslint src --ext .ts,.tsx",
        "lint:fix": "eslint src --ext .ts,.tsx --fix",
        "clean": "rm -rf dist node_modules bun.lockb coverage",
        "llm-log": "bun run scripts/llm-log.ts"
    },
    "engines": {
        "node": ">=18.0.0"
    },
    "publishConfig": {
        "access": "public",
        "registry": "https://registry.npmjs.org/"
    },
    "dependencies": {
        "@inquirer/search": "^3.0.15",
        "@modelcontextprotocol/sdk": "^1.13.2",
        "@nostr-dev-kit/ndk": "^2.14.33",
        "@types/inquirer": "^9.0.8",
        "chalk": "^5.4.1",
        "commander": "^13.1.0",
        "fs-extra": "^11.2.0",
        "inquirer": "^12.6.3",
        "js-base64": "^3.7.7",
        "jwt-js-decode": "^1.9.0",
        "multi-llm-ts": "^4.0.3",
        "nostr-tools": "^2.15.0",
        "repomix": "^1.1.0",
        "zod": "^3.25.67"
    },
    "devDependencies": {
        "@eslint/js": "^9.30.1",
        "@types/uuid": "^10.0.0",
        "esbuild": "^0.25.5",
        "eslint": "^9.30.1",
        "typescript": "^5.8.3",
        "typescript-eslint": "^8.35.1"
    },
    "type": "module",
    "keywords": [],
    "author": "",
    "license": "ISC"
}
</file>

<file path="test-stream-publisher.js">
// Test to verify StreamPublisher sends complete content in reply event
⋮----
// Mock context
⋮----
sign: async () => ({ sig: 'test-signature' })
⋮----
tagValue: (tag) => 'test-conversation-id'
⋮----
// Mock publisher
⋮----
publishResponse: async ({ content }) => {
console.log('✅ Final reply event content:', content);
console.log('✅ Content length:', content.length);
return new NDKEvent();
⋮----
// Test
async function testStreamPublisher() {
console.log('Testing StreamPublisher accumulated content fix...\n');
⋮----
// Create StreamPublisher instance
⋮----
const stream = new StreamPublisher(mockPublisher);
⋮----
// Simulate streaming content in chunks
⋮----
console.log('Adding content chunks:');
⋮----
console.log(`- Adding: "${chunk}"`);
stream.addContent(chunk);
// Simulate delay between chunks
await new Promise(resolve => setTimeout(resolve, 100));
⋮----
console.log('\nFinalizing stream...');
await stream.finalize({ phase: 'test' });
⋮----
const expectedContent = chunks.join('');
console.log(`\n✅ Expected total content: "${expectedContent}"`);
console.log(`✅ Expected length: ${expectedContent.length}`);
⋮----
// Run test
testStreamPublisher().catch(console.error);
</file>

<file path="tsconfig.build.json">
{
    "extends": "./tsconfig.json",
    "compilerOptions": {
        "noEmit": false,
        "declaration": true,
        "declarationMap": true,
        "sourceMap": true,
        "outDir": "./dist",
        "rootDir": "./src",
        "allowImportingTsExtensions": false,
        "module": "ESNext",
        "target": "ES2022",
        "moduleResolution": "bundler",
        "resolveJsonModule": true,
        "esModuleInterop": true,
        "allowSyntheticDefaultImports": true,
        "paths": {
            "@/*": ["./src/*"]
        }
    },
    "include": ["src/**/*"],
    "exclude": ["node_modules", "dist", "**/*.test.ts", "**/*.spec.ts", "src/**/__tests__/**"]
}
</file>

<file path="tsconfig.eslint.json">
{
    "extends": "./tsconfig.json",
    "include": [
        "src/**/*",
        "bin/**/*",
        "src/**/*.test.ts",
        "src/**/*.spec.ts",
        "src/**/*.integration.test.ts"
    ],
    "exclude": ["node_modules", "dist"]
}
</file>

<file path="tsconfig.json">
{
    "extends": "../tsconfig.base.json",
    "compilerOptions": {
        "noEmit": true,
        "rootDir": "..",
        "outDir": "./dist",
        "baseUrl": ".",
        "paths": {
            "@/*": ["./src/*"]
        },
        "noUnusedLocals": false,
        "noUnusedParameters": false
    },
    "include": ["src/**/*", "bin/**/*"],
    "exclude": ["node_modules", "dist", "**/*.test.ts", "**/*.spec.ts"]
}
</file>

</files>
